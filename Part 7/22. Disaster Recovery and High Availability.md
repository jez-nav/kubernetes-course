# Chapter 22: Disaster Recovery and High Availability

Ensuring applications remain available and data is protected even in the face of failures is paramount for production systems. Kubernetes provides building blocks for High Availability (HA), but a comprehensive Disaster Recovery (DR) strategy requires careful planning and tooling. This chapter covers designing HA clusters, backup and restore using Velero, multi-region/multi-cluster considerations, and handling failures.

## Designing HA Kubernetes Clusters

High Availability starts with the cluster architecture itself, aiming to eliminate single points of failure.

**Control Plane HA:**

*   **Multiple Master Nodes:** Run multiple instances (typically 3 or 5 for quorum) of control plane components (`kube-apiserver`, `kube-controller-manager`, `kube-scheduler`) spread across different nodes (ideally in different Availability Zones).
*   **Load Balancer:** Place a load balancer in front of the `kube-apiserver` instances to provide a single stable endpoint for clients (`kubectl`, worker nodes) and distribute load.
*   **etcd HA:** Run an etcd cluster with multiple members (typically 3 or 5), also spread across nodes/AZs. Etcd uses the Raft consensus algorithm to maintain consistency and tolerate member failures. Regular backups of etcd are critical (managed automatically by most managed services, requires manual setup for self-managed clusters).
*   **Managed Services (EKS, GKE, AKS):** Cloud providers handle control plane HA automatically, abstracting away the complexity of managing multiple masters and etcd. This is a major advantage of using managed Kubernetes.

**Worker Node HA:**

*   **Multiple Nodes:** Run a sufficient number of worker nodes to handle your workload and tolerate individual node failures.
*   **Multiple Availability Zones (AZs):** Distribute worker nodes across multiple AZs within a region. This protects against failures affecting an entire data center (AZ).
    *   Use node labels (e.g., `topology.kubernetes.io/zone`) provided by the cloud provider integration.
    *   Use Pod Anti-Affinity (Chapter 20) with `topologyKey: topology.kubernetes.io/zone` to spread replicas of critical Deployments/StatefulSets across AZs.
    *   Use Topology Spread Constraints for more fine-grained control over spreading Pods across AZs/nodes.
    *   Ensure your Persistent Volumes and StorageClasses are also AZ-aware (`volumeBindingMode: WaitForFirstConsumer` helps the scheduler place Pods in the same AZ as their required PV).

**Application HA:**

*   Run multiple replicas of your application Pods (Deployments, StatefulSets).
*   Use Pod Anti-Affinity to spread replicas across nodes and AZs.
*   Configure Pod Disruption Budgets (PDBs) to ensure a minimum number of replicas are available during voluntary disruptions (e.g., node upgrades, maintenance).
*   Implement proper Liveness and Readiness probes so Kubernetes knows the health of your Pods.

## Backup and Restore with Velero

While HA protects against immediate failures, Disaster Recovery requires the ability to recover from catastrophic events (cluster corruption, accidental deletion, region failure) by restoring from backups. **Velero** (formerly Heptio Ark) is the standard open-source tool for Kubernetes cluster backup, restore, and migration.

**Velero Concepts:**

*   **Backup:** Creates a point-in-time copy of cluster resource definitions (YAML objects) and optionally backs up PersistentVolume data.
*   **Restore:** Recreates cluster resources and optionally restores PV data from a backup.
*   **Storage Backends:** Stores backup data (resource definitions and PV backups) in external object storage (AWS S3, Google Cloud Storage, Azure Blob Storage, MinIO, etc.).
*   **Volume Snapshotters:** Integrates with cloud provider APIs or CSI drivers to take snapshots of PersistentVolumes (if configured).
*   **Filesystem Backups (Restic/Kopia):** Alternatively, can back up PV data at the filesystem level using integrated tools like Restic or Kopia (useful if volume snapshots aren't available or desired).
*   **Server & CLI:** Velero runs as a Deployment in the cluster, and you interact with it using the `velero` CLI tool.

**Installation and Configuration:**

1.  **Download `velero` CLI:** Get the latest release from the [Velero GitHub releases page](https://github.com/vmware-tanzu/velero/releases).
2.  **Prepare Object Storage:** Create a bucket (S3, GCS, Azure Blob) for Velero backups.
3.  **Configure Cloud Credentials:** Create credentials (e.g., an IAM user/role for S3, Service Account for GCS) that grant Velero permissions to access the object storage bucket and (optionally) take volume snapshots. Store these credentials securely (e.g., in a Kubernetes Secret).
4.  **Install Velero Server:** Use the `velero install` command, providing details about your object storage provider, bucket name, credentials file, and optionally enabling volume snapshotting plugins (e.g., `--use-volume-snapshots=true --plugins velero/velero-plugin-for-aws`). This command deploys the Velero Deployment, RBAC rules, and necessary CRDs.

**Basic Usage:**

*   **Create a Backup:**
    ```bash
    # Backup all resources in the 'webapp' namespace
    velero backup create webapp-backup-$(date +%Y%m%d%H%M%S) --include-namespaces webapp

    # Backup specific resources using labels
    velero backup create nginx-backup --selector app=nginx

    # Backup cluster-scoped resources (and optionally specific namespaces)
    velero backup create full-cluster-backup --include-cluster-resources=true # Be cautious with full cluster

    # Include PV snapshots (if snapshotter plugin is configured)
    velero backup create webapp-with-data --include-namespaces webapp --snapshot-volumes=true
    ```
*   **Check Backup Status:**
    ```bash
    velero backup get
    velero backup describe <backup-name>
    velero backup logs <backup-name>
    ```
*   **Schedule Backups:** Create `Schedule` CRDs to run backups automatically (e.g., daily).
    ```bash
    velero schedule create daily-webapp-backup --schedule="@daily" --include-namespaces webapp --snapshot-volumes=true --ttl 720h # Keep for 30 days
    ```
*   **Restore from Backup:**
    ```bash
    # Restore everything from a backup
    velero restore create --from-backup <backup-name>

    # Restore only specific namespaces
    velero restore create --from-backup <backup-name> --include-namespaces webapp

    # Restore specific resources
    velero restore create --from-backup <backup-name> --include-resources deployments,services --selector app=nginx

    # Restore PVs from snapshots (if included in backup)
    velero restore create --from-backup <backup-name> --restore-volumes=true
    ```
*   **Check Restore Status:**
    ```bash
    velero restore get
    velero restore describe <restore-name>
    velero restore logs <restore-name>
    ```

Velero is essential for any robust DR strategy in Kubernetes.

## Multi-Region and Multi-Cluster Setups

For higher levels of availability and disaster recovery, organizations often deploy applications across multiple Kubernetes clusters, potentially spanning different cloud regions.

**Motivations:**

*   **Region Failure Tolerance:** Survive the failure of an entire cloud region.
*   **Low Latency:** Serve users from a cluster geographically closer to them.
*   **Isolation:** Separate environments (e.g., staging vs. prod) into different clusters for better blast radius control.
*   **Specialized Clusters:** Have clusters optimized for specific workloads (e.g., GPU-intensive ML clusters).

**Challenges:**

*   **Configuration Management:** Keeping application deployments and configurations consistent across multiple clusters.
*   **Traffic Routing/Load Balancing:** Directing user traffic to the appropriate cluster (e.g., using global load balancers, GeoDNS).
*   **Service Discovery:** Enabling services in one cluster to discover and communicate with services in another cluster securely and reliably.
*   **Data Synchronization:** Keeping state consistent across databases or storage systems in different regions/clusters.
*   **Authentication/Authorization:** Managing user and service identities across clusters.

**Common Approaches & Tools:**

*   **Manual Deployment:** Use CI/CD pipelines to deploy the same application manifests to multiple clusters sequentially or in parallel. Simple but prone to inconsistencies.
*   **GitOps with Multiple Targets:** Configure GitOps tools (Argo CD, Flux) to manage deployments across multiple target clusters, potentially using different branches or directories in Git for cluster-specific configurations.
*   **Cluster Federation (KubeFed v2 - Less Common Now):** Aims to provide a single API endpoint to manage configuration across multiple registered clusters. Development has slowed, and adoption is limited compared to GitOps or Service Mesh approaches.
*   **Service Mesh (e.g., Istio, Linkerd):** Can be configured to span multiple clusters (multi-cluster mesh). They can provide:
    *   **Cross-Cluster Service Discovery:** Services in Cluster A can transparently discover and route traffic to services in Cluster B.
    *   **Unified Traffic Management:** Apply consistent traffic policies (retries, timeouts, canary releases) across clusters.
    *   **Mutual TLS (mTLS):** Secure communication between services across cluster boundaries.
    *   Requires careful setup of trust domains and network connectivity between clusters.
*   **Global Server Load Balancing (GSLB):** Use external DNS-based or cloud provider GSLB services to direct users to the nearest or healthiest cluster endpoint.

Multi-cluster setups add significant complexity but are necessary for achieving the highest levels of availability and geographic distribution.

## Handling Node and Cluster Failures

*   **Node Failure:**
    *   Kubernetes automatically detects node failures (e.g., kubelet stops reporting status).
    *   The node controller taints the node as unreachable.
    *   Pods on the failed node are marked as `Unknown` or `Terminating`.
    *   After a timeout (`pod-eviction-timeout`), Pods managed by controllers (Deployments, StatefulSets) are evicted and rescheduled onto healthy nodes (assuming sufficient cluster capacity).
    *   StatefulSet Pods will re-attach to their existing PVCs on the new node (if the PV is network-attached and accessible from the new node/AZ).
    *   **Impact:** Temporary unavailability of Pods on the failed node until they are rescheduled. Potential data loss for Pods using `hostPath` or non-replicated Local PVs.
*   **AZ Failure:**
    *   If nodes and application replicas are spread across AZs using anti-affinity/topology spread, the application should remain available, running on nodes in the surviving AZs.
    *   Cluster Autoscaler (if configured correctly with multi-AZ node groups) can automatically add replacement nodes in healthy AZs if needed.
    *   Storage must also be AZ-aware (e.g., use multi-AZ persistent disks or ensure application-level replication across AZs).
    *   **Impact:** Reduced capacity. Potential temporary performance degradation. Possible data unavailability if storage wasn't replicated/available across AZs.
*   **Cluster/Region Failure:**
    *   This is a disaster scenario requiring recovery from backups or failover to another cluster/region.
    *   **Recovery with Velero:** Provision a new cluster (e.g., using Terraform) in a different region. Restore cluster resources and PV data from Velero backups stored in object storage (ensure backups are replicated or stored multi-regionally). Update DNS/GSLB to point traffic to the new cluster.
    *   **Active-Active/Active-Passive Multi-Cluster:** If running a multi-cluster setup, GSLB or other traffic management tools can automatically redirect traffic away from the failed cluster/region to healthy ones. Requires careful planning for data synchronization between regions.
    *   **Impact:** Significant downtime during recovery unless a hot standby multi-cluster setup is in place. Potential data loss up to the last successful backup (Recovery Point Objective - RPO). Recovery time depends on the strategy (Recovery Time Objective - RTO).

## Lab: Simulate and Recover from a Cluster Failure (using Velero)

This lab simulates losing an application namespace and recovers it using Velero.

**Prerequisites:**

*   `kubectl` connected to a running Kubernetes cluster.
*   **Velero installed and configured** with an object storage backend (e.g., S3, MinIO) and appropriate credentials. Verify with `velero version` (client and server should match).
*   Optional: Velero volume snapshotter plugin installed and configured if you want to back up PV data via snapshots.

**Steps:**

1.  **Deploy a Sample Application:**
    Deploy a simple application with a Deployment and Service (e.g., the `nginx` deployment from earlier labs or the `ecommerce-app` from Chapter 18) into a dedicated namespace.
    ```bash
    kubectl create namespace disaster-demo
    # Example using nginx
    kubectl create deployment nginx-demo --image=nginx:1.21 -n disaster-demo
    kubectl expose deployment nginx-demo --port=80 --target-port=80 -n disaster-demo
    # Optional: Create a simple PVC and mount it if testing PV restore
    ```
    Verify the app is running: `kubectl get all -n disaster-demo`

2.  **Create a Velero Backup:**
    Back up the `disaster-demo` namespace. Include volumes if you created a PVC.
    ```bash
    # Backup without PVs
    velero backup create disaster-demo-backup-$(date +%Y%m%d%H%M%S) --include-namespaces disaster-demo

    # OR Backup *with* PVs (requires snapshot plugin configured)
    # velero backup create disaster-demo-backup-$(date +%Y%m%d%H%M%S) --include-namespaces disaster-demo --snapshot-volumes=true
    ```
    Wait for the backup to complete: `velero backup get` (Status should be `Completed`).
    Check logs: `velero backup logs <backup-name>`

3.  **Simulate Disaster:**
    Delete the entire namespace where the application was running. **Warning:** This is destructive!
    ```bash
    kubectl delete namespace disaster-demo
    # Output: namespace "disaster-demo" deleted
    ```
    Verify deletion: `kubectl get all -n disaster-demo` (Should return "No resources found").

4.  **Restore from Backup:**
    Use Velero to restore the namespace and its contents from the backup created earlier.
    ```bash
    # Find your backup name
    velero backup get

    # Create the restore (replace <backup-name> with your actual backup name)
    # Restore without PVs
    velero restore create disaster-demo-restore --from-backup <backup-name> --include-namespaces disaster-demo

    # OR Restore *with* PVs (requires PV data in backup)
    # velero restore create disaster-demo-restore --from-backup <backup-name> --include-namespaces disaster-demo --restore-volumes=true
    ```

5.  **Monitor Restore and Verify:**
    Check the restore status:
    ```bash
    velero restore get # Wait for Status=Completed
    velero restore describe disaster-demo-restore
    velero restore logs disaster-demo-restore
    ```
    Once the restore is complete, verify that the namespace and its resources (Deployment, Service, Pods, PVCs if applicable) have been recreated:
    ```bash
    kubectl get ns disaster-demo
    kubectl get all -n disaster-demo
    # Pods should be starting/running
    ```
    If you restored volumes, check if the data is present inside the Pods.

6.  **Clean Up:**
    ```bash
    kubectl delete namespace disaster-demo
    # Optionally delete Velero backups and schedules if desired
    # velero backup delete <backup-name>
    # velero schedule delete <schedule-name>
    # Uninstall Velero if needed (check Velero docs for uninstall steps)
    ```

**Congratulations!** You have successfully used Velero to back up a Kubernetes namespace and restore it after simulating a disaster (namespace deletion). This demonstrates the fundamental capability of Velero for disaster recovery scenarios. Remember to regularly test your backup and restore procedures.
