# Chapter 19: CI/CD for Kubernetes

Automating the build, test, and deployment process is essential for delivering software reliably and frequently. Continuous Integration (CI) and Continuous Deployment/Delivery (CD) pipelines are standard practice in modern software development, and Kubernetes provides a powerful platform to build upon. This chapter explores how to create CI/CD pipelines specifically for applications running on Kubernetes, covering common tools, Helm for packaging, and integrating infrastructure changes.

## Building CI/CD Pipelines

A CI/CD pipeline automates the steps required to get code changes from a developer's machine into production (or other environments). For Kubernetes applications, a typical pipeline involves these stages:

1.  **Source:** Triggered by a code change committed to a Git repository (e.g., push to `main` or merge of a pull request).
2.  **Build:**
    *   Compile code (if necessary).
    *   Build the container image using a `Dockerfile`.
    *   Tag the image appropriately (e.g., with the Git commit SHA or a semantic version).
3.  **Test:**
    *   Run unit tests.
    *   Run integration tests (potentially spinning up dependent services in containers or a temporary Kubernetes namespace).
    *   Perform static code analysis and security scans (on code and the container image).
4.  **Push:** Push the tagged container image to a container registry (Docker Hub, ECR, GCR, ACR, etc.).
5.  **Deploy:** Deploy the new version of the application to a Kubernetes environment (e.g., staging, production). This usually involves:
    *   Updating Kubernetes manifests (e.g., changing the `image:` tag in a Deployment YAML).
    *   Applying the updated manifests to the cluster (`kubectl apply`, `helm upgrade`).
6.  **Verify/Monitor (Optional but Recommended):** Run automated smoke tests or monitor application health/metrics after deployment to ensure the rollout was successful. Implement automated rollback if verification fails.

**Popular CI/CD Tools:**

Many tools can orchestrate these pipelines. Common choices include:

*   **GitHub Actions:** Integrated directly into GitHub repositories. Uses YAML workflow files stored in `.github/workflows/`. Provides hosted runners (Linux, Windows, macOS) or allows self-hosted runners. Extensive marketplace of pre-built actions (e.g., for Docker builds, cloud logins, `kubectl`, Helm).
*   **GitLab CI/CD:** Integrated into GitLab repositories. Uses a `.gitlab-ci.yml` file. Offers built-in container registry and powerful pipeline features. Uses GitLab Runners (shared or self-hosted).
*   **Jenkins:** A long-standing, highly extensible open-source automation server. Can be self-hosted (including running on Kubernetes itself). Pipelines typically defined using a `Jenkinsfile` (Groovy syntax). Requires more setup and maintenance but offers maximum flexibility.
*   **Cloud Provider Services:** AWS CodePipeline/CodeBuild/CodeDeploy, Google Cloud Build/Cloud Deploy, Azure DevOps Pipelines. Offer tight integration with their respective cloud platforms.

The choice often depends on where your code is hosted, existing tooling, and desired level of control vs. convenience.

## Automating Container Builds and Deployments

**Container Builds in CI:**

*   CI pipelines typically use Docker (or compatible tools like Buildah) running within the CI runner environment (often a container itself - "Docker-in-Docker" or socket mounting).
*   The pipeline checks out the code, runs `docker build -t <registry>/<image>:<tag> .`, and then `docker push <registry>/<image>:<tag>`.
*   Securely logging into the container registry is crucial, usually done via secrets/credentials stored in the CI/CD system.

**Automated Deployments:**

This is the core CD part. The pipeline needs to update the Kubernetes cluster with the new image version. Common methods:

1.  **Using `kubectl apply`:**
    *   The pipeline checks out the repository containing Kubernetes manifests.
    *   It modifies the image tag in the relevant Deployment YAML file (e.g., using `sed`, `yq`, or `kustomize edit set image`).
    *   It authenticates to the Kubernetes cluster (using a `kubeconfig` file, service account token, or cloud IAM integration configured as a CI/CD secret).
    *   It runs `kubectl apply -f <manifest-directory/>` or `kubectl apply -k <kustomize-directory/>`.
    *   Optionally, it monitors the rollout status: `kubectl rollout status deployment/<name>`.

2.  **Using Helm:** (See next section)
    *   The pipeline updates the image tag in the Helm chart's `values.yaml` file or passes it via `--set image.tag=<new-tag>`.
    *   It runs `helm upgrade <release-name> <chart-path> --install --namespace <ns> ...`.

3.  **Using GitOps Tools (Argo CD / Flux):**
    *   The CI pipeline's primary role is to build and push the image.
    *   It then updates the image tag in the *manifest repository* (the one monitored by Argo CD/Flux).
    *   The GitOps tool running in the cluster detects the change in the manifest repo and automatically pulls the new image and applies the update to the cluster. This decouples CI (build/push) from CD (deploy/sync). See Chapter 17.

## Using Helm for Package Management

**Helm** is the de facto package manager for Kubernetes. It helps you define, install, and upgrade even the most complex Kubernetes applications.

*   **Charts:** Helm packages are called Charts. A Chart is a collection of files describing a related set of Kubernetes resources. It includes:
    *   `Chart.yaml`: Metadata about the chart (name, version, description).
    *   `values.yaml`: Default configuration values for the chart.
    *   `templates/`: A directory containing Kubernetes manifest files written as Go templates. These templates use values from `values.yaml` (or overridden values) to generate the final Kubernetes manifests.
    *   `charts/`: Optional directory for chart dependencies (subcharts).
*   **Templates:** Use Go templating syntax (`{{ .Values.someKey }}`) to insert values into manifests, enabling parameterization and conditional logic.
*   **Values:** Configuration values that customize a chart release. Defaults are in `values.yaml`, but can be overridden during installation/upgrade via `--set key=value` or `-f my-values.yaml`.
*   **Releases:** An instance of a chart running in a Kubernetes cluster. Each release has a name. Helm tracks the history of releases, allowing easy upgrades and rollbacks.

**Why Use Helm in CI/CD?**

*   **Packaging:** Bundles all necessary Kubernetes resources for an application into a single versioned package.
*   **Parameterization:** Easily configure deployments for different environments (dev, staging, prod) by providing different `values.yaml` files or `--set` arguments.
*   **Lifecycle Management:** `helm install`, `helm upgrade`, `helm rollback`, `helm uninstall` provide standardized commands for managing application lifecycles.
*   **Reusability & Sharing:** Charts can be shared via Helm repositories (like Artifact Hub or private ones).

**Helm in a Pipeline:**

```bash
# Example steps in a CI/CD script

# Assume CHART_PATH points to the chart directory
# Assume RELEASE_NAME is the desired release name
# Assume NAMESPACE is the target namespace
# Assume NEW_IMAGE_TAG holds the newly built image tag

# Lint the chart (optional static analysis)
helm lint $CHART_PATH

# Package the chart (optional, needed for repositories)
# helm package $CHART_PATH

# Deploy or upgrade the release
helm upgrade $RELEASE_NAME $CHART_PATH \
  --install \ # Install if it doesn't exist, upgrade otherwise
  --namespace $NAMESPACE \
  --create-namespace \ # Create namespace if it doesn't exist
  --set image.tag=$NEW_IMAGE_TAG \ # Override image tag value
  # -f values-production.yaml # Optionally load environment-specific values
  --wait # Wait for resources to become ready (useful for verification)
```

## Integrating Terraform for Infrastructure Updates

Sometimes, deploying an application requires changes to the underlying infrastructure (e.g., creating a new database, setting up DNS records, adjusting IAM permissions). Terraform can be integrated into CI/CD pipelines to manage these infrastructure dependencies alongside application deployments.

**Integration Strategies:**

1.  **Separate Pipelines:** Have distinct pipelines for infrastructure (Terraform) and application (kubectl/Helm/GitOps) changes. Infrastructure changes might need to run and complete *before* the application pipeline triggers. This is often the simplest and safest approach.
2.  **Combined Pipeline (Triggered Infrastructure):** The application CI/CD pipeline includes steps to run `terraform plan` and potentially `terraform apply` for a specific infrastructure configuration related to the application.
    *   **Pros:** Keeps related infrastructure and application changes together.
    *   **Cons:** Can make pipelines slower and more complex. Requires careful management of Terraform state and credentials within the application pipeline. Increases the blast radius if something goes wrong.
3.  **Terraform Manages App Deployment (Less Common):** Use Terraform's Kubernetes or Helm provider to deploy the application directly as part of the `terraform apply`.
    *   **Pros:** Single tool manages everything.
    *   **Cons:** Blurs lines between infra and app management. Terraform isn't always the best tool for fine-grained application rollout strategies (canary, blue-green) compared to dedicated CD tools or service meshes. State file can become very large.

**Recommendation:** Start with **separate pipelines**. Use Terraform pipelines (potentially triggered manually or on infrastructure code changes) to manage the cluster and shared resources. Use separate application CI/CD pipelines (triggered on application code changes) to build images and deploy using `kubectl`, Helm, or GitOps tools. If an application deployment *requires* an infrastructure change, coordinate the pipeline runs or trigger the infrastructure pipeline first.

## Lab: Create a CI/CD Pipeline for a Kubernetes App (GitHub Actions)

This lab creates a simple GitHub Actions workflow that builds a Docker image for a basic web app, pushes it to Docker Hub, and deploys it to Kubernetes using `kubectl apply`.

**Prerequisites:**

*   `kubectl` connected to a running Kubernetes cluster.
*   A Docker Hub account ([https://hub.docker.com/](https://hub.docker.com/)).
*   A GitHub account and a repository for the application code.

**Steps:**

1.  **Create Sample Application:**
    *   In your GitHub repository, create:
        *   `Dockerfile`:
            ```dockerfile
            FROM nginx:1.21-alpine
            COPY index.html /usr/share/nginx/html/index.html
            ```
        *   `index.html`:
            ```html
            <!DOCTYPE html>
            <html>
            <head><title>CI/CD Demo V1</title></head>
            <body><h1>Hello from Kubernetes CI/CD! Version 1</h1></body>
            </html>
            ```
        *   `k8s/deployment.yaml`:
            ```yaml
            apiVersion: apps/v1
            kind: Deployment
            metadata:
              name: cicd-demo-app
            spec:
              replicas: 2
              selector:
                matchLabels:
                  app: cicd-demo
              template:
                metadata:
                  labels:
                    app: cicd-demo
                spec:
                  containers:
                  - name: web
                    # Image will be replaced by CI/CD
                    image: YOUR_DOCKERHUB_USERNAME/cicd-demo:latest # !!! REPLACE USERNAME !!!
                    ports:
                    - containerPort: 80
            ```
        *   `k8s/service.yaml`:
            ```yaml
            apiVersion: v1
            kind: Service
            metadata:
              name: cicd-demo-svc
            spec:
              selector:
                app: cicd-demo
              ports:
              - protocol: TCP
                port: 80
                targetPort: 80
              type: ClusterIP # Or LoadBalancer if on cloud
            ```
    *   **Important:** Replace `YOUR_DOCKERHUB_USERNAME` in `k8s/deployment.yaml` with your actual Docker Hub username.
    *   Commit and push these files to your repository.

2.  **Configure GitHub Secrets:**
    *   In your GitHub repository, go to `Settings` > `Secrets and variables` > `Actions`.
    *   Create the following repository secrets:
        *   `DOCKERHUB_USERNAME`: Your Docker Hub username.
        *   `DOCKERHUB_TOKEN`: A Docker Hub access token (create one in Docker Hub under Account Settings > Security > Access Tokens). **Do not use your password.**
        *   `KUBE_CONFIG_DATA`: The base64 encoded content of your `kubeconfig` file that provides access to your Kubernetes cluster.
            *   Find your kubeconfig file (usually `~/.kube/config`).
            *   **Important:** Ensure this kubeconfig grants sufficient permissions (e.g., create/update Deployments and Services in the target namespace). Consider creating a dedicated ServiceAccount with limited permissions for CI/CD in production.
            *   Encode it: `cat ~/.kube/config | base64` (on Linux/macOS) or use an online base64 encoder. Copy the *entire* output string.
            *   Paste the base64 string as the value for the `KUBE_CONFIG_DATA` secret.
        *   `KUBE_NAMESPACE`: The Kubernetes namespace where you want to deploy (e.g., `default`).

3.  **Create GitHub Actions Workflow:**
    *   In your repository, create the directory `.github/workflows/`.
    *   Inside that directory, create `cicd.yaml`:
        ```yaml
        name: Kubernetes CI/CD Demo

        on:
          push:
            branches: [ main ] # Trigger on push to main branch

        jobs:
          build-and-deploy:
            runs-on: ubuntu-latest # Use a Linux runner

            steps:
            - name: Checkout code
              uses: actions/checkout@v3

            - name: Set up Docker Buildx
              uses: docker/setup-buildx-action@v2

            - name: Login to Docker Hub
              uses: docker/login-action@v2
              with:
                username: ${{ secrets.DOCKERHUB_USERNAME }}
                password: ${{ secrets.DOCKERHUB_TOKEN }}

            - name: Build and push Docker image
              id: docker_build
              uses: docker/build-push-action@v4
              with:
                context: . # Build context is the repo root
                file: ./Dockerfile
                push: true
                tags: ${{ secrets.DOCKERHUB_USERNAME }}/cicd-demo:${{ github.sha }} # Tag with Git SHA
                # Also tag as latest for simplicity in deployment manifest initially
                # In real scenarios, you'd update the deployment manifest tag
                # tags: |
                #   ${{ secrets.DOCKERHUB_USERNAME }}/cicd-demo:${{ github.sha }}
                #   ${{ secrets.DOCKERHUB_USERNAME }}/cicd-demo:latest

            - name: Install kubectl
              uses: azure/setup-kubectl@v3
              # uses: azure/k8s-actions@v1 # Alternative action

            - name: Configure kubectl
              run: |
                mkdir -p ~/.kube
                echo "${{ secrets.KUBE_CONFIG_DATA }}" | base64 -d > ~/.kube/config
                chmod 600 ~/.kube/config

            # --- Deployment Strategy 1: Update manifest and apply ---
            - name: Update deployment image tag
              run: |
                IMAGE_TAG=${{ github.sha }}
                echo "Using image tag: $IMAGE_TAG"
                # Use sed to replace the image tag (adjust path if needed)
                sed -i "s|image: .*|image: ${{ secrets.DOCKERHUB_USERNAME }}/cicd-demo:$IMAGE_TAG|g" k8s/deployment.yaml
                echo "Updated deployment manifest:"
                cat k8s/deployment.yaml

            - name: Deploy to Kubernetes
              run: |
                kubectl apply -f k8s/ -n ${{ secrets.KUBE_NAMESPACE }}
                kubectl rollout status deployment/cicd-demo-app -n ${{ secrets.KUBE_NAMESPACE }} --timeout=60s

            # --- Deployment Strategy 2: Using Helm (Alternative - requires Helm chart) ---
            # - name: Install Helm
            #   uses: azure/setup-helm@v3
            #
            # - name: Deploy with Helm
            #   run: |
            #     helm upgrade cicd-demo ./helm-chart/ \ # Assuming chart is in ./helm-chart
            #       --install \
            #       --namespace ${{ secrets.KUBE_NAMESPACE }} \
            #       --create-namespace \
            #       --set image.repository=${{ secrets.DOCKERHUB_USERNAME }}/cicd-demo \
            #       --set image.tag=${{ github.sha }} \
            #       --wait
        ```

4.  **Commit and Push Workflow:**
    Commit `.github/workflows/cicd.yaml` and push it to your `main` branch.

5.  **Trigger and Monitor Workflow:**
    *   The push to `main` will automatically trigger the GitHub Actions workflow.
    *   Go to the "Actions" tab in your GitHub repository.
    *   Click on the running workflow to see the logs for each step (Build, Push, Deploy).
    *   If successful, verify the deployment in your cluster:
        ```bash
        kubectl get deployment cicd-demo-app -n <your-namespace>
        kubectl get pods -l app=cicd-demo -n <your-namespace>
        # Optionally port-forward the service and check the content
        # kubectl port-forward service/cicd-demo-svc 8080:80 -n <your-namespace>
        # curl http://localhost:8080
        ```

6.  **Test an Update:**
    *   Edit `index.html` in your repository (e.g., change "Version 1" to "Version 2").
    *   Commit and push the change to the `main` branch.
    *   Observe the GitHub Actions workflow trigger again.
    *   It will build a new image tagged with the new commit SHA, update the Deployment manifest, and apply it.
    *   Verify the rollout: `kubectl rollout status deployment/cicd-demo-app -n <your-namespace>`
    *   Check the application content again (via port-forward/curl) - it should now show "Version 2".

7.  **Clean Up:**
    ```bash
    kubectl delete -f k8s/ -n <your-namespace>
    # Delete secrets from GitHub repository settings
    # Delete Docker Hub image if desired
    ```

**Congratulations!** You have created a basic CI/CD pipeline using GitHub Actions that automatically builds and deploys changes to your Kubernetes application upon code commits. This automation significantly improves deployment speed and reliability.
