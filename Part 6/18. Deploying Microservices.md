# Chapter 18: Deploying Microservices

Microservices architecture involves breaking down a large application into smaller, independent, and loosely coupled services. Kubernetes is an excellent platform for deploying and managing microservices due to its features like service discovery, load balancing, scaling, and resilience. This chapter discusses designing microservices for Kubernetes, communication patterns, handling dependencies, and advanced deployment strategies.

## Designing Microservices for Kubernetes

When designing microservices to run on Kubernetes, consider leveraging its capabilities:

*   **Containerization:** Package each microservice as a lightweight container image (Chapter 2). Use minimal base images and follow image security best practices (Chapter 14).
*   **Statelessness (where possible):** Design services to be stateless whenever feasible. Store state externally (databases, caches, object storage) managed via PersistentVolumes (Chapter 7) or external services. Stateless services are easier to scale horizontally and manage with Deployments.
*   **Configuration:** Externalize configuration using ConfigMaps and Secrets (Chapter 11). Avoid embedding configuration in container images.
*   **Health Checks:** Implement robust Liveness and Readiness probes (Chapter 4) so Kubernetes can accurately manage Pod lifecycle and traffic routing. Readiness probes are crucial to prevent traffic from being sent to a service before it's fully initialized or ready to serve.
*   **Resource Requests/Limits:** Define appropriate CPU and Memory requests and limits (Chapter 4) for each microservice Pod to ensure proper scheduling and prevent resource contention.
*   **Graceful Shutdown:** Ensure your application handles termination signals (SIGTERM) gracefully, finishing in-flight requests and closing connections before exiting. Configure `terminationGracePeriodSeconds` appropriately in the Pod spec.
*   **API Design:** Design clear, well-documented APIs (REST, gRPC) for inter-service communication.

## Service Discovery and Communication

Kubernetes provides built-in mechanisms for services to find and communicate with each other:

1.  **Services (ClusterIP):**
    *   The standard way for internal communication. Create a `Service` of type `ClusterIP` for each microservice Deployment (or StatefulSet).
    *   This Service gets a stable internal IP address (ClusterIP) and a DNS name (`<service-name>.<namespace>.svc.cluster.local`).
    *   Other Pods within the cluster can reliably connect to the microservice using its Service DNS name. Kubernetes DNS resolves this name to the ClusterIP, and `kube-proxy` load balances traffic destined for the ClusterIP across the healthy backend Pods selected by the Service.
    *   Example: A `frontend` Pod can connect to `http://backend-api.production.svc.cluster.local` (or just `http://backend-api` if in the same `production` namespace).

2.  **Headless Services:**
    *   A Service with `spec.clusterIP: None`.
    *   Instead of providing a single virtual IP, Kubernetes DNS returns the individual IP addresses of all Pods selected by the Service.
    *   Useful when clients need to connect to specific Pod instances directly (e.g., peer-to-peer discovery in a StatefulSet) or implement their own client-side load balancing.

3.  **Environment Variables (Legacy/Limited):**
    *   When a Pod starts, Kubernetes can optionally inject environment variables for existing Services (e.g., `MY_SERVICE_SERVICE_HOST`, `MY_SERVICE_SERVICE_PORT`).
    *   **Discouraged:** This creates an ordering dependency (Services must exist *before* Pods that consume them) and pollutes the environment variable space. DNS-based service discovery is strongly preferred.

**Communication Patterns:**

*   **Synchronous (Request/Response):** One service makes a request (e.g., HTTP REST, gRPC) to another and waits for a response. Simple but can lead to tight coupling and cascading failures if a downstream service is slow or unavailable. Use timeouts, retries (with backoff), and circuit breakers.
*   **Asynchronous (Event-Driven/Messaging):** Services communicate indirectly via events or messages published to a message queue (e.g., Kafka, RabbitMQ, NATS) or event bus. Decouples services, improves resilience (producer doesn't wait for consumer), but adds complexity of managing the messaging system.

## Handling Inter-Service Dependencies

Microservices often depend on each other. Managing these dependencies during startup and operation is important:

*   **Startup Dependencies:** Service A might need Service B to be running and ready before it can start properly.
    *   **Readiness Probes:** Ensure Service B has a proper readiness probe. Service A can attempt to connect to Service B in a loop within its own startup logic, only becoming "ready" (passing its own readiness probe) once it successfully connects to Service B.
    *   **Init Containers:** Use Init Containers in Service A's Pod definition. An Init Container can run a script that waits (e.g., polls Service B's endpoint or DNS name) until Service B is available before the main application container (Service A) is started.
        ```yaml
        # Example Init Container in Pod Spec
        spec:
          initContainers:
          - name: wait-for-backend
            image: busybox:1.28
            command: ['sh', '-c', 'until nslookup backend-service.default.svc.cluster.local; do echo waiting for backend service; sleep 2; done;']
          containers:
          - name: main-app-container
            image: my-app
            # ...
        ```
*   **Runtime Dependencies:** Service A needs Service B during operation.
    *   **Resilience Patterns:** Implement patterns like timeouts, retries (with exponential backoff), and circuit breakers (using libraries like Hystrix, Resilience4j, or built into service meshes) in Service A's client code to handle temporary unavailability or failures of Service B gracefully.
    *   **Service Mesh:** Tools like Istio or Linkerd can manage retries, timeouts, and circuit breaking transparently at the network level via sidecar proxies, reducing the need for boilerplate resilience code in each microservice.

## Blue-Green and Canary Deployments

Kubernetes Deployments support `RollingUpdate` by default, which minimizes downtime but mixes old and new versions during the rollout. For more control, consider Blue-Green or Canary strategies, often implemented by manipulating Service selectors or using more advanced tools.

1.  **Blue-Green Deployment:**
    *   **Concept:** Maintain two identical environments ("Blue" - the current production, "Green" - the new version). Route traffic to Blue. Deploy the new version to Green. Test Green. Once confident, switch the router (e.g., Kubernetes Service selector) to point all traffic from Blue to Green instantly. Keep Blue running temporarily for easy rollback.
    *   **Implementation with Services:**
        1.  Deploy `my-app-v1` (Blue) with label `version: v1`.
        2.  Create a Service `my-app-service` selecting `app: my-app` (initially selects nothing or only Blue).
        3.  Point the main Service selector to Blue: `kubectl patch service my-app-service -p '{"spec": {"selector": {"app": "my-app", "version": "v1"}}}'`. Traffic flows to v1.
        4.  Deploy `my-app-v2` (Green) with label `version: v2`. Test Green internally (e.g., via port-forward or a separate test Service).
        5.  Switch traffic: `kubectl patch service my-app-service -p '{"spec": {"selector": {"app": "my-app", "version": "v2"}}}'`. All traffic now goes to v2.
        6.  Monitor v2. If issues arise, switch back to v1 instantly.
        7.  Once confident, decommission v1.
    *   **Pros:** Instant cutover, zero downtime (if done correctly), simple rollback.
    *   **Cons:** Requires double the resources during the transition. Doesn't test the new version with production load before full cutover.

2.  **Canary Release:**
    *   **Concept:** Gradually shift a small percentage of production traffic to the new version (the "canary"). Monitor the canary closely for errors or performance issues. If it performs well, gradually increase the traffic percentage until all traffic goes to the new version. If issues arise, roll back traffic quickly.
    *   **Implementation with Multiple Deployments & Service:**
        1.  Have `my-app-v1` Deployment running (e.g., 9 replicas) selected by `my-app-service`.
        2.  Deploy `my-app-v2` (Canary) Deployment with fewer replicas (e.g., 1 replica) and the *same* `app: my-app` label but potentially a different `version: v2` label.
        3.  The Service `my-app-service` now selects Pods from *both* Deployments (total 10 Pods). Kubernetes load balancing (kube-proxy) will distribute traffic roughly proportionally to the number of Pods (approx. 10% to v2, 90% to v1).
        4.  Monitor v2 metrics and logs.
        5.  If v2 is healthy, gradually scale up the v2 Deployment and scale down the v1 Deployment while keeping the total replica count consistent until v1 has 0 replicas.
        6.  If v2 has issues, scale it down to 0 replicas quickly.
    *   **Implementation with Ingress Controllers / Service Mesh:** More sophisticated canary releases (e.g., based on traffic percentage, specific headers, user groups) can be achieved using features in advanced Ingress controllers (like Nginx Ingress with annotations, Traefik) or Service Meshes (Istio, Linkerd) which offer fine-grained traffic splitting capabilities.
    *   **Pros:** Tests new version with real production traffic gradually. Minimizes blast radius if issues occur. Zero downtime.
    *   **Cons:** More complex to manage traffic shifting (especially without advanced tooling). Requires robust monitoring to evaluate canary health. Can take longer to fully roll out.

## Lab: Deploy a Microservices-Based E-commerce App

This lab deploys a simplified e-commerce application consisting of three microservices: `frontend`, `products-api`, and `orders-api`. It demonstrates basic service discovery and communication.

**Prerequisites:**

*   `kubectl` connected to a running Kubernetes cluster.

**Steps:**

1.  **Create Manifests:**
    Create a directory `ecommerce-app`. Inside it, create the following files:

    *   `products-deployment.yaml`:
        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: products-api
        spec:
          replicas: 2
          selector:
            matchLabels:
              app: products-api
          template:
            metadata:
              labels:
                app: products-api
            spec:
              containers:
              - name: products
                image: nginx:1.21 # Simulate API with Nginx
                ports:
                - containerPort: 80
        ```
    *   `products-service.yaml`:
        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: products-api-svc # Service name used for discovery
        spec:
          selector:
            app: products-api
          ports:
          - protocol: TCP
            port: 80
            targetPort: 80
          type: ClusterIP
        ```
    *   `orders-deployment.yaml`:
        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: orders-api
        spec:
          replicas: 2
          selector:
            matchLabels:
              app: orders-api
          template:
            metadata:
              labels:
                app: orders-api
            spec:
              containers:
              - name: orders
                image: k8s.gcr.io/echoserver:1.10 # Simulate API with echoserver
                ports:
                - containerPort: 8080
        ```
    *   `orders-service.yaml`:
        ```yaml
        apiVersion: v1
        kind: Service
        metadata:
          name: orders-api-svc # Service name used for discovery
        spec:
          selector:
            app: orders-api
          ports:
          - protocol: TCP
            port: 80
            targetPort: 8080 # Match echoserver container port
          type: ClusterIP
        ```
    *   `frontend-deployment.yaml`:
        ```yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          name: frontend
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: frontend
          template:
            metadata:
              labels:
                app: frontend
            spec:
              containers:
              - name: frontend-app
                image: busybox:latest # Use busybox to simulate calls
                command: ["/bin/sh", "-c"]
                args:
                  - >
                    while true; do
                      echo "--- Frontend Log $(date) ---";
                      echo "Attempting to reach Products API...";
                      # Use service DNS name for discovery
                      wget -q -O- http://products-api-svc.default.svc.cluster.local || echo "Failed to reach Products API";
                      echo "";
                      echo "Attempting to reach Orders API...";
                      # Use short service name (works within same namespace)
                      wget -q -O- http://orders-api-svc || echo "Failed to reach Orders API";
                      echo "------------------------------";
                      sleep 10;
                    done
        ```

2.  **Deploy the Application:**
    Apply all manifests from the `ecommerce-app` directory:
    ```bash
    kubectl apply -f ecommerce-app/
    # Output: deployment.apps/products-api created, service/products-api-svc created, ... etc.
    ```

3.  **Verify Deployments and Services:**
    ```bash
    kubectl get deployments
    # NAME           READY   UP-TO-DATE   AVAILABLE   AGE
    # frontend       1/1     1            1           ...
    # orders-api     2/2     2            2           ...
    # products-api   2/2     2            2           ...

    kubectl get services
    # NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
    # kubernetes         ClusterIP   10.96.0.1       <none>        443/TCP   ...
    # orders-api-svc     ClusterIP   10.100.x.x      <none>        80/TCP    ...
    # products-api-svc   ClusterIP   10.101.y.y      <none>        80/TCP    ...

    kubectl get pods -l app=frontend -o wide
    kubectl get pods -l app=products-api -o wide
    kubectl get pods -l app=orders-api -o wide
    ```

4.  **Check Frontend Logs:**
    View the logs of the `frontend` Pod to see if it can communicate with the backend services using their DNS names.
    ```bash
    kubectl logs -f deployment/frontend
    ```
    You should see output similar to:
    ```
    --- Frontend Log Thu Apr 18 03:55:00 UTC 2025 ---
    Attempting to reach Products API...
    <!DOCTYPE html>
    <html>
    <head>
    <title>Welcome to nginx!</title>
    ... (Nginx welcome page HTML) ...
    </html>

    Attempting to reach Orders API...
    CLIENT VALUES:
    client_address=...
    command=GET
    ... (echoserver output) ...
    ------------------------------
    ```
    This confirms the frontend Pod successfully discovered and communicated with the `products-api-svc` and `orders-api-svc` using their internal Kubernetes Service DNS names.

5.  **Clean Up:**
    Delete all the resources created:
    ```bash
    kubectl delete -f ecommerce-app/
    ```

**Congratulations!** You have deployed a simple microservices application on Kubernetes, demonstrating how services discover and communicate with each other using built-in Kubernetes Service discovery (DNS). This forms the basis for deploying more complex, distributed systems.
