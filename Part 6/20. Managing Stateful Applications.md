# Chapter 20: Managing Stateful Applications

While Kubernetes excels at managing stateless applications, many real-world systems rely on stateful components like databases, message queues, and key-value stores. Managing these requires careful consideration of data persistence, identity, and lifecycle. This chapter revisits StatefulSets and explores strategies for deploying databases, handling backups and restores, managing data migrations, and achieving high availability for stateful workloads on Kubernetes.

## Deploying Databases with StatefulSets

As introduced in Chapter 7, **StatefulSets** are the primary Kubernetes controller for managing stateful applications. They provide crucial guarantees that Deployments lack:

*   **Stable, Unique Network Identifiers:** Pods get predictable DNS names (`<statefulset-name>-<ordinal>.<headless-service-name>...`) essential for peer discovery and client connections. Usually paired with a Headless Service.
*   **Stable, Persistent Storage:** Each Pod replica gets its own unique PersistentVolumeClaim (PVC) based on `volumeClaimTemplates`. When a Pod is rescheduled, it re-attaches to its *exact same* PVC, ensuring data persistence for that specific instance.
*   **Ordered, Graceful Deployment and Scaling:** Pods are created, updated (using `RollingUpdate` or `OnDelete` strategies), and deleted in a strict, predictable order (0, 1, 2... or N-1, N-2,... 0). This is vital for applications like clustered databases that require ordered startup or shutdown.

**Example Recap (PostgreSQL StatefulSet - Single Instance):**

Refer back to the `postgres-statefulset.yaml` example in Chapter 7 (Lab section). Key elements include:

*   `StatefulSet` kind.
*   `serviceName` pointing to a Headless Service (`clusterIP: None`).
*   `replicas` field (can be scaled, respecting order).
*   `template` defining the Pod spec (container image, ports, env vars like `POSTGRES_PASSWORD`, `PGDATA`).
*   `volumeClaimTemplates` defining the PVC (`postgres-data`) that will be created for each replica, ensuring each Pod gets its own persistent storage.
*   `volumeMounts` in the container spec to mount the PVC (`postgres-data`) at the correct path (`/var/lib/postgresql/data`).

**Deploying Common Databases:**

*   **PostgreSQL/MySQL:** Often deployed using StatefulSets with a single replica for simple setups or multiple replicas configured for primary/standby or primary/primary replication (requires additional configuration within the database container or via an Operator). Official Docker images often require environment variables for initialization (e.g., root password, database name). Persistent storage via `volumeClaimTemplates` is essential.
*   **MongoDB:** Can be deployed as a Replica Set using a StatefulSet. Each Pod acts as a member of the MongoDB replica set, using the stable network IDs for peer discovery. Requires careful configuration of MongoDB itself within the container entrypoint or via init containers.
*   **Redis:** Can be deployed standalone (Deployment + PVC) or as a cluster (StatefulSet + Headless Service + Redis cluster configuration).
*   **Elasticsearch/Kafka:** Typically deployed using StatefulSets due to their clustered nature, reliance on stable identity, and persistent storage requirements.

**Using Operators:**

For complex stateful applications like databases or Kafka, managing replication, backups, upgrades, and failure recovery manually via StatefulSets can still be challenging. **Database Operators** (Chapter 15) automate these tasks. Examples:

*   [Zalando Postgres Operator](https://github.com/zalando/postgres-operator)
*   [CrunchyData PostgreSQL Operator](https://www.crunchydata.com/products/crunchy-postgresql-for-kubernetes/)
*   [Percona Operator for MySQL](https://www.percona.com/software/mysql-database/percona-operator-for-mysql)
*   [MongoDB Enterprise Kubernetes Operator](https://www.mongodb.com/docs/kubernetes-operator/stable/)
*   [Strimzi (Kafka Operator)](https://strimzi.io/)

Operators provide CRDs (e.g., `kind: Postgresql`, `kind: KafkaCluster`) that simplify deployment and management significantly compared to raw StatefulSets. If a mature Operator exists for your stateful application, it's often the recommended approach for production deployments.

## Backup and Restore Strategies

Protecting the data stored in PersistentVolumes is critical. Strategies vary depending on the storage type, application, and recovery requirements (RPO/RTO).

1.  **Volume Snapshots (CSI):**
    *   **Concept:** Leverage the Container Storage Interface (CSI) drivers provided by your storage backend (e.g., AWS EBS CSI Driver, GCE PD CSI Driver) to create point-in-time snapshots of PersistentVolumes at the storage system level.
    *   **Kubernetes Objects:** Uses `VolumeSnapshotClass`, `VolumeSnapshot`, and `VolumeSnapshotContent` CRDs (part of the Kubernetes storage API).
    *   **Pros:** Application-agnostic (usually). Fast backups and restores (often block-level). Consistent if the filesystem is frozen or the application quiesced during the snapshot.
    *   **Cons:** Requires a CSI driver that supports snapshots. Consistency might require application coordination (e.g., freezing writes). Restoring might involve creating a new PV/PVC from the snapshot.
    *   **Tools:** `kubectl` can manage snapshot objects. Tools like Velero can orchestrate snapshot creation.

2.  **Application-Level Backups:**
    *   **Concept:** Use the database's or application's native backup tools (e.g., `pg_dump` for PostgreSQL, `mysqldump` for MySQL, `mongodump` for MongoDB) to create logical backups.
    *   **Implementation:** Often run as a Kubernetes `CronJob` or `Job` that:
        *   Executes the dump command inside the database Pod (`kubectl exec`) or runs a client Pod that connects to the database Service.
        *   Streams or copies the backup file to external storage (e.g., S3, GCS, Azure Blob) using tools like `aws s3 cp`, `gsutil cp`, or `azcopy`.
    *   **Pros:** Application-consistent backups. Flexible format (SQL, binary). Allows point-in-time recovery (PITR) if combined with WAL archiving (for databases like PostgreSQL). Portable across different storage systems.
    *   **Cons:** Can be slower than snapshots for large datasets. Restore process involves setting up a new instance and importing the dump. Requires managing credentials for external storage.

3.  **Velero (Formerly Heptio Ark):**
    *   **Concept:** An open-source tool specifically designed for backing up and restoring Kubernetes cluster resources and PersistentVolumes.
    *   **How it works:**
        *   Installs server components into the cluster.
        *   Uses the Kubernetes API to back up object definitions (Deployments, Services, CRs, etc.).
        *   Integrates with cloud provider APIs (or uses Restic for filesystem-level backups) to back up PersistentVolume data (via snapshots or filesystem copies) to object storage (S3, GCS, Azure Blob).
    *   **Pros:** Comprehensive cluster backup (resources + data). Handles CRDs. Can migrate resources between clusters. Pluggable backend support.
    *   **Cons:** Requires setup and configuration. Backup time depends on the chosen PV backup method (snapshots vs. Restic).

**Choosing a Strategy:** Often, a combination is best. Use Volume Snapshots for fast disaster recovery and application-level dumps for granular restores or PITR. Velero provides a good orchestration layer over these techniques.

## Handling Data Migrations

When deploying new versions of stateful applications, you often need to update the database schema or migrate data. This needs careful handling in Kubernetes.

**Common Approaches:**

1.  **Kubernetes Jobs or Init Containers:**
    *   Create a Kubernetes `Job` as part of your deployment process (or before upgrading the main application).
    *   The Job runs a Pod using a container image containing your migration tool (e.g., Flyway, Liquibase, Alembic, custom scripts) and database client libraries.
    *   The Job's Pod connects to the database Service and applies the necessary schema changes or data migrations.
    *   Configure the main application deployment to only proceed *after* the migration Job completes successfully.
    *   Alternatively, use an `Init Container` within the application Pod itself to run migrations before the main application container starts. This ensures migrations run before the app starts but couples migration logic tightly with the application Pod lifecycle.

2.  **Application-Managed Migrations:**
    *   The application code itself checks the current schema version on startup and applies necessary migrations before becoming ready (passing its readiness probe).
    *   Requires careful implementation to handle concurrent startups if multiple replicas start simultaneously (e.g., using database advisory locks or a dedicated migration table).

3.  **External Migration Tools:**
    *   Run migration tools from outside the cluster (e.g., from a CI/CD pipeline runner) that connect directly to the database (if exposed externally, e.g., via LoadBalancer or NodePort, or through a bastion host/VPN).

**Considerations:**

*   **Atomicity:** Ensure migrations are atomic or idempotent (safe to run multiple times).
*   **Rollback:** Have a plan for rolling back migrations if they fail or cause issues.
*   **Zero-Downtime:** For critical applications, migrations might need to be backward-compatible to allow the old and new application versions to run simultaneously during a rolling update. This often involves multi-step migrations (e.g., add new column -> deploy new code using new column -> migrate data -> deploy code removing old column usage -> remove old column).

## High Availability for Stateful Apps

Achieving high availability (HA) for stateful applications involves redundancy at multiple levels:

*   **Multiple Pod Replicas:** Run multiple instances (replicas) of your stateful application using a StatefulSet.
*   **Anti-Affinity:** Use Pod anti-affinity rules (`spec.template.spec.affinity.podAntiAffinity`) to encourage Kubernetes to schedule replicas on different nodes, preventing a single node failure from taking down all instances.
    ```yaml
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution: # Or preferred...
        - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - my-database # Match pods with the same app label
          topologyKey: "kubernetes.io/hostname" # Spread across nodes
    ```
*   **Pod Disruption Budgets (PDBs):** Create a `PodDisruptionBudget` to limit the number of Pods from your StatefulSet that can be voluntarily disrupted (e.g., during node maintenance or drains) simultaneously, ensuring a minimum number remain available.
    ```yaml
    apiVersion: policy/v1
    kind: PodDisruptionBudget
    metadata:
      name: mydb-pdb
    spec:
      minAvailable: 2 # Or maxUnavailable: 1
      selector:
        matchLabels:
          app: my-database
    ```
*   **Application-Level Replication:** Configure replication within the application itself (e.g., PostgreSQL primary/standby replication, MongoDB replica sets, Kafka cluster replication). The StatefulSet provides the stable identity needed for this.
*   **Multi-AZ Deployments:**
    *   Ensure your Kubernetes cluster spans multiple Availability Zones (AZs) in your cloud region.
    *   Configure your StatefulSet and storage (PVs/StorageClass) to distribute Pods and their storage across these AZs using topology-aware scheduling (e.g., `topologyKey` in anti-affinity, `volumeBindingMode: WaitForFirstConsumer` in StorageClass). This protects against an entire AZ failure.

## Lab: Deploy a PostgreSQL Cluster with Backups

This lab demonstrates deploying a simplified PostgreSQL primary/standby setup using a StatefulSet and then performing a logical backup using `pg_dump` via `kubectl exec`.

**Prerequisites:**

*   `kubectl` connected to a running Kubernetes cluster.
*   A default StorageClass available (check with `kubectl get sc`).

**Steps:**

1.  **Create PostgreSQL ConfigMap (Optional but good practice):**
    Save as `postgres-config.yaml`:
    ```yaml
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: postgres-init-config
    data:
      init.sh: |
        #!/bin/bash
        set -e
        # Simple script placeholder - real HA setup is more complex
        # involving replication configuration based on ordinal index etc.
        echo "Initializing PostgreSQL instance..."
        # In a real setup, check if $PGDATA is empty, run initdb if needed,
        # configure replication based on hostname (e.g., postgres-db-0 is primary)
        # For this lab, we assume the image entrypoint handles basic init.
        ls -l $PGDATA
        echo "Initialization script finished."
    ```
    Apply: `kubectl apply -f postgres-config.yaml`

2.  **Create StatefulSet and Service Manifest:**
    Save as `postgres-sts-ha.yaml`: (This is simplified, real HA needs more config)
    ```yaml
    apiVersion: v1
    kind: Service # Headless service
    metadata:
      name: postgres-headless
    spec:
      clusterIP: None
      selector:
        app: postgres-sts-ha
      ports:
      - port: 5432
        name: postgres
    ---
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
      name: postgres-db-ha
    spec:
      serviceName: "postgres-headless"
      replicas: 2 # Deploy a primary (0) and a standby (1)
      selector:
        matchLabels:
          app: postgres-sts-ha
      template:
        metadata:
          labels:
            app: postgres-sts-ha
        spec:
          terminationGracePeriodSeconds: 10
          # Add Anti-Affinity
          affinity:
            podAntiAffinity:
              preferredDuringSchedulingIgnoredDuringExecution:
              - weight: 100
                podAffinityTerm:
                  labelSelector:
                    matchExpressions:
                    - key: app
                      operator: In
                      values:
                      - postgres-sts-ha
                  topologyKey: "kubernetes.io/hostname"
          # Init container could be used for complex setup/replication config
          # initContainers:
          # - name: init-postgres
          #   image: postgres:13
          #   command: ["/bin/bash", "/scripts/init.sh"]
          #   env:
          #     - name: POD_NAME
          #       valueFrom: { fieldRef: { fieldPath: metadata.name } }
          #     # Add other env vars needed by init script
          #   volumeMounts:
          #   - name: postgres-init-scripts
          #     mountPath: /scripts
          #   - name: postgres-data
          #     mountPath: /var/lib/postgresql/data
          containers:
          - name: postgres
            image: postgres:13
            ports:
            - containerPort: 5432
              name: postgres
            env:
            - name: POSTGRES_PASSWORD
              value: "pgpassword" # Use Secrets!
            - name: PGDATA
              value: /var/lib/postgresql/data/pgdata
            volumeMounts:
            - name: postgres-data
              mountPath: /var/lib/postgresql/data
            # Basic readiness probe
            readinessProbe:
              exec:
                command: ["psql", "-U", "postgres", "-c", "SELECT 1"]
              initialDelaySeconds: 15
              periodSeconds: 10
              timeoutSeconds: 5
          volumes: # Volume for init script ConfigMap
          - name: postgres-init-scripts
            configMap:
              name: postgres-init-config
              defaultMode: 0755 # Make script executable
      volumeClaimTemplates:
      - metadata:
          name: postgres-data
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 1Gi # Small size for demo
          # storageClassName: standard
    ```
    Apply: `kubectl apply -f postgres-sts-ha.yaml`

3.  **Verify Deployment:**
    Wait for both Pods (`postgres-db-ha-0`, `postgres-db-ha-1`) to become ready.
    ```bash
    kubectl get statefulset postgres-db-ha
    kubectl get pods -l app=postgres-sts-ha -w # Wait for READY 1/1 for both
    kubectl get pvc -l app=postgres-sts-ha # See two PVCs created
    ```

4.  **Perform a Backup (Logical Dump):**
    We'll connect to the primary Pod (`postgres-db-ha-0`) and run `pg_dumpall`.
    ```bash
    # Exec into the primary pod and run pg_dumpall, redirecting output locally
    kubectl exec postgres-db-ha-0 \
      -- env PGPASSWORD=pgpassword \ # Pass password via env var
      pg_dumpall -U postgres > backup_$(date +%Y%m%d_%H%M%S).sql

    echo "Backup created:"
    ls -lh backup_*.sql
    ```
    This creates a SQL file containing the logical dump of all databases on your local machine. In a real scenario, this command would run in a CronJob and upload the backup to S3/GCS/etc.

5.  **Simulate Restore (Conceptual):**
    A restore would typically involve:
    *   Starting a *new* PostgreSQL instance (potentially a fresh StatefulSet or Pod).
    *   Using `kubectl exec` or a client Pod to run `psql -U postgres < backup_file.sql` against the new instance to import the data.

6.  **Clean Up:**
    ```bash
    kubectl delete statefulset postgres-db-ha
    kubectl delete service postgres-headless
    kubectl delete configmap postgres-init-config
    # IMPORTANT: Delete PVCs manually
    kubectl delete pvc postgres-data-postgres-db-ha-0 postgres-data-postgres-db-ha-1
    rm backup_*.sql # Remove local backup file
    ```

**Congratulations!** You've deployed a multi-replica stateful application using a StatefulSet and performed a basic logical backup. While this lab simplifies HA configuration, it demonstrates the core principles of managing stateful workloads, including stable identity, persistent storage per replica, and application-level backup procedures within Kubernetes. For production HA and backups, consider using dedicated Operators and tools like Velero or Volume Snapshots.
