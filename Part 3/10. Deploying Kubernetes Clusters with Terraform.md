# Chapter 10: Deploying Kubernetes Clusters with Terraform

In the previous chapters, we learned the fundamentals of Terraform and used it to provision basic cloud infrastructure like VPCs and subnets. Now, we'll apply this knowledge to one of the most common use cases for Terraform in the Kubernetes ecosystem: provisioning managed Kubernetes clusters provided by cloud vendors.

## Provisioning Managed Clusters (EKS, GKE, AKS)

Cloud providers offer managed Kubernetes services that handle the complexity of setting up and operating the Kubernetes control plane (API server, etcd, scheduler, controller manager). This allows teams to focus on deploying and managing their applications rather than the underlying cluster infrastructure.

*   **Amazon Elastic Kubernetes Service (EKS):** AWS's managed Kubernetes service.
*   **Google Kubernetes Engine (GKE):** Google Cloud's managed Kubernetes service.
*   **Azure Kubernetes Service (AKS):** Microsoft Azure's managed Kubernetes service.

Using Terraform to provision these managed clusters offers significant advantages:

*   **Consistency:** Define your cluster configuration (version, node types, networking) in code, ensuring consistency across environments (dev, staging, prod).
*   **Repeatability:** Easily spin up identical clusters for testing or disaster recovery.
*   **Integration:** Manage the cluster alongside its dependent cloud resources (VPCs, IAM roles, load balancers) within the same Terraform configuration.
*   **Lifecycle Management:** Use `terraform apply` to create or update clusters and `terraform destroy` to tear them down cleanly.

**Terraform Modules for Managed Clusters:**

While you *can* define all the individual cloud resources needed for a managed cluster directly in Terraform, it's often complex. Thankfully, HashiCorp and the cloud providers maintain high-quality, official Terraform modules that abstract much of this complexity:

*   **EKS:** [`terraform-aws-modules/eks/aws`](https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/)
*   **GKE:** [`terraform-google-modules/kubernetes-engine/google`](https://registry.terraform.io/modules/terraform-google-modules/kubernetes-engine/google)
*   **AKS:** [`Azure/aks/azurerm`](https://registry.terraform.io/modules/Azure/aks/azurerm) (Community supported, widely used)

These modules encapsulate best practices and simplify cluster creation by exposing configurable input variables for common settings. We strongly recommend using these modules for provisioning managed clusters.

## Configuring Clusters with Terraform Modules

When using modules like `terraform-aws-modules/eks/aws`, you configure the cluster by setting input variables for the module. Common configuration aspects include:

*   **Cluster Identification:** Name, tags.
*   **Kubernetes Version:** Specifying the desired K8s version (e.g., `1.27`).
*   **Networking:**
    *   VPC ID and Subnet IDs: Often requires providing the IDs of the VPC and subnets (public and/or private) where the cluster control plane and worker nodes should reside. You can use outputs from a VPC module or the resources created in the previous chapter's lab.
    *   API Endpoint Access: Configuring whether the K8s API server endpoint is public, private, or both.
    *   Security Groups: Defining network access rules.
*   **IAM Roles/Service Accounts:** Modules typically create the necessary IAM roles for the EKS control plane and node groups, but you might need to customize permissions.
*   **Node Groups (Worker Nodes):**
    *   **Instance Types:** Specifying the VM types for worker nodes (e.g., `t3.medium`, `m5.large`).
    *   **Desired/Min/Max Size:** Configuring the number of nodes and autoscaling parameters.
    *   **Disk Size:** Setting the root volume size for nodes.
    *   **AMI Type:** Choosing the operating system image (e.g., Amazon Linux 2, Bottlerocket, Windows).
    *   **Labels & Taints:** Applying Kubernetes labels and taints to nodes within the group.
*   **Add-ons:** Configuring managed add-ons like CoreDNS, kube-proxy, VPC CNI (for EKS).

*Example Snippet using the EKS Module:*
```hcl
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0" # Use a recent compatible version

  cluster_name    = "my-k8s-course-cluster"
  cluster_version = "1.27"

  vpc_id     = module.vpc.vpc_id # Assuming a VPC module named 'vpc' exists
  subnet_ids = module.vpc.private_subnet_ids # Deploy nodes in private subnets

  # Define managed node groups
  eks_managed_node_groups = {
    general_purpose = {
      min_size     = 1
      max_size     = 3
      desired_size = 2

      instance_types = ["t3.medium"]
      # More configuration like disk size, labels, taints can go here
    }
    # Add more node groups if needed
    # spot_instances = { ... }
  }

  tags = {
    Environment = "Development"
    Project     = "K8s-Course"
  }
}

# Assume a VPC module is defined elsewhere, e.g.:
# module "vpc" {
#   source = "terraform-aws-modules/vpc/aws"
#   # ... VPC configuration ...
# }
```

## Integrating Terraform with Kubernetes Providers

Once the cluster is provisioned, you might want Terraform to also manage resources *inside* the cluster (e.g., installing an Ingress controller, creating namespaces, deploying applications). Terraform provides providers for this:

*   **Kubernetes Provider:** [`hashicorp/kubernetes`](https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs)
    *   Allows you to define Kubernetes objects like Deployments, Services, ConfigMaps, Secrets, etc., directly in HCL.
    *   Requires configuring authentication to the cluster API server, often using outputs from the cluster provisioning step.
*   **Helm Provider:** [`hashicorp/helm`](https://registry.terraform.io/providers/hashicorp/helm/latest/docs)
    *   Allows you to manage Helm chart releases using Terraform. Useful for deploying packaged applications or operators.
    *   Also requires Kubernetes cluster authentication.

*Example Kubernetes Provider Configuration:*
```hcl
data "aws_eks_cluster" "cluster" {
  name = module.eks.cluster_id # Get cluster details using a data source
}

data "aws_eks_cluster_auth" "cluster" {
  name = module.eks.cluster_id # Get auth token using a data source
}

provider "kubernetes" {
  host                   = data.aws_eks_cluster.cluster.endpoint
  cluster_ca_certificate = base64decode(data.aws_eks_cluster.cluster.certificate_authority.0.data)
  token                  = data.aws_eks_cluster_auth.cluster.token
}

# Now you can define Kubernetes resources
resource "kubernetes_namespace" "dev" {
  metadata {
    name = "development"
  }
}

resource "kubernetes_deployment" "nginx" {
  # ... deployment definition ...
  metadata {
    namespace = kubernetes_namespace.dev.metadata.0.name # Deploy in the created namespace
  }
  # ... spec ...
}
```

**Considerations:**

*   Using Terraform for Kubernetes resources ties infrastructure and application deployment together.
*   Alternatives like `kubectl apply`, Kustomize, Helm CLI, or GitOps tools (Argo CD, Flux) are often preferred for managing application deployments *within* the cluster, separating concerns.
*   Choose the approach that best fits your team's workflow and tooling.

## Managing Cluster Authentication and Kubeconfig

After Terraform provisions a cluster, you need to configure `kubectl` to connect to it. The cluster module or resources typically provide the necessary information as outputs:

*   **API Server Endpoint:** The URL of the Kubernetes API server.
*   **Cluster Certificate Authority (CA):** The certificate used to verify the API server's identity.
*   **Authentication Token/Method:** How to authenticate to the API server (e.g., temporary token, client certificate, cloud IAM integration).

You can use Terraform outputs to construct a `kubeconfig` file or use cloud provider CLI commands to update your local `kubeconfig`.

*Example Outputs from EKS Module:*
```hcl
output "cluster_endpoint" {
  description = "Kubernetes cluster endpoint"
  value       = module.eks.cluster_endpoint
}

output "cluster_certificate_authority_data" {
  description = "Base64 encoded cluster certificate authority data"
  value       = module.eks.cluster_certificate_authority_data
}

output "cluster_name" {
  description = "EKS Cluster name"
  value       = module.eks.cluster_id
}

output "configure_kubectl_command" {
  description = "Command to configure kubectl for this cluster (requires AWS CLI)"
  value       = "aws eks update-kubeconfig --region ${module.eks.cluster_region} --name ${module.eks.cluster_id}"
}
```

After running `terraform apply`, you can run the command from the `configure_kubectl_command` output (if you have the AWS CLI installed) to automatically update your `~/.kube/config` file.

## Lab: Deploy an EKS Cluster with Terraform

This lab guides you through deploying a basic AWS EKS cluster using the official `terraform-aws-modules/eks/aws` module.

**Prerequisites:**

*   Terraform installed (~> 1.0).
*   AWS Account.
*   AWS Credentials configured for Terraform (e.g., via environment variables or `aws configure`). Ensure the credentials have sufficient permissions to create EKS clusters, VPCs, IAM roles, EC2 instances, etc.
*   AWS CLI installed (optional, but useful for configuring `kubectl`).
*   `kubectl` installed.

**Steps:**

1.  **Create Project Directory:** `mkdir terraform-eks-lab && cd terraform-eks-lab`
2.  **Create `versions.tf`:**
    ```hcl
    # versions.tf
    terraform {
      required_version = ">= 1.0" # Specify minimum Terraform version

      required_providers {
        aws = {
          source  = "hashicorp/aws"
          version = "~> 4.16" # Or a newer compatible version
        }
      }
    }

    provider "aws" {
      region = var.aws_region
    }
    ```
3.  **Create `variables.tf`:**
    ```hcl
    # variables.tf
    variable "aws_region" {
      description = "AWS region for EKS cluster"
      type        = string
      default     = "us-west-2" # Choose a region that supports EKS
    }

    variable "cluster_name" {
      description = "Name for the EKS cluster and associated resources"
      type        = string
      default     = "k8s-course-eks-demo"
    }

    variable "cluster_version" {
      description = "Kubernetes version for the EKS cluster"
      type        = string
      default     = "1.27" # Choose a supported version
    }
    ```
4.  **Create `main.tf`:**
    ```hcl
    # main.tf

    # --- VPC ---
    # Use the official VPC module to create networking resources
    module "vpc" {
      source  = "terraform-aws-modules/vpc/aws"
      version = "~> 5.0" # Use a recent compatible version

      name = "${var.cluster_name}-vpc"
      cidr = "10.0.0.0/16"

      azs             = ["${var.aws_region}a", "${var.aws_region}b", "${var.aws_region}c"] # Use 3 AZs
      private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
      public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]

      enable_nat_gateway = true # Needed for private nodes to pull images/updates
      single_nat_gateway = true # Use one NAT GW for cost savings in demo

      # Tags required by EKS
      public_subnet_tags = {
        "kubernetes.io/cluster/${var.cluster_name}" = "shared"
        "kubernetes.io/role/elb"                  = "1"
      }
      private_subnet_tags = {
        "kubernetes.io/cluster/${var.cluster_name}" = "shared"
        "kubernetes.io/role/internal-elb"         = "1"
      }

      tags = {
        Environment = "Development"
        Terraform   = "true"
      }
    }

    # --- EKS Cluster ---
    module "eks" {
      source  = "terraform-aws-modules/eks/aws"
      version = "~> 19.0" # Use a recent compatible version

      cluster_name    = var.cluster_name
      cluster_version = var.cluster_version

      vpc_id     = module.vpc.vpc_id
      subnet_ids = module.vpc.private_subnets # Deploy control plane and nodes in private subnets

      # Managed Node Group Configuration
      eks_managed_node_group_defaults = {
        ami_type = "AL2_x86_64" # Amazon Linux 2
      }

      eks_managed_node_groups = {
        initial_nodes = {
          name           = "general-purpose-nodes"
          instance_types = ["t3.medium"] # Choose appropriate instance type
          min_size       = 1
          max_size       = 3
          desired_size   = 2
        }
      }

      # Cluster access configuration (optional, allows your IAM user/role to administer)
      # manage_aws_auth_configmap = true # Let module manage aws-auth
      # aws_auth_users = [
      #   {
      #     userarn = "arn:aws:iam::ACCOUNT_ID:user/YOUR_IAM_USER_NAME" # Replace with your user ARN
      #     username = "YOUR_IAM_USER_NAME"
      #     groups   = ["system:masters"]
      #   }
      # ]

      tags = {
        Environment = "Development"
        Terraform   = "true"
      }
    }
    ```
5.  **Create `outputs.tf`:**
    ```hcl
    # outputs.tf
    output "cluster_name" {
      description = "EKS cluster name"
      value       = module.eks.cluster_name
    }

    output "cluster_endpoint" {
      description = "Endpoint for EKS control plane"
      value       = module.eks.cluster_endpoint
    }

    output "cluster_security_group_id" {
      description = "Security group ids attached to the cluster control plane"
      value       = module.eks.cluster_security_group_id
    }

    output "region" {
      description = "AWS region"
      value       = var.aws_region
    }

    output "configure_kubectl" {
      description = "Configure kubectl: make sure you're logged in with the correct AWS profile and run the following command"
      value       = "aws eks update-kubeconfig --region ${var.aws_region} --name ${module.eks.cluster_name}"
    }
    ```
6.  **Run Terraform Workflow:**
    ```bash
    terraform init
    terraform validate
    terraform plan
    # Review the plan carefully - it will create a VPC, subnets, NAT GW, IGW, EKS control plane, IAM roles, Node Group, etc.
    terraform apply # Type 'yes'. This will take several minutes (10-20+ mins).
    ```

7.  **Configure `kubectl`:**
    Once `apply` is complete, copy the command from the `configure_kubectl` output and run it in your terminal:
    ```bash
    aws eks update-kubeconfig --region <your-region> --name <your-cluster-name>
    # Example: aws eks update-kubeconfig --region us-west-2 --name k8s-course-eks-demo
    ```

8.  **Verify Cluster Access:**
    ```bash
    kubectl get nodes
    # Should show the worker nodes from your EKS node group in Ready status.
    kubectl get svc
    # Should show the default kubernetes service.
    ```

9.  **Clean Up:**
    **Important:** EKS clusters and associated resources incur costs. Destroy them when finished.
    ```bash
    terraform destroy # Type 'yes'. This will also take several minutes.
    ```

**Congratulations!** You have successfully provisioned a managed Kubernetes cluster (EKS) on AWS using Terraform and its powerful module ecosystem. You've seen how to configure networking, node groups, and obtain access credentials, all defined as code. This is a fundamental skill for managing Kubernetes infrastructure reliably and scalably in the cloud.
