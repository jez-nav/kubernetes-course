# Chapter 9: Terraform Basics for Kubernetes

Chapter 8 introduced Terraform and its basic workflow. Now, we'll build upon that foundation, exploring core concepts essential for managing real-world infrastructure, particularly the cloud resources needed for Kubernetes clusters. We'll cover the workflow in more detail, provider configuration, parameterizing configurations with variables, exposing information with outputs, organizing code with modules, and crucially, managing Terraform's state.

## Terraform Workflow Revisited

The core Terraform workflow consists of a few key commands you'll use constantly:

1.  **`terraform init`**:
    *   **Purpose:** Initializes a working directory containing Terraform configuration files.
    *   **Actions:**
        *   Downloads and installs provider plugins declared in the `required_providers` block.
        *   Downloads modules referenced in the configuration.
        *   Initializes the backend (for state storage, discussed later).
    *   **When to run:** Run this command once when you start a new project, clone an existing project, or add/change providers or modules. It's safe to run multiple times.

2.  **`terraform validate`**:
    *   **Purpose:** Checks if the configuration syntax is valid and internally consistent.
    *   **Actions:** Performs static analysis of your `.tf` files, checking for syntax errors, incorrect argument types, undefined variables, etc. It does *not* check external APIs or current infrastructure state.
    *   **When to run:** Run frequently during development to catch errors early, especially before committing code or running `plan`.

3.  **`terraform plan`**:
    *   **Purpose:** Creates an execution plan, showing what actions Terraform *would* take to make the real infrastructure match the desired state defined in your configuration. **This is a crucial safety step.**
    *   **Actions:**
        *   Refreshes the Terraform state file to match the real-world infrastructure (unless `-refresh=false` is used).
        *   Compares the current state with the desired configuration.
        *   Determines the necessary create, update, or destroy actions.
        *   Outputs a summary of the planned changes (+ add, ~ change, - destroy).
    *   **When to run:** Always run `plan` before `apply` to understand the impact of your changes. You can save a plan to a file (`terraform plan -out=tfplan`) and apply that specific plan later (`terraform apply tfplan`) for consistency.

4.  **`terraform apply`**:
    *   **Purpose:** Executes the actions proposed in the execution plan to reach the desired state.
    *   **Actions:**
        *   By default, runs a `plan` first and asks for confirmation before proceeding.
        *   Interacts with provider APIs (e.g., AWS, GCP, Azure) to create, update, or delete resources.
        *   Updates the Terraform state file to reflect the new state of the infrastructure.
    *   **When to run:** Run after reviewing and approving the output of `terraform plan`. Can also apply a saved plan file directly (`terraform apply tfplan`).

5.  **`terraform destroy`**:
    *   **Purpose:** Destroys all remote resources managed by the current Terraform configuration.
    *   **Actions:** Creates a plan to destroy all tracked resources and asks for confirmation. Executes the destruction plan upon approval. Updates the state file (often leaving it mostly empty).
    *   **When to run:** Use carefully when you want to tear down the infrastructure defined in the configuration (e.g., cleaning up a temporary environment).

## Managing Cloud Providers (AWS, GCP, Azure)

To manage resources in a specific cloud, you need to declare and configure the corresponding provider.

**Provider Configuration:**

Configuration happens within the `provider "<name>" {}` block. The primary task is usually authentication.

**Authentication Methods (General Principles):**

Terraform providers typically support multiple ways to authenticate, often checking in this order:

1.  **Explicit Credentials in Configuration:** (e.g., `access_key`, `secret_key` directly in the `provider` block). **Strongly discouraged** for security reasons â€“ never commit credentials to version control.
2.  **Environment Variables:** Standard variables recognized by the provider (e.g., `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `GOOGLE_CREDENTIALS`, `ARM_CLIENT_ID`, etc.). A common and secure method for CI/CD pipelines or local development.
3.  **Shared Credential/Configuration Files:** Default files used by cloud provider CLIs (e.g., `~/.aws/credentials`, `~/.azure/credentials`, `gcloud auth application-default login` credentials). Convenient for local development if you already use the cloud CLI.
4.  **IAM Roles/Service Accounts (on Cloud Compute):** If Terraform runs on a VM or container within the cloud (e.g., an EC2 instance, GCE VM, AKS Pod), it can often automatically inherit permissions from the instance's assigned IAM role or service account. **This is the recommended approach for running Terraform within cloud environments.**

**Example Provider Configurations:**

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.16" # Use a recent version
    }
    google = {
      source  = "hashicorp/google"
      version = "~> 4.0"
    }
    azurerm = {
      source  = "hashicorp/azurerm"
      version = "~> 3.0"
    }
  }
}

# --- AWS Provider ---
provider "aws" {
  region = "us-east-1" # Specify the desired AWS region

  # Authentication: Choose ONE method below (or rely on env vars/shared files/IAM role)
  # access_key = "..." # Avoid this
  # secret_key = "..." # Avoid this

  # Example using a specific profile from ~/.aws/credentials
  # profile = "my-tf-profile"
}

# --- Google Cloud Provider ---
provider "google" {
  project = "my-gcp-project-id" # Your GCP Project ID
  region  = "us-central1"
  zone    = "us-central1-c"

  # Authentication: Choose ONE (or rely on env vars/gcloud auth/service account)
  # credentials = file("path/to/service-account-key.json") # Avoid if possible
}

# --- Azure Provider ---
provider "azurerm" {
  features {} # Required empty block

  # Authentication: Choose ONE (or rely on env vars/az login/managed identity)
  # subscription_id = "..."
  # client_id       = "..."
  # client_secret   = "..." # Avoid this
  # tenant_id       = "..."

  # Example using Azure CLI login
  # (Run `az login` before running terraform)
}

```

**Recommendation:** For local development, use environment variables or shared credential files. For CI/CD or running within the cloud, use IAM roles or service accounts. Avoid hardcoding credentials.

## Variables, Outputs, and Modules

Hardcoding values like region names, instance sizes, or IP ranges directly into resource blocks makes configurations inflexible and hard to reuse. Terraform provides mechanisms to parameterize and organize your code:

**Input Variables:**

*   **Purpose:** Parameterize your configuration, allowing you to pass in values from outside (CLI, `.tfvars` files, environment variables).
*   **Declaration (`variables.tf` or any `.tf` file):**
    ```hcl
    variable "aws_region" {
      description = "The AWS region to deploy resources in."
      type        = string # Specify the data type (string, number, bool, list, map, object, any)
      default     = "us-west-2" # Optional default value
      # validation { ... } # Optional validation rules
      # sensitive = true # Optional: Mark as sensitive to hide from output
    }

    variable "instance_type" {
      description = "EC2 instance type"
      type        = string
      # No default - value must be provided
    }

    variable "subnet_cidrs" {
      description = "List of CIDR blocks for subnets"
      type        = list(string)
      default     = []
    }
    ```
*   **Usage:** Reference variables using `var.<variable_name>`.
    ```hcl
    provider "aws" {
      region = var.aws_region # Use the variable value
    }

    resource "aws_instance" "example" {
      ami           = "ami-0c55b159cbfafe1f0" # Example AMI
      instance_type = var.instance_type # Use the variable value
    }
    ```
*   **Setting Values:** Terraform looks for variable values in this order:
    1.  Command-line flags: `-var="aws_region=eu-west-1"` or `-var="subnet_cidrs=[\"10.0.1.0/24\", \"10.0.2.0/24\"]"`
    2.  `.tfvars` files: Files named `*.auto.tfvars` are loaded automatically. Others can be specified with `-var-file=myvars.tfvars`.
    3.  Environment variables: `TF_VAR_<variable_name>` (e.g., `TF_VAR_instance_type=t3.micro`).
    4.  Default values in the `variable` block.
    5.  If no value is found, Terraform prompts the user interactively (unless in non-interactive mode).

**Output Values:**

*   **Purpose:** Expose information about the managed infrastructure after `terraform apply` runs. Useful for displaying connection details, resource IDs, or passing data between configurations.
*   **Declaration (`outputs.tf` or any `.tf` file):**
    ```hcl
    output "instance_public_ip" {
      description = "Public IP address of the EC2 instance."
      value       = aws_instance.example.public_ip # Reference an attribute of a managed resource
      # sensitive = true # Optional: Mark as sensitive
    }

    output "vpc_id" {
      description = "ID of the created VPC."
      value       = aws_vpc.main.id
    }
    ```
*   **Viewing Outputs:** Run `terraform output` after applying, or see them displayed at the end of `terraform apply`.

**Modules:**

*   **Purpose:** Encapsulate a set of related resources into a reusable, configurable unit. Promotes code organization, reuse, and maintainability. Think of them like functions or classes for infrastructure.
*   **Structure:** A module is simply a directory containing `.tf` files (often including `variables.tf`, `main.tf`, `outputs.tf`).
*   **Usage (Calling a Module):**
    ```hcl
    # In your root configuration (e.g., main.tf)

    module "my_vpc" {
      source = "./modules/vpc" # Path to the module directory (local)
      # source = "terraform-aws-modules/vpc/aws" # Source from Terraform Registry
      # version = "~> 3.0" # Specify version for remote modules

      # Pass input variables to the module
      aws_region    = var.aws_region
      vpc_cidr      = "10.0.0.0/16"
      num_subnets   = 3
    }

    # Access outputs from the module
    resource "aws_instance" "example" {
      # ...
      subnet_id = module.my_vpc.public_subnet_ids[0] # Use an output from the 'my_vpc' module
    }
    ```
*   **Benefits:** Reduces code duplication, standardizes resource creation, simplifies complex configurations. You can use community modules from the [Terraform Registry](https://registry.terraform.io/) or create your own.

## State Management and Remote Backends

Terraform needs to keep track of the infrastructure it manages. It does this using a **state file** (usually `terraform.tfstate`).

*   **Purpose of State:**
    *   Maps resources defined in your configuration to real-world objects.
    *   Stores metadata about managed resources (e.g., resource IDs).
    *   Tracks dependencies between resources.
    *   Improves performance by caching resource attributes.
    *   Enables detection of infrastructure drift.

**Local State (Default):**

*   By default, Terraform stores the state file locally in your working directory (`terraform.tfstate`).
*   **Problems:**
    *   **Collaboration:** If multiple people work on the same infrastructure, keeping local state files synchronized is difficult and error-prone.
    *   **Secrets:** The state file can contain sensitive information (passwords, keys) in plain text. Committing it to Git is a major security risk.
    *   **Locking:** If two people run `terraform apply` simultaneously using local state, they can overwrite each other's changes, leading to corruption.
    *   **Availability:** If your local machine is lost, the state file is lost.

**Remote Backends:**

*   **Solution:** Store the state file remotely in a shared, persistent location.
*   **Benefits:**
    *   **Collaboration:** Provides a central, shared location for state.
    *   **Security:** Many backends support encryption at rest. Avoids committing state to Git.
    *   **Locking:** Most remote backends support **state locking**, preventing concurrent `apply` operations and ensuring consistency.
    *   **Availability & Durability:** Leverages robust storage services (e.g., S3, Azure Blob, GCS).
*   **Common Backends:**
    *   AWS S3 (often with DynamoDB for locking)
    *   Azure Blob Storage
    *   Google Cloud Storage (GCS)
    *   HashiCorp Terraform Cloud / Terraform Enterprise
*   **Configuration (`backend.tf` or any `.tf` file):**
    ```hcl
    # Example: S3 backend configuration
    terraform {
      backend "s3" {
        bucket         = "my-terraform-state-bucket-unique-name" # Your S3 bucket name
        key            = "global/s3/terraform.tfstate" # Path within the bucket
        region         = "us-east-1"
        dynamodb_table = "my-terraform-state-lock-table" # DynamoDB table for locking
        encrypt        = true # Enable server-side encryption
      }
      # required_providers { ... } # Keep provider declarations
    }
    ```
*   **Initialization:** After configuring a backend, run `terraform init`. Terraform will prompt you to migrate your existing local state (if any) to the remote backend.

**Recommendation:** **Always use a remote backend** with state locking for any non-trivial Terraform project, especially when working in a team.

## Lab: Create a VPC and Subnets for a Kubernetes Cluster

This lab uses Terraform with the AWS provider to create a basic Virtual Private Cloud (VPC) and associated subnets, demonstrating variables, outputs, and provider configuration.

**Prerequisites:**

*   Terraform installed.
*   AWS Account.
*   AWS Credentials configured for Terraform (e.g., via environment variables `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, `AWS_REGION`, or using `aws configure` and relying on `~/.aws/credentials`).

**Steps:**

1.  **Create Project Directory:** `mkdir terraform-vpc-lab && cd terraform-vpc-lab`
2.  **Create `providers.tf`:**
    ```hcl
    # providers.tf
    terraform {
      required_providers {
        aws = {
          source  = "hashicorp/aws"
          version = "~> 4.16"
        }
      }
    }

    provider "aws" {
      region = var.aws_region # Use variable for region
    }
    ```
3.  **Create `variables.tf`:**
    ```hcl
    # variables.tf
    variable "aws_region" {
      description = "AWS region for deployment"
      type        = string
      default     = "us-west-2" # Or your preferred region
    }

    variable "vpc_cidr" {
      description = "CIDR block for the VPC"
      type        = string
      default     = "10.0.0.0/16"
    }

    variable "public_subnet_cidrs" {
      description = "List of CIDR blocks for public subnets"
      type        = list(string)
      default     = ["10.0.1.0/24", "10.0.2.0/24"]
    }

    variable "private_subnet_cidrs" {
      description = "List of CIDR blocks for private subnets"
      type        = list(string)
      default     = ["10.0.101.0/24", "10.0.102.0/24"]
    }

    variable "project_name" {
      description = "Base name for resources"
      type        = string
      default     = "k8s-course-vpc"
    }
    ```
4.  **Create `main.tf`:**
    ```hcl
    # main.tf

    # --- VPC ---
    resource "aws_vpc" "main" {
      cidr_block = var.vpc_cidr
      enable_dns_support   = true
      enable_dns_hostnames = true

      tags = {
        Name = "${var.project_name}-vpc"
      }
    }

    # --- Internet Gateway for Public Subnets ---
    resource "aws_internet_gateway" "gw" {
      vpc_id = aws_vpc.main.id

      tags = {
        Name = "${var.project_name}-igw"
      }
    }

    # --- Public Subnets ---
    resource "aws_subnet" "public" {
      count             = length(var.public_subnet_cidrs) # Create one subnet per CIDR in the list
      vpc_id            = aws_vpc.main.id
      cidr_block        = var.public_subnet_cidrs[count.index]
      availability_zone = data.aws_availability_zones.available.names[count.index % length(data.aws_availability_zones.available.names)] # Distribute across AZs
      map_public_ip_on_launch = true # Instances launched here get public IPs

      tags = {
        Name = "${var.project_name}-public-subnet-${count.index + 1}"
      }
    }

    # --- Public Route Table ---
    resource "aws_route_table" "public" {
      vpc_id = aws_vpc.main.id

      route {
        cidr_block = "0.0.0.0/0" # Route traffic for the internet
        gateway_id = aws_internet_gateway.gw.id # Through the Internet Gateway
      }

      tags = {
        Name = "${var.project_name}-public-rt"
      }
    }

    # --- Public Route Table Associations ---
    resource "aws_route_table_association" "public" {
      count          = length(aws_subnet.public)
      subnet_id      = aws_subnet.public[count.index].id
      route_table_id = aws_route_table.public.id
    }

    # --- Private Subnets ---
    resource "aws_subnet" "private" {
      count             = length(var.private_subnet_cidrs)
      vpc_id            = aws_vpc.main.id
      cidr_block        = var.private_subnet_cidrs[count.index]
      availability_zone = data.aws_availability_zones.available.names[count.index % length(data.aws_availability_zones.available.names)]

      tags = {
        Name = "${var.project_name}-private-subnet-${count.index + 1}"
      }
    }

    # --- Data source to get available AZs in the region ---
    data "aws_availability_zones" "available" {}

    # Note: For private subnets to reach the internet, you'd typically add a NAT Gateway
    # and a private route table pointing 0.0.0.0/0 to the NAT GW. Omitted here for brevity.
    ```
5.  **Create `outputs.tf`:**
    ```hcl
    # outputs.tf
    output "vpc_id" {
      description = "The ID of the created VPC"
      value       = aws_vpc.main.id
    }

    output "public_subnet_ids" {
      description = "List of IDs for the public subnets"
      value       = aws_subnet.public[*].id # Splat expression to get all IDs
    }

    output "private_subnet_ids" {
      description = "List of IDs for the private subnets"
      value       = aws_subnet.private[*].id
    }
    ```
6.  **Run the Terraform Workflow:**
    ```bash
    terraform init
    terraform validate
    terraform plan
    # Review the plan (should create VPC, IGW, Subnets, Route Tables, Associations)
    terraform apply # Type 'yes' to confirm
    ```
7.  **Check Outputs:**
    ```bash
    terraform output
    # Should display the VPC ID and lists of subnet IDs
    ```
    You can also verify the resources were created in the AWS Management Console.

8.  **Clean Up:**
    ```bash
    terraform destroy # Type 'yes' to confirm
    ```

**Congratulations!** You've used Terraform to provision core networking infrastructure in AWS, utilizing variables for parameterization and outputs to expose results. This forms the foundation upon which you can build Kubernetes clusters and other cloud resources. The next chapter will focus specifically on using Terraform to deploy managed Kubernetes clusters like EKS.
