# Chapter 13: Observability and Monitoring

Running applications in Kubernetes introduces complexity. Understanding what's happening inside your cluster and applications is crucial for troubleshooting issues, ensuring performance, and maintaining reliability. This is where **observability** comes in â€“ the ability to infer the internal state of a system from its external outputs. The three pillars of observability are typically considered **logs**, **metrics**, and **traces**. This chapter explores how Kubernetes handles these pillars and introduces common tools used for monitoring.

## Logging with Kubernetes

Logs provide detailed, event-specific information about what happened within an application or system component. Kubernetes provides basic logging capabilities out-of-the-box and enables more advanced patterns.

**1. Standard Output (stdout) and Standard Error (stderr):**

*   **Concept:** The simplest and most common approach in Kubernetes is for containerized applications to write their logs to standard output (`stdout`) and standard error (`stderr`).
*   **How it works:**
    *   The container runtime (e.g., Docker, containerd) captures these streams.
    *   The `kubelet` on the node reads these streams and typically writes them to files in a specific directory on the node (e.g., `/var/log/pods/...` or managed by journald).
    *   The `kubectl logs <pod-name> [-c <container-name>]` command retrieves these logs by querying the kubelet on the node where the Pod is running.
*   **Pros:** Simple for applications; requires no application-level changes if apps already log to stdout/stderr. Basic log access via `kubectl`.
*   **Cons:**
    *   **Node-Level Storage:** Logs are stored on the node. If the node fails or the Pod is deleted/rescheduled, the logs might be lost unless a node-level logging agent is collecting them.
    *   **Limited Querying/Aggregation:** `kubectl logs` is basic; you cannot easily search across multiple Pods or aggregate logs centrally.
    *   **No Rotation/Retention:** Kubernetes itself doesn't manage log rotation or long-term retention on the node; this is often handled by the node's OS or logging agent.

**2. Sidecar Pattern for Logging:**

*   **Concept:** Deploy a dedicated logging agent container (a **sidecar**) within the same Pod as the application container. The application writes logs to a file on a shared volume (e.g., `emptyDir`), and the sidecar container reads from this volume and forwards the logs to a centralized logging backend (e.g., Elasticsearch, Loki, Splunk, Datadog).
*   **How it works:**
    *   Define an `emptyDir` volume in the Pod spec.
    *   Mount this volume in both the application container (e.g., at `/var/log/app`) and the sidecar container (e.g., at `/mnt/log`).
    *   Configure the application to write logs to its mounted path (`/var/log/app/app.log`).
    *   Configure the sidecar container (e.g., running Fluentd, Fluent Bit, Vector) to read logs from its mounted path (`/mnt/log/app.log`) and ship them to the desired backend.
*   **Pros:** Decouples logging logic from the application. Allows use of specialized logging agents. Application doesn't need to know about the logging backend. Works even if the app cannot log to stdout/stderr.
*   **Cons:** Adds resource overhead (CPU/Memory) for the sidecar container in every Pod. Requires configuring the shared volume and sidecar.

**3. Node-Level Logging Agent:**

*   **Concept:** Deploy a logging agent (like Fluentd, Fluent Bit, Logstash) as a **DaemonSet**. This ensures the agent runs on every node in the cluster.
*   **How it works:**
    *   The DaemonSet Pod mounts host directories containing container logs (e.g., `/var/log/pods`, `/var/lib/docker/containers`) into the agent container.
    *   The agent reads logs directly from these node directories, enriches them with Kubernetes metadata (Pod name, namespace, labels), and forwards them to a central logging backend.
*   **Pros:** Centralized agent management (one per node, not per Pod). Lower resource overhead compared to sidecars for many Pods. Captures logs from all Pods on the node automatically (if they write to stdout/stderr).
*   **Cons:** Requires cluster-level permissions for the DaemonSet (accessing host paths). Configuration can be more complex than sidecars. Might miss logs if the agent fails temporarily.

**Recommendation:** Start with applications logging to `stdout`/`stderr`. Implement a node-level logging agent (DaemonSet) forwarding to a central backend (like Loki, Elasticsearch, or a cloud provider service) for aggregation, searching, and retention. Use the sidecar pattern only when applications cannot log to stdout/stderr or require specialized in-Pod processing.

## Metrics with Prometheus and Grafana

Metrics are numerical measurements of system behavior over time (time-series data), such as CPU usage, memory consumption, request latency, error rates, queue depths, etc. They are essential for understanding performance, identifying trends, and triggering alerts.

**Key Components:**

*   **Metrics Server:** (As discussed in Chapter 12) Provides basic resource metrics (CPU/Memory) for Pods and Nodes via the `metrics.k8s.io` API. Used by HPA and `kubectl top`.
*   **Exporters:** Applications or sidecar containers that expose metrics in a specific format (often the Prometheus exposition format) over an HTTP endpoint (e.g., `/metrics`). Many applications (like databases, message queues) have official or community exporters. Custom applications can use client libraries (e.g., Prometheus client libraries for Go, Python, Java) to instrument their code and expose metrics. `kube-state-metrics` is a crucial exporter that provides metrics about the state of Kubernetes objects themselves (Deployments, Pods, Nodes, etc.).
*   **Prometheus:** An open-source monitoring system and time-series database.
    *   **Pull Model:** Prometheus periodically scrapes (pulls) metrics from configured HTTP endpoints (exporters, application `/metrics` endpoints).
    *   **Storage:** Stores collected metrics efficiently in its time-series database.
    *   **PromQL:** Provides a powerful query language (PromQL) for selecting, aggregating, and analyzing metrics.
    *   **Alerting:** Includes Alertmanager for defining alerting rules based on PromQL expressions and routing notifications.
*   **Grafana:** An open-source platform for data visualization and analytics.
    *   **Data Sources:** Connects to various data sources, including Prometheus.
    *   **Dashboards:** Allows creating interactive dashboards with graphs, tables, gauges, etc., to visualize metrics queried from Prometheus (using PromQL).

**Typical Architecture:**

1.  Applications and infrastructure components expose metrics via HTTP endpoints (using exporters or built-in instrumentation).
2.  Prometheus is configured to discover and scrape these endpoints (often using Kubernetes service discovery mechanisms).
3.  Prometheus stores the metrics.
4.  Grafana is configured with Prometheus as a data source.
5.  Users create dashboards in Grafana, writing PromQL queries to visualize metrics from Prometheus.
6.  Prometheus evaluates alerting rules and sends alerts to Alertmanager, which routes them to notification channels (Slack, PagerDuty, email).

**Deployment:** Deploying Prometheus and Grafana can be done manually, but it's often simplified using the **Prometheus Operator** or community Helm charts (e.g., `kube-prometheus-stack`), which manage Prometheus, Alertmanager, Grafana, and common exporters declaratively using Custom Resource Definitions (CRDs).

## Tracing with Jaeger or Zipkin

While logs tell you *what* happened and metrics tell you *how much* or *how often*, **distributed tracing** tells you *why* something happened, especially in complex microservice architectures.

*   **Concept:** Tracing follows a single request as it propagates through multiple services in a distributed system. Each service adds context (a **span**) containing timing information and metadata. These spans are collected and assembled to visualize the entire request lifecycle, showing dependencies and latency bottlenecks.
*   **Key Components:**
    *   **Instrumentation:** Application code needs to be instrumented (using libraries like OpenTelemetry, OpenTracing, or Zipkin Brave) to start/stop spans and propagate trace context between service calls (usually via HTTP headers).
    *   **Trace Collector:** Receives spans sent from instrumented applications (e.g., Jaeger Agent/Collector, Zipkin Collector).
    *   **Trace Storage:** Stores the collected trace data (e.g., Elasticsearch, Cassandra, Jaeger's native storage).
    *   **Trace UI:** A web interface for querying and visualizing traces (e.g., Jaeger UI, Zipkin UI).
*   **Common Tools:**
    *   **Jaeger:** An open-source, end-to-end distributed tracing system (originated at Uber, now CNCF).
    *   **Zipkin:** Another popular open-source distributed tracing system (originated at Twitter).
    *   **OpenTelemetry (OTel):** A newer, vendor-neutral CNCF project providing a standard set of APIs, SDKs, and tools for generating telemetry data (traces, metrics, logs). Often used for instrumentation, allowing flexibility in choosing the backend (Jaeger, Zipkin, Prometheus, commercial vendors).

Implementing tracing requires code instrumentation, which is more involved than basic logging or metrics, but provides invaluable insights into request flows and performance issues in microservices.

## Kubernetes Dashboard and kubectl Debugging

Beyond dedicated observability tools, Kubernetes provides built-in ways to inspect and debug:

*   **Kubernetes Dashboard:** A general-purpose web UI for Kubernetes clusters. Allows users to manage applications, troubleshoot them, and manage the cluster itself. It provides views of Pods, Deployments, Services, logs, resource usage, etc. Needs to be deployed separately and requires careful consideration of authentication and authorization.
*   **`kubectl` Debugging Commands:**
    *   `kubectl get <resource> <name> -o yaml`: View the full YAML definition and status of an object.
    *   `kubectl describe <resource> <name>`: Provides human-readable details, including status, configuration, and recent **Events**. Events are crucial for diagnosing scheduling issues, probe failures, image pull errors, etc.
    *   `kubectl logs <pod-name> [-c <container>] [-f] [--previous]`: View container logs (live with `-f`, previous instance with `--previous`).
    *   `kubectl exec -it <pod-name> [-c <container>] -- <command>`: Execute commands inside a running container (e.g., `bash`, `ls`, `ping`, `curl`).
    *   `kubectl port-forward <pod-or-service-name> <local-port>:<remote-port>`: Access a Pod/Service port directly from your local machine.
    *   `kubectl top pod <pod-name> --containers`: View live CPU/Memory usage (requires Metrics Server).
    *   `kubectl top node <node-name>`: View live node CPU/Memory usage (requires Metrics Server).
    *   `kubectl debug node/<node-name> -it --image=busybox`: (Newer feature) Creates a temporary Pod on a specific node with host namespaces mounted, useful for node-level debugging.

## Lab: Deploy a Monitoring Stack with Prometheus and Grafana

This lab uses the `kube-prometheus-stack` Helm chart, which conveniently bundles the Prometheus Operator, Prometheus, Grafana, Alertmanager, node-exporter, and kube-state-metrics.

**Prerequisites:**

*   `kubectl` connected to a running Kubernetes cluster.
*   **Helm v3 installed.** ([https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/))

**Steps:**

1.  **Add Helm Repository:**
    ```bash
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo update
    ```

2.  **Install `kube-prometheus-stack`:**
    We'll install it into a dedicated `monitoring` namespace.
    ```bash
    kubectl create namespace monitoring

    # Install the chart (this might take a few minutes as images are pulled)
    helm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring
    # Output: Should indicate successful deployment
    ```
    *(Note: For production, you would customize values using `--values values.yaml` or `--set key=value`)*

3.  **Verify Deployments:**
    Check that Prometheus, Grafana, Alertmanager, node-exporter (DaemonSet), and kube-state-metrics Pods are running in the `monitoring` namespace.
    ```bash
    kubectl get pods -n monitoring
    # Wait until most pods are Running (prometheus-operator, grafana, prometheus-*, alertmanager-*, node-exporter-*, kube-state-metrics-*)
    kubectl get ds -n monitoring # Check node-exporter
    kubectl get statefulset -n monitoring # Check prometheus, alertmanager
    kubectl get deployment -n monitoring # Check grafana, kube-state-metrics, operator
    ```

4.  **Access Grafana:**
    The default chart installation exposes Grafana via a ClusterIP Service. We'll use port-forwarding to access it.
    ```bash
    # Find the Grafana service name (usually includes the release name 'prometheus')
    kubectl get svc -n monitoring | grep grafana
    # Example output: prometheus-grafana ClusterIP ... 80/TCP ...

    # Port-forward to the Grafana service (replace service name if different)
    kubectl port-forward service/prometheus-grafana 8080:80 -n monitoring
    # Output: Forwarding from 127.0.0.1:8080 -> 3000 (Grafana's default internal port is 3000, but the service exposes 80)
    #         Forwarding from [::1]:8080 -> 3000
    ```

5.  **Log in to Grafana:**
    *   Open your web browser and navigate to `http://localhost:8080`.
    *   The default username is `admin`.
    *   To get the default password, run:
        ```bash
        kubectl get secret --namespace monitoring prometheus-grafana -o jsonpath="{.data.admin-password}" | base64 --decode ; echo
        ```
    *   Copy the password and paste it into the Grafana login page. You might be prompted to change the password.

6.  **Explore Dashboards:**
    *   The `kube-prometheus-stack` chart usually pre-installs several useful dashboards.
    *   Click the "Dashboards" icon (four squares) on the left sidebar.
    *   Browse folders like "Kubernetes" or "Prometheus".
    *   Explore dashboards like "Kubernetes / Compute Resources / Cluster", "Kubernetes / Compute Resources / Namespace (Pods)", "Node Exporter / Nodes" to see CPU, memory, network, and disk metrics for your cluster and nodes.

7.  **Clean Up:**
    ```bash
    helm uninstall prometheus --namespace monitoring
    kubectl delete namespace monitoring
    ```
    *(Stop the `kubectl port-forward` command in your terminal if it's still running)*

**Congratulations!** You have deployed a comprehensive monitoring stack using Prometheus and Grafana via a Helm chart. You can now visualize key metrics about your Kubernetes cluster, providing essential insights for performance tuning and troubleshooting. This forms the basis for building more advanced application-specific monitoring and alerting.
