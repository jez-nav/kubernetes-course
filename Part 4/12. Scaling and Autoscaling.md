# Chapter 12: Scaling and Autoscaling

One of the primary benefits of Kubernetes is its ability to scale applications easily to meet changing demands. This chapter explores different scaling mechanisms available in Kubernetes, from manual adjustments to automated scaling based on resource utilization or other metrics. We'll cover scaling Pods horizontally (HPA), scaling cluster nodes (Cluster Autoscaler), and adjusting Pod resource allocations vertically (VPA).

## Manual Scaling of Pods and Nodes

The simplest way to scale is manually adjusting the number of replicas for controllers like Deployments or StatefulSets, or adding/removing nodes from the cluster.

**Scaling Pods (via Deployments):**

You can change the `replicas` field in a Deployment manifest and re-apply it, or use the `kubectl scale` command.

```bash
# Get current replica count
kubectl get deployment my-web-app

# Scale up to 5 replicas
kubectl scale deployment my-web-app --replicas=5

# Scale down to 2 replicas
kubectl scale deployment my-web-app --replicas=2
```
The Deployment controller will then create or delete Pods to match the new desired count, following its update strategy.

**Scaling Nodes:**

*   **Cloud Environments:** If using managed Kubernetes (EKS, GKE, AKS) or node groups managed by tools like Terraform or Cluster API, you typically adjust the desired/min/max size settings for the node group in your cloud console or IaC configuration and apply the changes. The cloud provider or management tool handles adding or removing VM instances and registering/deregistering them with the Kubernetes cluster.
*   **On-Premises/Bare Metal:** Requires manually adding new physical or virtual machines, installing the necessary Kubernetes node components (kubelet, kube-proxy, container runtime), and joining them to the cluster using `kubeadm join` or a similar mechanism. Removing nodes involves draining the node (`kubectl drain <node-name> --ignore-daemonsets`) to safely evict Pods before shutting down the machine and removing it from the cluster (`kubectl delete node <node-name>`).

Manual scaling works but requires constant monitoring and intervention. For dynamic workloads, automated scaling is preferred.

## Horizontal Pod Autoscaler (HPA)

The **Horizontal Pod Autoscaler (HPA)** automatically scales the number of Pod replicas in a Deployment, ReplicaSet, or StatefulSet based on observed metrics like CPU utilization or memory usage.

*   **Purpose:** Adjust the number of Pods running to match the current load, ensuring performance during peaks and saving resources during lulls.
*   **How it works:**
    1.  The HPA controller periodically queries resource metrics (usually CPU/Memory) for the Pods targeted by the HPA object.
    2.  It compares the current average metric value against the target value defined in the HPA specification.
    3.  It calculates the desired number of replicas needed to bring the average metric value closer to the target.
    4.  It updates the `replicas` field on the target controller (e.g., Deployment).
    5.  The target controller (e.g., Deployment) then creates or deletes Pods to match the new replica count.
*   **Metrics Source:** HPA relies on the **Kubernetes Metrics Server** to provide resource metrics (CPU/Memory). The Metrics Server is a lightweight cluster add-on that collects resource usage data from kubelets and exposes it via the Metrics API (`metrics.k8s.io`). It must be installed in your cluster for HPA based on CPU/Memory to function. (Managed services like EKS/GKE/AKS usually include it or make it easy to enable). You can also configure HPA to use custom metrics or external metrics via adapters.
*   **Target Value:** You specify a target average utilization (for CPU) or average value (for Memory) per Pod. For example, target 50% CPU utilization means HPA will try to keep the average CPU usage across all Pods at 50% of their requested CPU.

**Creating an HPA:**

1.  **Imperative Command:** (Requires Deployment/RS/StatefulSet to exist)
    ```bash
    # Target CPU utilization at 50%, scale between 2 and 10 replicas
    kubectl autoscale deployment my-web-app --cpu-percent=50 --min=2 --max=10
    ```

2.  **Declarative Manifest:** (Recommended)
    ```yaml
    apiVersion: autoscaling/v2 # Use v2 for more features like memory/custom metrics
    kind: HorizontalPodAutoscaler
    metadata:
      name: my-web-app-hpa
    spec:
      scaleTargetRef: # Points to the controller to scale
        apiVersion: apps/v1
        kind: Deployment
        name: my-web-app # Name of the Deployment
      minReplicas: 2   # Minimum number of replicas
      maxReplicas: 10  # Maximum number of replicas
      metrics:
      - type: Resource # Scale based on CPU or Memory
        resource:
          name: cpu
          target:
            type: Utilization # Target average utilization across all pods
            averageUtilization: 50 # Target 50% of requested CPU
      # Example for Memory (target average value, not percentage)
      # - type: Resource
      #   resource:
      #     name: memory
      #     target:
      #       type: AverageValue
      #       averageValue: 256Mi # Target 256 Mebibytes average per pod
      # behavior: # Optional: Control scale up/down speed and stabilization
      #   scaleDown:
      #     stabilizationWindowSeconds: 300 # Wait 5 mins before scaling down
      #     policies:
      #     - type: Percent
      #       value: 100 # Allow scaling down all necessary pods at once
      #       periodSeconds: 15
      #   scaleUp:
      #     stabilizationWindowSeconds: 0 # Scale up immediately
      #     policies:
      #     - type: Percent
      #       value: 100
      #       periodSeconds: 15
      #     - type: Pods
      #       value: 4 # Allow adding max 4 pods at once
      #       periodSeconds: 15
      #     selectPolicy: Max # Choose the policy that allows the biggest scale-up
    ```
    Apply with `kubectl apply -f hpa.yaml`.

**Important Considerations for HPA:**

*   **Resource Requests:** Pods targeted by HPA *must* have resource requests set (e.g., `spec.containers[].resources.requests.cpu`). HPA calculates utilization based on these requests.
*   **Metrics Server:** Ensure the Metrics Server is installed and running (`kubectl get pods -n kube-system | grep metrics-server`).
*   **Load Testing:** You need to generate load against your application to trigger scaling based on CPU/Memory.

**Managing HPAs:**
```bash
kubectl get hpa # List HPAs, shows TARGETS, MINPODS, MAXPODS, REPLICAS, AGE
kubectl describe hpa my-web-app-hpa # Shows detailed status, metrics, and events
kubectl delete hpa my-web-app-hpa
```

## Cluster Autoscaler (CA)

While HPA scales Pods, what happens if you need more Pods than can fit on your existing nodes due to resource constraints? The **Cluster Autoscaler (CA)** handles this by automatically adjusting the number of *nodes* in your cluster.

*   **Purpose:** Add nodes when there are pending Pods that cannot be scheduled due to insufficient resources (CPU, Memory, GPU), and remove underutilized nodes to save costs.
*   **How it works:**
    1.  The CA periodically checks for Pods in the `Pending` state that failed scheduling due to resource constraints.
    2.  If pending Pods exist, it simulates adding a new node from one of the configured node groups (managed by your cloud provider or Cluster API).
    3.  If adding a node would allow the pending Pod(s) to be scheduled, the CA interacts with the cloud provider API (or Cluster API) to increase the size of the corresponding node group, effectively adding a new node.
    4.  It also checks for nodes that have been underutilized for a certain period (e.g., low CPU/Memory requests compared to capacity) and whose Pods can be safely rescheduled onto other nodes.
    5.  If such nodes are found, it drains them (evicts Pods gracefully) and then interacts with the cloud provider API to decrease the node group size, removing the node.
*   **Integration:** CA typically runs as a Deployment in the cluster but needs specific permissions (e.g., via IAM roles/service accounts) to interact with the cloud provider API to manage node groups/VM scale sets/etc. It needs to know about the node groups it can manage and their min/max size limits.
*   **Configuration:** Usually deployed via Helm charts or manifests specific to the cloud provider (e.g., `cluster-autoscaler/aws`, `cluster-autoscaler/gcp`, `cluster-autoscaler/azure`). Configuration involves setting cloud provider credentials, identifying manageable node groups, and tuning parameters like scan intervals and utilization thresholds.

**Key Considerations:**

*   CA scales based on **resource requests** of pending Pods, not actual usage.
*   It respects PodDisruptionBudgets (PDBs), taints, tolerations, and affinity rules during scale-down simulations.
*   It doesn't directly interact with HPA, but they work together: HPA creates more Pods -> if nodes are full, Pods become Pending -> CA adds nodes -> kube-scheduler places Pending Pods on new nodes.

## Vertical Pod Autoscaler (VPA)

While HPA scales horizontally (more Pods), the **Vertical Pod Autoscaler (VPA)** adjusts the resource **requests** and **limits** (CPU, Memory) of existing Pods vertically.

*   **Purpose:** Automatically set optimal resource requests/limits for containers, improving resource utilization and scheduling efficiency. Can also just provide recommendations without applying changes.
*   **How it works:**
    1.  **VPA Recommender:** Monitors historical resource usage of Pods targeted by a VPA object and calculates recommended request values.
    2.  **VPA Updater:** If the VPA's `updateMode` is set to `Auto` or `Recreate`, it evicts Pods that need resource adjustments (respecting PDBs). When the Pod is recreated (by its controller, e.g., Deployment), the VPA Admission Controller modifies the Pod spec to apply the recommended resource requests.
    3.  **VPA Admission Controller:** A webhook that intercepts Pod creation requests. If a matching VPA object exists, it overwrites the container resource requests with the VPA's recommendations. It can also adjust limits based on configured policies.
*   **Update Modes:**
    *   `Off`: Only provides recommendations; does not change Pod resources. Useful for analysis.
    *   `Initial`: Only sets requests when a Pod is first created; does not update existing Pods.
    *   `Recreate` / `Auto`: Evicts Pods and applies recommended requests upon recreation. (Requires Pod recreation, causing brief disruption).
*   **Limitations:**
    *   **Cannot be used with HPA on CPU or Memory simultaneously.** You can use HPA for custom/external metrics while VPA manages CPU/Memory requests, but not HPA targeting CPU/Memory utilization/value directly. VPA adjusts the requests HPA uses for its calculations, which can lead to conflicts.
    *   Requires Pod restarts to apply changes in `Auto` mode.

**Creating a VPA:**

```yaml
apiVersion: autoscaling.k8s.io/v1 # VPA API group
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef: # Points to the controller whose Pods VPA should manage
    apiVersion: "apps/v1"
    kind:       Deployment
    name:       my-app # Name of the Deployment
  updatePolicy:
    updateMode: "Off" # Start with recommendations only ("Auto" to apply changes)
  resourcePolicy: # Optional: Control how requests/limits are set
    containerPolicies:
      - containerName: '*' # Apply to all containers in the Pod
        minAllowed: # Minimum allowed requests
          cpu: 100m
          memory: 50Mi
        maxAllowed: # Maximum allowed requests/limits
          cpu: 1
          memory: 500Mi
        # controlledResources: ["cpu", "memory"] # Specify which resources VPA manages
        # controlledValues: RequestsAndLimits # Or RequestsOnly
```

**Managing VPAs:**
```bash
kubectl get vpa
kubectl describe vpa my-app-vpa # Shows recommendations under Status
```
VPA installation usually involves deploying its components (Recommender, Updater, Admission Controller) into the cluster.

## Lab: Set up HPA for a Web Application

This lab demonstrates setting up a Horizontal Pod Autoscaler for a simple PHP-Apache application that can simulate CPU load.

**Prerequisites:**

*   `kubectl` connected to a running Kubernetes cluster.
*   **Metrics Server installed and running.**
    *   If using Minikube: `minikube addons enable metrics-server`
    *   For other clusters, check installation guides: [https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)
    *   Verify: `kubectl get deployment metrics-server -n kube-system` (should show AVAILABLE=1) and `kubectl get apiservice v1beta1.metrics.k8s.io` (should show AVAILABLE=True). It might take a minute or two after enabling/installing for metrics to become available.

**Steps:**

1.  **Deploy the Sample Application:**
    Save as `php-apache.yaml`:
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: php-apache
    spec:
      selector:
        matchLabels:
          run: php-apache
      replicas: 1 # Start with 1 replica
      template:
        metadata:
          labels:
            run: php-apache
        spec:
          containers:
          - name: php-apache
            # This image has apache and php, index.php simulates load
            image: k8s.gcr.io/hpa-example
            ports:
            - containerPort: 80
            resources: # IMPORTANT: Set resource requests for HPA
              requests:
                cpu: 200m # Request 0.2 CPU core
                memory: 100Mi
              limits:
                cpu: 500m
                memory: 200Mi
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: php-apache
    spec:
      selector:
        run: php-apache
      ports:
      - port: 80 # Service port
      # type: LoadBalancer # Optional: Use LoadBalancer if on cloud
      type: ClusterIP # Use ClusterIP + port-forward for local testing
    ```
    Apply it: `kubectl apply -f php-apache.yaml`

2.  **Create the HPA:**
    Save as `php-hpa.yaml`:
    ```yaml
    apiVersion: autoscaling/v2
    kind: HorizontalPodAutoscaler
    metadata:
      name: php-apache-hpa
    spec:
      scaleTargetRef:
        apiVersion: apps/v1
        kind: Deployment
        name: php-apache # Target the deployment
      minReplicas: 1
      maxReplicas: 10
      metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 50 # Target 50% CPU utilization
    ```
    Apply it: `kubectl apply -f php-hpa.yaml`

3.  **Check HPA Status:**
    ```bash
    kubectl get hpa php-apache-hpa
    # Initially, TARGETS might show <unknown>/50% until metrics are gathered.
    # Wait a minute, then run again. It should show the current CPU % / 50%.
    # Example: TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
    #          0%/50%    1         10        1          1m
    ```

4.  **Generate Load:**
    Open a *new terminal* and run a temporary Pod to continuously send requests to the `php-apache` service.
    ```bash
    # Start port-forwarding in the *first* terminal if using ClusterIP service
    # kubectl port-forward service/php-apache 8080:80

    # In the *second* terminal:
    kubectl run load-generator --image=busybox:1.28 --rm -it -- /bin/sh
    # Inside the load-generator shell:
    # while true; do wget -q -O- http://php-apache.default.svc.cluster.local; done
    # Or if using port-forwarding from your local machine:
    # while true; do wget -q -O- http://localhost:8080; done
    ```
    This loop continuously hits the `index.php` page, which performs calculations to consume CPU.

5.  **Observe HPA Scaling Up:**
    In the *first* terminal, watch the HPA and Deployment:
    ```bash
    kubectl get hpa php-apache-hpa -w
    # Also watch the pods in another terminal if possible:
    # kubectl get pods -l run=php-apache -w
    ```
    *   You should see the `TARGETS` CPU percentage increase above 50%.
    *   After a short delay, the `REPLICAS` count should start increasing (e.g., to 2, then 3, etc., up to 10).
    *   The HPA aims to bring the *average* CPU utilization back down towards 50% by adding more Pods to share the load.

6.  **Stop the Load:**
    Go back to the *second* terminal (load-generator) and press `Ctrl+C` to stop the `while` loop, then type `exit`.

7.  **Observe HPA Scaling Down:**
    In the *first* terminal, continue watching the HPA:
    ```bash
    kubectl get hpa php-apache-hpa -w
    ```
    *   The `TARGETS` CPU percentage should drop significantly now that the load is gone.
    *   After a stabilization period (default is 5 minutes, but configurable via `behavior`), the `REPLICAS` count should decrease back towards the `minReplicas` (1 in this case).

8.  **Clean Up:**
    ```bash
    kubectl delete -f php-apache.yaml
    kubectl delete -f php-hpa.yaml
    # The load-generator pod was deleted automatically due to --rm
    ```

**Congratulations!** You have successfully configured and observed the Horizontal Pod Autoscaler automatically scaling a Deployment based on CPU utilization in response to load changes. This is a powerful mechanism for ensuring application responsiveness and resource efficiency.
