# Chapter 16: Advanced Terraform Techniques

Previous chapters introduced Terraform for provisioning cloud infrastructure and Kubernetes clusters. As your infrastructure grows in complexity and your team collaborates, you'll need more advanced techniques to keep your Terraform code manageable, reusable, and reliable across different environments. This chapter covers writing effective modules, strategies for multi-environment deployments, best practices for state management in teams, and integrating Terraform into CI/CD pipelines.

## Writing Reusable Terraform Modules

Modules are the cornerstone of writing clean, reusable, and maintainable Terraform code. A module encapsulates a set of related resources that represent a logical component of your infrastructure (e.g., a VPC, a Kubernetes cluster, a database setup, a web application stack).

**Standard Module Structure:**

While flexible, a common structure for a Terraform module directory includes:

*   `main.tf`: Contains the core resource definitions managed by the module.
*   `variables.tf`: Defines the input variables the module accepts, allowing customization. Use descriptions and types.
*   `outputs.tf`: Defines the output values the module exposes, allowing other parts of your configuration to use information about the resources created by the module (e.g., VPC ID, instance IPs).
*   `README.md`: Essential documentation explaining what the module does, its inputs, outputs, and usage examples.
*   `LICENSE`: The license under which the module is distributed.
*   `versions.tf` (Optional but recommended): Specifies required Terraform and provider versions for the module.

**Key Principles for Good Modules:**

*   **Focused Scope:** Modules should manage a well-defined, logical set of resources. Avoid creating overly large "god" modules.
*   **Clear Inputs/Outputs:** Define clear, well-documented input variables (`variables.tf`) for customization and output values (`outputs.tf`) to expose necessary information.
*   **Sensible Defaults:** Provide reasonable default values for input variables where appropriate to make the module easier to use for common cases.
*   **No Provider Configuration:** Modules should generally *not* contain `provider` blocks. Provider configurations should be handled in the root module (the main configuration calling the module) and passed implicitly to the module.
*   **Use `locals` for Internal Logic:** Use local values (`locals {}` block) for intermediate calculations, complex expressions, or naming conventions within the module to keep resource blocks cleaner.
    ```hcl
    locals {
      common_tags = {
        Environment = var.environment
        Project     = var.project_name
        ManagedBy   = "Terraform"
      }
      instance_name = "${var.project_name}-${var.environment}-instance"
    }

    resource "aws_instance" "example" {
      # ...
      tags = local.common_tags
      # ...
    }
    ```
*   **Versioning:** If sharing modules (e.g., via a private Git repository or Terraform Registry), use versioning (Git tags) to manage changes and ensure consumers use stable versions.

**Calling Modules:**

As seen before, you use the `module "<instance_name>" {}` block in your root configuration:

```hcl
module "production_vpc" {
  source = "./modules/vpc" # Local path

  # Pass required input variables
  environment = "production"
  vpc_cidr    = "10.100.0.0/16"
  # ... other variables ...
}

module "eks_cluster" {
  source  = "terraform-aws-modules/eks/aws" # From Terraform Registry
  version = "~> 19.0"

  cluster_name = "prod-cluster"
  vpc_id       = module.production_vpc.vpc_id # Use output from another module
  subnet_ids   = module.production_vpc.private_subnet_ids
  # ... other variables ...
}
```

## Managing Multi-Environment Setups (Dev, Staging, Prod)

Most applications require distinct environments (e.g., development, staging, production) with potentially different configurations (instance sizes, replica counts, feature flags). Managing these effectively with Terraform requires a clear strategy.

**Common Strategies:**

1.  **Workspaces:**
    *   **Concept:** Terraform's built-in mechanism to manage multiple state files for the *same* configuration. Each workspace has its own independent `.tfstate` file.
    *   **How it works:**
        *   `terraform workspace new <name>`: Creates and switches to a new workspace.
        *   `terraform workspace select <name>`: Switches to an existing workspace.
        *   `terraform workspace list`: Shows available workspaces.
        *   When you run `plan` or `apply`, Terraform uses the state file corresponding to the currently selected workspace.
    *   **Usage:** Often combined with conditional logic or different `.tfvars` files per workspace to vary configurations.
        ```hcl
        # Example using workspace name in locals
        locals {
          instance_type = terraform.workspace == "production" ? "m5.large" : "t3.micro"
        }

        resource "aws_instance" "example" {
          instance_type = local.instance_type
          # ...
        }
        ```
        Or load environment-specific variables: `terraform apply -var-file="${terraform.workspace}.tfvars"`
    *   **Pros:** Simple for managing slightly different states of the *same* configuration. Built-in.
    *   **Cons:** Can become complex if configurations diverge significantly between environments. All environments share the same codebase, making isolated changes harder. Risk of accidentally applying changes to the wrong environment if the workspace isn't selected carefully. Generally less favored for distinct environment promotion than directory-based approaches.

2.  **Directory Structure (Per Environment):**
    *   **Concept:** Create separate directories for each environment. Each directory contains its own Terraform configuration (or references shared modules) and manages its own state file (ideally via a remote backend configured per environment).
    *   **Structure Example:**
        ```
        ├── environments/
        │   ├── dev/
        │   │   ├── main.tf
        │   │   ├── variables.tf
        │   │   ├── backend.tf # Configured for dev state
        │   │   └── terraform.tfvars # Dev-specific values
        │   ├── staging/
        │   │   ├── main.tf
        │   │   ├── variables.tf
        │   │   ├── backend.tf # Configured for staging state
        │   │   └── terraform.tfvars # Staging-specific values
        │   └── prod/
        │       ├── main.tf
        │       ├── variables.tf
        │       ├── backend.tf # Configured for prod state
        │       └── terraform.tfvars # Prod-specific values
        ├── modules/
        │   ├── vpc/
        │   │   └── ... (shared VPC module)
        │   └── eks/
        │       └── ... (shared EKS module)
        ```
    *   **Workflow:** `cd environments/dev && terraform apply`, `cd environments/staging && terraform apply`, etc.
    *   **Pros:** Clear separation between environments. Independent state management. Easier to promote code (module versions) through environments. Aligns well with Git branching strategies.
    *   **Cons:** More boilerplate configuration files compared to workspaces (though shared modules mitigate this significantly). Requires navigating directories.

3.  **Tools like Terragrunt:**
    *   **Concept:** Terragrunt is a thin wrapper around Terraform that provides extra tools for managing configurations, remote state, and locking, especially for multi-environment setups based on directory structures.
    *   **Features:** Helps keep backend and provider configurations DRY (Don't Repeat Yourself), simplifies passing outputs between modules/stacks, enforces structure.
    *   **Pros:** Reduces boilerplate in directory-based approaches. Adds useful orchestration features.
    *   **Cons:** Introduces another tool and layer of abstraction to learn.

**Recommendation:** For managing distinct environments like dev, staging, and prod, the **directory structure approach using shared modules** is generally preferred for its clarity, isolation, and ease of promotion. Workspaces are better suited for temporary variations or feature branches within a single conceptual environment.

## Handling Terraform State in Teams

As discussed in Chapter 9, using **remote backends** with **state locking** is non-negotiable for team collaboration.

**Recap of Best Practices:**

*   **Use Remote Backends:** Store state centrally and securely (S3, GCS, Azure Blob, Terraform Cloud).
*   **Enable State Locking:** Prevent concurrent `apply` operations that could corrupt state (e.g., using DynamoDB with S3, or built-in locking with other backends).
*   **Backup State:** Ensure your remote backend storage is backed up.
*   **Minimize State File Size:** Avoid storing large files or unnecessary data in state. Use modules to keep root state files smaller.
*   **Protect Access:** Use IAM policies or access controls on your backend storage to restrict who can read/write the state file. State files can contain sensitive information.

**State Manipulation (Use with Extreme Caution):**

Terraform provides commands to interact with the state file directly. These should be used rarely and carefully, typically only for recovery or specific refactoring scenarios:

*   `terraform state list`: List resources tracked in the state.
*   `terraform state show <resource_address>`: Show attributes of a specific resource in state.
*   `terraform state mv <source> <destination>`: Move/rename a resource within the state file (useful when refactoring code or moving resources between modules without destroying/recreating).
*   `terraform state rm <resource_address>`: Remove a resource from Terraform's state tracking (Terraform will no longer manage it, but the real resource is *not* destroyed).
*   `terraform import <resource_address> <resource_id>`: Import an existing, manually created resource into Terraform's state so it can be managed declaratively going forward.

**Warning:** Incorrect state manipulation can lead to inconsistencies between your state file and real infrastructure, potentially causing Terraform to destroy or modify resources unexpectedly on the next `apply`. Always back up your state file before performing manual state operations.

## Integrating Terraform with CI/CD Pipelines

Automating Terraform runs within a Continuous Integration/Continuous Deployment (CI/CD) pipeline ensures consistency, provides audit trails, and enables automated testing and deployment.

**Common CI/CD Workflow:**

1.  **Trigger:** Pipeline triggers on code changes (e.g., push to `main` branch, pull request creation).
2.  **Setup:** CI runner checks out the code, installs Terraform, configures cloud credentials securely (e.g., via CI/CD secrets management injecting environment variables or using cloud IAM roles for runners).
3.  **Initialize:** `terraform init -input=false` (non-interactive).
4.  **Validate:** `terraform validate`.
5.  **Plan:** `terraform plan -input=false -out=tfplan`.
    *   For Pull Requests: Post the plan output as a comment for review. Require approval before merging/applying.
    *   For Main Branch: Store the plan artifact.
6.  **Apply (Optional Manual Approval):**
    *   `terraform apply -input=false tfplan`.
    *   Often requires manual approval step in the CI/CD tool for production environments.
7.  **Notifications:** Notify team members (e.g., via Slack) about plan results and apply success/failure.

**Tools:**

*   **GitHub Actions:** Use official `hashicorp/setup-terraform` action and community actions for planning/applying. Store credentials in GitHub Secrets.
*   **GitLab CI:** Use Docker images with Terraform pre-installed. Store credentials in GitLab CI/CD variables (masked).
*   **Jenkins:** Use Terraform plugin or execute Terraform commands in shell steps. Manage credentials via Jenkins credentials store.
*   **Atlantis:** An open-source application specifically for Terraform pull request automation. It runs `plan` automatically and allows team members to approve and run `apply` via PR comments.

**Key Considerations for CI/CD:**

*   **Credentials Management:** Never store credentials in code. Use the CI/CD system's secure secret management.
*   **Non-Interactive Mode:** Use `-input=false` for `init`, `plan`, `apply`, `destroy`.
*   **State Locking:** Essential to prevent race conditions if multiple pipelines run concurrently.
*   **Plan Review/Approval:** Implement mandatory review and approval steps, especially for production changes.
*   **Error Handling:** Ensure the pipeline fails appropriately if any Terraform command fails.

## Lab: Refactor EKS Cluster to a Module

This lab refactors the EKS cluster deployment from Chapter 10 into a local Terraform module to demonstrate module creation and usage.

**Prerequisites:**

*   Completed (or have the code from) the Chapter 10 Lab (`terraform-eks-lab`).
*   Terraform installed.
*   AWS Credentials configured.

**Steps:**

1.  **Restructure Project:**
    Start in the `terraform-eks-lab` directory from Chapter 10. Create the following structure:
    ```
    terraform-eks-lab/
    ├── modules/
    │   └── eks/
    │       ├── main.tf      # EKS module resources
    │       ├── variables.tf # EKS module inputs
    │       └── outputs.tf   # EKS module outputs
    ├── main.tf          # Root module - calls VPC and EKS modules
    ├── variables.tf     # Root module variables (region, cluster name)
    ├── outputs.tf       # Root module outputs (kubectl command)
    └── versions.tf      # Root module provider/TF versions
    ```

2.  **Move EKS Logic to Module (`modules/eks/`):**
    *   **`modules/eks/variables.tf`:** Define input variables needed specifically for the EKS module (cluster name, version, VPC ID, subnet IDs, tags, etc.). Copy relevant variable blocks from the original root `variables.tf` and `main.tf` (module "eks" block).
        ```hcl
        # modules/eks/variables.tf
        variable "cluster_name" {
          description = "Name for the EKS cluster"
          type        = string
        }
        variable "cluster_version" {
          description = "Kubernetes version for EKS"
          type        = string
        }
        variable "vpc_id" {
          description = "ID of the VPC where the cluster runs"
          type        = string
        }
        variable "subnet_ids" {
          description = "List of subnet IDs for control plane and nodes"
          type        = list(string)
        }
        variable "instance_types" {
          description = "List of instance types for the default node group"
          type        = list(string)
          default     = ["t3.medium"]
        }
        variable "min_size" {
          description = "Minimum number of nodes"
          type        = number
          default     = 1
        }
        variable "max_size" {
          description = "Maximum number of nodes"
          type        = number
          default     = 3
        }
        variable "desired_size" {
          description = "Desired number of nodes"
          type        = number
          default     = 2
        }
        variable "tags" {
          description = "Tags to apply to EKS resources"
          type        = map(string)
          default     = {}
        }
        # Add other variables as needed (e.g., for aws_auth_users)
        ```
    *   **`modules/eks/main.tf`:** Move the `module "eks" {}` block content *from the original root `main.tf`* here. Replace hardcoded values or root variable references (like `var.cluster_name`) with references to the module's *own* input variables (e.g., `var.cluster_name`, `var.vpc_id`).
        ```hcl
        # modules/eks/main.tf
        module "eks" {
          source  = "terraform-aws-modules/eks/aws"
          version = "~> 19.0" # Pin version within the module

          cluster_name    = var.cluster_name # Use module variable
          cluster_version = var.cluster_version # Use module variable

          vpc_id     = var.vpc_id     # Use module variable
          subnet_ids = var.subnet_ids # Use module variable

          eks_managed_node_group_defaults = {
            ami_type = "AL2_x86_64"
          }

          eks_managed_node_groups = {
            default_nodes = { # Renamed for clarity within module
              name           = "${var.cluster_name}-nodes"
              instance_types = var.instance_types # Use module variable
              min_size       = var.min_size       # Use module variable
              max_size       = var.max_size       # Use module variable
              desired_size   = var.desired_size   # Use module variable
            }
          }
          tags = var.tags # Use module variable
          # Add aws_auth_users config here if needed, referencing module variables
        }
        ```
    *   **`modules/eks/outputs.tf`:** Define outputs the EKS module should expose. Copy relevant output blocks from the original root `outputs.tf` and reference the *internal* `module.eks` resource created within *this* module's `main.tf`.
        ```hcl
        # modules/eks/outputs.tf
        output "cluster_name" {
          description = "EKS cluster name"
          value       = module.eks.cluster_name # Reference internal module block
        }
        output "cluster_endpoint" {
          description = "Endpoint for EKS control plane"
          value       = module.eks.cluster_endpoint
        }
        output "cluster_certificate_authority_data" {
          description = "Base64 encoded cluster certificate authority data"
          value       = module.eks.cluster_certificate_authority_data
        }
        output "cluster_region" {
          description = "AWS Region for the cluster"
          # Note: The EKS module itself doesn't directly output the region easily
          # It's better practice to pass the region *into* the module if needed,
          # or rely on the root module's region variable.
          # For simplicity here, we might omit this or derive it differently if needed.
          value = module.eks.cluster_region # Check module docs if this output exists
        }
        # Add other outputs like security group ID if needed
        ```

3.  **Update Root Module Files (`terraform-eks-lab/`):**
    *   **`versions.tf`:** Should already exist and be correct (defines AWS provider).
    *   **`variables.tf`:** Keep only the variables needed for the *root* configuration (e.g., region, maybe a base cluster name if used for VPC naming). Remove variables now defined within the EKS module.
        ```hcl
        # variables.tf (Root)
        variable "aws_region" {
          description = "AWS region for deployment"
          type        = string
          default     = "us-west-2"
        }
        variable "cluster_name_prefix" {
          description = "Prefix for naming resources (VPC, EKS cluster)"
          type        = string
          default     = "k8s-course-refactored"
        }
        # Add variables for different environments if desired (e.g., instance_type_dev, instance_type_prod)
        ```
    *   **`main.tf`:** Update this to call the VPC module (if not already done) and the *new local EKS module*. Pass values from root variables or hardcode environment-specific values here.
        ```hcl
        # main.tf (Root)

        # --- VPC --- (Keep the VPC module call from Chapter 10) ---
        module "vpc" {
          source  = "terraform-aws-modules/vpc/aws"
          version = "~> 5.0"

          name = "${var.cluster_name_prefix}-vpc"
          cidr = "10.0.0.0/16"
          azs  = ["${var.aws_region}a", "${var.aws_region}b", "${var.aws_region}c"]
          private_subnets = ["10.0.1.0/24", "10.0.2.0/24", "10.0.3.0/24"]
          public_subnets  = ["10.0.101.0/24", "10.0.102.0/24", "10.0.103.0/24"]
          enable_nat_gateway = true
          single_nat_gateway = true
          public_subnet_tags = {
            "kubernetes.io/cluster/${var.cluster_name_prefix}-eks" = "shared"
            "kubernetes.io/role/elb"                  = "1"
          }
          private_subnet_tags = {
            "kubernetes.io/cluster/${var.cluster_name_prefix}-eks" = "shared"
            "kubernetes.io/role/internal-elb"         = "1"
          }
          tags = { Environment = "Development", Terraform = "true" }
        }

        # --- EKS Cluster (Using the local module) ---
        module "eks_cluster" {
          source = "./modules/eks" # Call the local module

          # Pass values to the module's input variables
          cluster_name    = "${var.cluster_name_prefix}-eks"
          cluster_version = "1.27" # Or use a root variable
          vpc_id          = module.vpc.vpc_id # Use VPC module output
          subnet_ids      = module.vpc.private_subnets # Use VPC module output

          # Example: Override defaults for specific environment
          instance_types = ["t3.medium"] # Or use var.eks_instance_types
          desired_size   = 2             # Or use var.eks_desired_size

          tags = { Environment = "Development", Terraform = "true" }
        }
        ```
    *   **`outputs.tf`:** Update outputs to reference the *local EKS module instance* (`module.eks_cluster`).
        ```hcl
        # outputs.tf (Root)
        output "cluster_name" {
          description = "EKS cluster name"
          value       = module.eks_cluster.cluster_name # Reference local module output
        }
        output "cluster_endpoint" {
          description = "Endpoint for EKS control plane"
          value       = module.eks_cluster.cluster_endpoint # Reference local module output
        }
        output "region" {
          description = "AWS region"
          value       = var.aws_region
        }
        output "configure_kubectl" {
          description = "Configure kubectl command"
          # Note: Need cluster_region output from module or use var.aws_region
          value       = "aws eks update-kubeconfig --region ${var.aws_region} --name ${module.eks_cluster.cluster_name}"
        }
        ```

4.  **Run Terraform Workflow:**
    *   `terraform init` (This will initialize the root module and download providers/modules, including recognizing the local `./modules/eks` path).
    *   `terraform plan` (Review the plan. If you ran `destroy` in Chapter 10, it should plan to create resources. If the cluster still exists, it might plan minor changes or no changes if inputs match).
    *   `terraform apply` (Apply the changes using the refactored module structure).
    *   `terraform output` (Verify outputs).
    *   `terraform destroy` (Clean up when finished).

**Congratulations!** You have refactored your EKS cluster configuration into a reusable local module. This structure makes your root configuration cleaner and allows you to potentially reuse the EKS module for other environments or projects by simply calling it with different input variables. This is a key step towards managing larger Terraform codebases effectively.
