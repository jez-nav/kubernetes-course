# Chapter 6: Services and Networking

Effective networking is crucial for connecting application components within Kubernetes and exposing them to the outside world. This chapter explores Kubernetes Services for internal load balancing and discovery, Ingress for managing external access, DNS for naming, and NetworkPolicies for security.

## Service Types Revisited

In Chapter 3, we introduced Services as an abstraction providing a stable IP address and DNS name for a set of Pods. Let's delve deeper into the primary Service types:

*   **`ClusterIP` (Default):**
    *   **Purpose:** Exposes the Service on an internal IP address within the cluster.
    *   **Accessibility:** Only reachable from *within* the cluster (other Pods, Nodes).
    *   **Use Case:** Internal communication between microservices. The frontend Pod talks to the backend Pod via the backend's ClusterIP Service.
    *   **How it works:** `kube-proxy` on each node manages iptables rules (or IPVS) to intercept traffic destined for the Service's ClusterIP:Port and load balance it across the healthy backend Pods (identified by the Service's selector).

    *Example (from Chapter 5):*
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: echo-service
    spec:
      selector:
        app: echoserver
      ports:
      - protocol: TCP
        port: 80
        targetPort: 8080
      type: ClusterIP # Explicitly stated, but default if omitted
    ```

*   **`NodePort`:**
    *   **Purpose:** Exposes the Service on each Node's IP address at a static port (the NodePort).
    *   **Accessibility:** Reachable from *outside* the cluster by accessing `<NodeIP>:<NodePort>`. Also creates a ClusterIP service for internal access.
    *   **Use Case:** Development, testing, or simple scenarios where external access is needed without a dedicated load balancer. Not typically recommended for production web traffic due to manual NodeIP management and potential security exposure.
    *   **How it works:** `kube-proxy` configures rules to forward traffic arriving at the `<NodeIP>:<NodePort>` to the internal ClusterIP:Port, which then load balances to the backend Pods. The NodePort must be within a configurable range (default: 30000-32767).

    *Example:*
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: my-nodeport-service
    spec:
      selector:
        app: myapp
      ports:
      - protocol: TCP
        port: 80       # Internal ClusterIP port
        targetPort: 8080 # Pod's container port
        # nodePort: 30080 # Optional: Specify a port in the range, otherwise one is auto-assigned
      type: NodePort
    ```
    *(Access via `http://<any-node-ip>:30080`)*

*   **`LoadBalancer`:**
    *   **Purpose:** Exposes the Service externally using a cloud provider's load balancer.
    *   **Accessibility:** Reachable via the cloud load balancer's public IP address. Automatically creates `NodePort` and `ClusterIP` services as well.
    *   **Use Case:** The standard way to expose production web services running in a cloud environment (AWS, GCP, Azure, etc.).
    *   **How it works:** Requires integration with a cloud provider. When you create a Service of type `LoadBalancer`, the cloud controller manager provisions an external load balancer (e.g., AWS ELB, Google Cloud Load Balancer) and configures it to route traffic to the `NodePort` on your cluster nodes. The external IP address is then populated in the Service's status.

    *Example (Cloud environment needed):*
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: my-lb-service
    spec:
      selector:
        app: myapp
      ports:
      - protocol: TCP
        port: 80
        targetPort: 8080
      type: LoadBalancer
      # cloud provider specific annotations might be needed here
    ```
    *(Run `kubectl get service my-lb-service --watch` to see the `EXTERNAL-IP` get assigned)*

*   **`ExternalName`:**
    *   **Purpose:** Maps a Service name within the cluster to an external DNS name (e.g., a managed database service outside Kubernetes).
    *   **Accessibility:** Acts as an alias. When Pods query this Service name via Kubernetes DNS, they receive a CNAME record pointing to the external name.
    *   **Use Case:** Providing a stable internal name for an external dependency, allowing you to switch the backend service without changing application code.

    *Example:*
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: external-db
    spec:
      type: ExternalName
      externalName: my-prod-db.example.com # The actual external DNS name
    ```

## Ingress Controllers and Ingress Resources

While `LoadBalancer` Services work well, they typically provision one cloud load balancer per Service, which can be expensive. **Ingress** provides a more flexible and efficient way to manage external access, especially for HTTP/HTTPS traffic.

*   **Ingress Resource:** A Kubernetes object that defines rules for routing external HTTP/HTTPS traffic to internal Services. It specifies hostnames (e.g., `myapp.example.com`), paths (e.g., `/api`), and the backend Services to route traffic to.
*   **Ingress Controller:** A Pod (usually managed by a Deployment or DaemonSet) running in the cluster that *watches* Ingress resources and *implements* the routing rules. It typically runs a reverse proxy/load balancer (like Nginx, HAProxy, Traefik) and configures it based on the Ingress objects. The Ingress Controller itself is usually exposed via a `NodePort` or `LoadBalancer` Service.

**Workflow:**
1.  Deploy an Ingress Controller into your cluster (e.g., Nginx Ingress Controller, Traefik). This is often done via Helm charts or operator manifests.
2.  Expose the Ingress Controller using a Service (usually `LoadBalancer` in cloud environments, `NodePort` otherwise). This gives you a single entry point IP address.
3.  Create Ingress resources defining your routing rules (hosts, paths, backend services).
4.  Configure your external DNS to point your desired hostnames (e.g., `myapp.example.com`) to the external IP address of the Ingress Controller's Service.
5.  Traffic flow: User -> DNS -> Ingress Controller's External IP -> Ingress Controller Pod -> Backend Service (ClusterIP) -> Backend Pod.

**Benefits of Ingress:**
*   **Cost-Effective:** Single load balancer (for the controller) can handle traffic for many Services.
*   **HTTP/HTTPS Routing:** Provides host-based and path-based routing.
*   **TLS/SSL Termination:** Can handle TLS certificates, decrypting traffic before forwarding it to backend Services.
*   **Advanced Features:** Many controllers offer features like load balancing algorithms, authentication, rate limiting, etc. via annotations on the Ingress resource.

*Example Ingress Resource (`my-ingress.yaml`):*
```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: simple-fanout-ingress
  annotations:
    # Annotations are controller-specific, e.g., for nginx-ingress:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  # ingressClassName: nginx # Specify if multiple controllers exist
  rules:
  - host: myapp.example.com # Optional: Route based on hostname
    http:
      paths:
      - path: /foo # Route requests for myapp.example.com/foo
        pathType: Prefix # Matches paths starting with /foo
        backend:
          service:
            name: service-foo # To Service named 'service-foo'
            port:
              number: 80 # On port 80 of that service
      - path: /bar
        pathType: Prefix
        backend:
          service:
            name: service-bar
            port:
              number: 8080
  # tls: # Optional: Define TLS termination
  # - hosts:
  #   - myapp.example.com
  #   secretName: myapp-tls-secret # K8s Secret containing tls.crt and tls.key
```

**Managing Ingress:**
```bash
# Assuming an Ingress Controller is already running
kubectl apply -f my-ingress.yaml
kubectl get ingress # List Ingress resources
kubectl describe ingress simple-fanout-ingress
kubectl delete ingress simple-fanout-ingress
```

## DNS in Kubernetes

Kubernetes provides an internal DNS service (usually CoreDNS, deployed as Pods) that enables service discovery within the cluster.

*   **How it works:** When a Service is created, the DNS service automatically creates DNS records for it. Pods are configured (via `/etc/resolv.conf` injected by the kubelet) to use the internal DNS server.
*   **Record Format:**
    *   **Services:** `<service-name>.<namespace-name>.svc.cluster.local`
        *   Example: `my-service.default.svc.cluster.local`
    *   **Pods:** `<pod-ip-dashed>.<namespace-name>.pod.cluster.local` (less commonly used directly)
        *   Example: `10-1-2-3.default.pod.cluster.local`
*   **Short Names:** From within the same namespace, you can usually just use the `<service-name>` (e.g., `curl http://my-service`). From a different namespace, use `<service-name>.<namespace-name>` (e.g., `curl http://my-service.other-ns`). The full name always works.
*   **Headless Services:** A Service with `spec.clusterIP: None`. Instead of providing a single virtual IP, DNS queries for a headless service return the IP addresses of *all* the backing Pods directly. Useful for stateful applications (like databases) where clients need to connect to specific peers.

## Network Policies for Security

By default, all Pods in a Kubernetes cluster can communicate with all other Pods, regardless of namespace. **NetworkPolicy** resources allow you to restrict network traffic flow between Pods (and to/from external endpoints) based on labels and namespaces.

*   **Purpose:** Implement network segmentation and enforce security boundaries (zero-trust networking).
*   **How it works:** NetworkPolicies are implemented by the **CNI plugin**. Not all CNI plugins support NetworkPolicy (e.g., Flannel has limited support, Calico, Cilium, Weave Net have full support). Policies use selectors to specify which Pods they apply to and define rules for allowed ingress (incoming) and egress (outgoing) traffic.
*   **Default Behavior:** If no NetworkPolicy selects a Pod, all traffic is allowed. If *any* NetworkPolicy selects a Pod, only the traffic explicitly allowed by *at least one* policy is permitted (default deny).

*Example NetworkPolicy (`deny-all-ingress.yaml`):*
```yaml
# Apply this policy to deny all incoming traffic to pods with app=web
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
spec:
  podSelector: # Selects the pods this policy applies to
    matchLabels:
      app: web
  policyTypes: # Specifies which types of traffic this policy affects
  - Ingress # This policy only affects incoming traffic
  # No ingress rules defined, meaning ALL ingress is denied
```

*Example NetworkPolicy (`allow-frontend-to-backend.yaml`):*
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: backend-allow-frontend
spec:
  podSelector: # Apply to backend pods
    matchLabels:
      app: backend
  policyTypes:
  - Ingress
  ingress: # Define allowed incoming traffic rules
  - from: # Allow traffic FROM pods matching these selectors
    - podSelector:
        matchLabels:
          app: frontend # Allow from frontend pods
    ports: # Allow traffic TO these ports on the backend pods
    - protocol: TCP
      port: 8080
```

**Managing Network Policies:**
```bash
kubectl apply -f my-network-policy.yaml
kubectl get networkpolicy # or netpol
kubectl describe networkpolicy <policy-name>
kubectl delete networkpolicy <policy-name>
```

## Lab: Expose an App with Ingress and Secure it with Network Policies

This lab deploys two simple web services, exposes them via an Ingress resource, and then uses NetworkPolicies to restrict traffic flow.

**Prerequisites:**
*   `kubectl` connected to a running Kubernetes cluster.
*   **An Ingress Controller installed and running.** (e.g., Nginx Ingress). If using Minikube, enable the ingress addon: `minikube addons enable ingress`. Verify it's running: `kubectl get pods -n ingress-nginx`.
*   **A CNI plugin that supports Network Policies.** (Minikube's default might not; consider starting Minikube with Calico: `minikube start --network-plugin=cni --cni=calico`)

**Steps:**

1.  **Deploy Two Sample Applications:**
    Save as `apps.yaml`:
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: hello-app-v1
    spec:
      replicas: 1
      selector: { matchLabels: { app: hello, version: v1 } }
      template:
        metadata: { labels: { app: hello, version: v1 } }
        spec: { containers: [ { name: hello, image: "gcr.io/google-samples/hello-app:1.0", ports: [ { containerPort: 8080 } ] } ] }
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: hello-service-v1
    spec:
      selector: { app: hello, version: v1 }
      ports: [ { port: 80, targetPort: 8080 } ]
      type: ClusterIP
    ---
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: hello-app-v2
    spec:
      replicas: 1
      selector: { matchLabels: { app: hello, version: v2 } }
      template:
        metadata: { labels: { app: hello, version: v2 } }
        spec: { containers: [ { name: hello, image: "gcr.io/google-samples/hello-app:2.0", ports: [ { containerPort: 8080 } ] } ] }
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: hello-service-v2
    spec:
      selector: { app: hello, version: v2 }
      ports: [ { port: 80, targetPort: 8080 } ]
      type: ClusterIP
    ```
    Apply it: `kubectl apply -f apps.yaml`

2.  **Create the Ingress Resource:**
    Save as `hello-ingress.yaml`:
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: hello-ingress
      annotations: # Use appropriate annotations for your controller if needed
        nginx.ingress.kubernetes.io/rewrite-target: /$2 # Example for Nginx Ingress
    spec:
      ingressClassName: nginx # Specify your ingress class name
      rules:
      - http:
          paths:
          - path: /v1(/|$)(.*) # Route /v1/* to hello-service-v1
            pathType: Prefix
            backend:
              service:
                name: hello-service-v1
                port: { number: 80 }
          - path: /v2(/|$)(.*) # Route /v2/* to hello-service-v2
            pathType: Prefix
            backend:
              service:
                name: hello-service-v2
                port: { number: 80 }
    ```
    Apply it: `kubectl apply -f hello-ingress.yaml`

3.  **Find Ingress IP and Test:**
    Get the external IP of your Ingress controller's service:
    ```bash
    # If using Minikube ingress addon:
    minikube service list # Find IP/Port for ingress-nginx-controller
    INGRESS_IP=$(minikube ip) # Usually the Minikube IP
    INGRESS_PORT=$(kubectl get svc -n ingress-nginx ingress-nginx-controller -o jsonpath='{.spec.ports[0].nodePort}')
    # If using LoadBalancer service:
    # INGRESS_IP=$(kubectl get svc -n <ingress-namespace> <ingress-controller-service> -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
    # INGRESS_PORT=80 # Default HTTP port
    ```
    Test access (replace IP/Port if needed):
    ```bash
    curl http://$INGRESS_IP:$INGRESS_PORT/v1/
    # Should show "Hello, world! Version: 1.0.0" and hostname

    curl http://$INGRESS_IP:$INGRESS_PORT/v2/
    # Should show "Hello, world! Version: 2.0.0" and hostname
    ```
    *(Note: If using minikube IP/NodePort, direct host routing in Ingress won't work without extra setup like editing /etc/hosts. Path routing works.)*

4.  **Apply Default Deny Network Policy:**
    Save as `default-deny.yaml`:
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: default-deny-all
    spec:
      podSelector: {} # Empty selector selects ALL pods in the namespace
      policyTypes:
      - Ingress
      - Egress
      # No rules defined = deny all ingress and egress
    ```
    Apply it: `kubectl apply -f default-deny.yaml`

5.  **Test Access After Deny:**
    ```bash
    curl http://$INGRESS_IP:$INGRESS_PORT/v1/
    # This should now TIMEOUT, as ingress traffic to the pods is blocked.
    curl http://$INGRESS_IP:$INGRESS_PORT/v2/
    # Should also timeout.
    ```

6.  **Allow Ingress Traffic from Ingress Controller:**
    Save as `allow-ingress.yaml`:
    ```yaml
    apiVersion: networking.k8s.io/v1
    kind: NetworkPolicy
    metadata:
      name: allow-ingress-traffic
    spec:
      podSelector: # Apply to both hello apps
        matchLabels:
          app: hello
      policyTypes:
      - Ingress
      ingress:
      - from:
        # Allow traffic FROM pods in the ingress-nginx namespace
        # AND specifically pods with the ingress controller label
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx # Adjust if your ingress ns is different
          podSelector:
            matchLabels:
              app.kubernetes.io/name: ingress-nginx # Adjust label if needed
        ports: # Allow traffic TO the container port
        - protocol: TCP
          port: 8080
    ```
    Apply it: `kubectl apply -f allow-ingress.yaml`

7.  **Test Access Again:**
    ```bash
    curl http://$INGRESS_IP:$INGRESS_PORT/v1/
    # Should work again!

    curl http://$INGRESS_IP:$INGRESS_PORT/v2/
    # Should work again!
    ```
    *(Note: Egress might still be blocked depending on your CNI and if the app needs to reach external services, like DNS. You might need an egress policy too.)*

8.  **Clean Up:**
    ```bash
    kubectl delete -f apps.yaml
    kubectl delete -f hello-ingress.yaml
    kubectl delete -f default-deny.yaml
    kubectl delete -f allow-ingress.yaml
    ```

**Congratulations!** You have successfully exposed applications using an Ingress controller and resource, and then secured them using NetworkPolicies to control traffic flow based on labels and namespaces. This demonstrates essential techniques for managing external access and network security in Kubernetes.
