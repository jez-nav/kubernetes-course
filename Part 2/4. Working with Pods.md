# Chapter 4: Working with Pods

In Part 1, we introduced the Pod as the fundamental building block in Kubernetes. This chapter dives deeper into creating, managing, and understanding the lifecycle of Pods. We'll explore multi-container Pod patterns, health checks, and how to manage resource allocation for your Pods.

## Creating and Managing Pods

While you typically manage Pods indirectly through higher-level controllers like Deployments or StatefulSets (covered in Chapter 5), understanding how to work with Pods directly is essential for debugging and grasping core concepts.

**Creating a Pod (Declarative Approach):**

The most common way to create Kubernetes objects is by defining them in a YAML manifest file and applying it using `kubectl`.

*Example (`simple-pod.yaml`):*
```yaml
apiVersion: v1 # Specifies the API version
kind: Pod       # Specifies the type of object
metadata:
  name: my-simple-pod # Name of the Pod
  labels:
    app: webserver   # Labels for organization and selection
spec: # Specification of the desired state
  containers:
  - name: nginx-container # Name of the container within the Pod
    image: nginx:1.21    # Docker image to use
    ports:
    - containerPort: 80 # Port the container exposes
      protocol: TCP
```

**Apply the manifest:**
```bash
kubectl apply -f simple-pod.yaml
# Output: pod/my-simple-pod created
```

**Managing Pods with `kubectl`:**

*   **List Pods:**
    ```bash
    kubectl get pods
    # Output shows NAME, READY state (containers ready/total), STATUS, RESTARTS, AGE
    ```
*   **Get Detailed Information:**
    ```bash
    kubectl describe pod my-simple-pod
    # Shows labels, node assignment, IP address, container details, volumes, events, etc.
    ```
*   **View Logs:**
    ```bash
    kubectl logs my-simple-pod
    # Shows the standard output of the first container in the Pod
    kubectl logs my-simple-pod -c <container-name> # Specify container if multiple exist
    kubectl logs -f my-simple-pod # Follow log output (like tail -f)
    ```
*   **Execute Commands Inside a Container:**
    ```bash
    kubectl exec -it my-simple-pod -- /bin/bash
    # Opens an interactive shell inside the nginx-container
    # Use 'exit' to leave the shell
    kubectl exec my-simple-pod -- ls /usr/share/nginx/html # Run a single command
    ```
*   **Port Forwarding (for local access/debugging):**
    ```bash
    kubectl port-forward pod/my-simple-pod 8080:80
    # Forwards connections from your local machine's port 8080 to the Pod's port 80
    # Access via http://localhost:8080 in your browser
    ```
    *(Press Ctrl+C in the terminal to stop forwarding)*
*   **Delete a Pod:**
    ```bash
    kubectl delete pod my-simple-pod
    # Output: pod "my-simple-pod" deleted
    # Or delete using the manifest file:
    # kubectl delete -f simple-pod.yaml
    ```

**Important:** Remember that Pods created directly are not resilient. If the node fails or the Pod is deleted, it won't be automatically recreated. Use Deployments or other controllers for managing application lifecycle.

## Multi-Container Pods and Sidecars

A Pod can contain multiple containers that run alongside each other and share the same network namespace and storage volumes. This is useful for tightly coupled helper processes.

**Common Patterns:**

*   **Sidecar:** A helper container that assists the main application container. Examples:
    *   **Logging Agent:** Collects logs from the main app container (e.g., via a shared volume) and forwards them to a central logging system.
    *   **Proxy:** Intercepts network traffic to/from the main container for monitoring, security, or routing (common in service meshes like Istio).
    *   **Data Synchronizer:** Pulls configuration or data updates for the main application.
*   **Adapter:** Standardizes or modifies the output or interface of the main container.
*   **Ambassador:** Proxies connections to external services, simplifying configuration for the main application.

**Example: Pod with a Main App and a Sidecar Log Shipper**

Imagine a main application container writing logs to a file in a shared volume, and a sidecar container reading from that volume and sending logs elsewhere.

*Example (`sidecar-pod.yaml`):*
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: app-with-sidecar
spec:
  volumes: # Define a shared volume
  - name: shared-logs
    emptyDir: {} # Simple empty directory volume, lives as long as the Pod

  containers:
  - name: main-app # The primary application container
    image: busybox:latest
    # Simulate writing logs every 5 seconds
    command: ["/bin/sh", "-c"]
    args:
      - >
        i=0;
        while true; do
          echo "$i: Main app log message - $(date)" >> /var/log/app.log;
          i=$((i+1));
          sleep 5;
        done
    volumeMounts: # Mount the shared volume
    - name: shared-logs
      mountPath: /var/log

  - name: sidecar-logger # The sidecar container
    image: busybox:latest
    # Simulate reading logs every 6 seconds
    command: ["/bin/sh", "-c", "while true; do echo 'Sidecar reading:'; cat /var/log/app.log; sleep 6; done"]
    volumeMounts: # Mount the same shared volume
    - name: shared-logs
      mountPath: /var/log
```

**Apply and Inspect:**
```bash
kubectl apply -f sidecar-pod.yaml
kubectl get pods app-with-sidecar # Wait for READY 2/2
kubectl logs app-with-sidecar -c main-app -f # See main app writing logs
kubectl logs app-with-sidecar -c sidecar-logger -f # See sidecar reading logs
kubectl delete -f sidecar-pod.yaml
```

**Key Points for Multi-Container Pods:**
*   Containers share the network (localhost communication is possible) and volumes.
*   Each container has its own filesystem layer (except shared volumes).
*   The Pod terminates only when *all* its containers have stopped.
*   Resource requests/limits are set per container.

## Pod Lifecycle and Health Checks

Pods go through various phases in their lifecycle:

*   **Pending:** The Pod has been accepted by the cluster, but one or more container images have not been created. This includes time spent scheduling the Pod onto a node and downloading images.
*   **Running:** The Pod has been bound to a node, and all of the containers have been created. At least one container is still running, or is in the process of starting or restarting.
*   **Succeeded:** All containers in the Pod have terminated successfully (exit code 0), and will not be restarted. (Common for Jobs).
*   **Failed:** All containers in the Pod have terminated, and at least one container has terminated in failure (non-zero exit code).
*   **Unknown:** The state of the Pod could not be obtained, typically due to an error communicating with the kubelet on the node where the Pod is supposed to be running.

Kubernetes uses **probes** to monitor the health of containers within a Pod:

*   **Liveness Probe:** Checks if a container is still running and responsive. If the liveness probe fails (e.g., the application deadlocks), the `kubelet` kills the container, and the container is subject to its **restart policy**.
    *   **Restart Policies:** Defined in `spec.restartPolicy` (Default: `Always`).
        *   `Always`: Restart container on any failure.
        *   `OnFailure`: Restart only if the container exits with a non-zero status.
        *   `Never`: Do not restart the container.
*   **Readiness Probe:** Checks if a container is ready to start accepting traffic. If the readiness probe fails, the Pod's IP address is removed from the Endpoints object for any matching Services. This prevents traffic from being sent to a Pod that is running but not yet ready (e.g., still initializing, loading data).
*   **Startup Probe:** Checks if a container application has started successfully. If configured, all other probes are disabled until the startup probe succeeds. This is useful for slow-starting containers, preventing liveness/readiness probes from killing them prematurely.

**Types of Probes:**

*   **HTTP GET:** Performs an HTTP GET request against a specific path and port on the container's IP. Success is indicated by a status code between 200 and 399.
*   **TCP Socket:** Attempts to open a TCP connection to a specified port on the container's IP. Success if the connection can be established.
*   **Exec:** Executes a specified command inside the container. Success if the command exits with status code 0.

**Example: Pod with Liveness and Readiness Probes**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: probe-demo
spec:
  containers:
  - name: nginx-probed
    image: nginx:1.21
    ports:
    - containerPort: 80
    livenessProbe:
      httpGet:
        path: / # Path to check on the nginx server
        port: 80
      initialDelaySeconds: 5 # Wait 5s before first probe
      periodSeconds: 10     # Probe every 10s
      failureThreshold: 3   # Consider failed after 3 consecutive failures
    readinessProbe:
      httpGet:
        path: /
        port: 80
      initialDelaySeconds: 3
      periodSeconds: 5
      successThreshold: 1   # Consider ready after 1 success
      failureThreshold: 2
```

**Apply and Observe:**
```bash
kubectl apply -f probe-demo.yaml
kubectl describe pod probe-demo # Look at the Events section to see probe results
# To simulate failure, exec into the pod and break nginx or delete index.html
# kubectl exec -it probe-demo -- /bin/bash
# root@probe-demo:/# rm /usr/share/nginx/html/index.html
# Observe 'kubectl describe pod probe-demo' again - readiness fails, restarts may occur if liveness fails enough times.
kubectl delete pod probe-demo
```

## Resource Limits and Requests

You can specify how much CPU and memory (RAM) each container in a Pod needs (**requests**) and the maximum amount it's allowed to consume (**limits**).

*   **Requests:** The minimum amount of resources the container needs. Kubernetes uses requests for scheduling decisions â€“ a Pod will only be scheduled on a node that has enough available resources to satisfy the Pod's total requests. Requests also influence resource guarantees; if a container requests resources, it's guaranteed to get them.
*   **Limits:** The maximum amount of resources the container can use.
    *   **CPU Limit:** If a container exceeds its CPU limit, it gets throttled (its CPU usage is capped).
    *   **Memory Limit:** If a container exceeds its memory limit, it might be terminated by the kernel (OOMKilled - Out Of Memory Killed).

**Units:**
*   **CPU:** Measured in "CPU units". `1` core = 1 AWS vCPU = 1 GCP Core = 1 Azure Core = 1 Hyperthread. You can specify fractions, often using millicores/millicpus (e.g., `500m` = 0.5 CPU).
*   **Memory:** Measured in bytes. Use suffixes like `Ki` (Kibibyte), `Mi` (Mebibyte), `Gi` (Gibibyte) or `K`, `M`, `G`. (Powers of 2 are standard: 1 Mi = 1024 Ki).

**Example: Pod with Resource Requests and Limits**

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
  - name: nginx-resourced
    image: nginx:1.21
    ports:
    - containerPort: 80
    resources:
      requests: # Minimum guaranteed resources
        memory: "64Mi" # 64 Mebibytes
        cpu: "250m"    # 0.25 CPU core
      limits:   # Maximum allowed resources
        memory: "128Mi" # 128 Mebibytes
        cpu: "500m"     # 0.5 CPU core
```

**Why Set Requests and Limits?**

*   **Scheduling:** Ensures Pods land on nodes with sufficient capacity.
*   **Resource Guarantee:** Requests provide a baseline level of resources.
*   **Node Stability:** Limits prevent containers from consuming excessive resources and impacting other workloads or the node itself.
*   **Quality of Service (QoS):** Kubernetes assigns QoS classes based on requests/limits:
    *   **Guaranteed:** Requests == Limits (for all resources, CPU & Memory). Highest priority, least likely to be killed under node pressure.
    *   **Burstable:** Requests < Limits (or requests set, limits not). Medium priority. Can use more resources than requested if available, up to the limit.
    *   **BestEffort:** No requests or limits set. Lowest priority, most likely to be killed if the node runs out of resources.

## Lab: Deploy a Pod with a Sidecar Logging Container

This lab combines concepts: creating a multi-container Pod where a main application writes to a shared volume, and a sidecar container reads from it.

**Prerequisites:**
*   `kubectl` connected to a running Kubernetes cluster (Minikube is fine).

**Steps:**

1.  **Create the Manifest File:**
    Save the following content as `lab-sidecar-pod.yaml`:
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: counter-app
    spec:
      volumes:
      - name: app-logs
        emptyDir: {} # Shared volume for logs

      containers:
      - name: counter # Main application container
        image: busybox:latest
        command: ["/bin/sh", "-c"]
        args:
          - >
            i=0;
            while true; do
              echo "$(date) - Count: $i" >> /var/log/count.log;
              i=$((i+1));
              sleep 2;
            done
        volumeMounts:
        - name: app-logs # Mount the shared volume
          mountPath: /var/log
        resources: # Add some basic resource requests/limits
          requests:
            cpu: "50m"
            memory: "32Mi"
          limits:
            cpu: "100m"
            memory: "64Mi"

      - name: log-reader # Sidecar container
        image: busybox:latest
        command: ["/bin/sh", "-c", "tail -f /var/log/count.log"] # Continuously tail the log file
        volumeMounts:
        - name: app-logs # Mount the same shared volume
          mountPath: /var/log
        resources:
          requests:
            cpu: "25m"
            memory: "16Mi"
          limits:
            cpu: "50m"
            memory: "32Mi"
    ```

2.  **Deploy the Pod:**
    ```bash
    kubectl apply -f lab-sidecar-pod.yaml
    ```

3.  **Check Pod Status:**
    Wait for the Pod to become ready (READY 2/2):
    ```bash
    kubectl get pod counter-app -w # -w watches for changes
    # Press Ctrl+C when READY is 2/2
    ```

4.  **View Logs from the Sidecar:**
    Since the sidecar is tailing the log file written by the main container, viewing its logs effectively shows the application's output:
    ```bash
    kubectl logs counter-app -c log-reader -f
    # You should see lines like: Thu Apr 17 03:50:00 UTC 2025 - Count: 0, ... Count: 1, etc.
    # Press Ctrl+C to stop following
    ```

5.  **Inspect the Pod:**
    ```bash
    kubectl describe pod counter-app
    # Examine the details:
    # - Both containers listed
    # - The shared 'app-logs' volume defined and mounted in both containers
    # - Resource requests and limits applied
    # - Events showing container creation and startup
    ```

6.  **Clean Up:**
    ```bash
    kubectl delete pod counter-app
    # Or: kubectl delete -f lab-sidecar-pod.yaml
    ```

**Congratulations!** You've successfully deployed a multi-container Pod using the sidecar pattern, managed shared storage with volumes, and observed logs flowing between containers. This demonstrates the power and flexibility of the Pod abstraction.
