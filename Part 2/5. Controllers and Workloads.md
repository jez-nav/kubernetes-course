# Chapter 5: Controllers and Workloads

While Pods are the basic execution unit, they are rarely deployed directly in production due to their ephemeral nature. Kubernetes provides higher-level **controllers** that manage Pods, ensuring reliability, scalability, and handling specific workload types. This chapter explores the most important controllers: ReplicaSets, Deployments, DaemonSets, Jobs, and CronJobs.

## ReplicaSets for Replication

As briefly mentioned in Chapter 3, a **ReplicaSet** ensures that a specified number of Pod replicas are running at any given time.

*   **Purpose:** Maintain a stable set of replica Pods. Guarantees availability and basic scaling.
*   **How it works:** It uses a `selector` to identify the Pods it manages (based on labels) and a `template` to define the specification for new Pods it needs to create. If there are too few Pods matching the selector, it creates more based on the template. If there are too many, it deletes extras.
*   **Usage:** You typically don't interact with ReplicaSets directly. **Deployments** are the recommended way to manage ReplicaSets and provide update capabilities. A Deployment creates and manages ReplicaSets for you.

*Review ReplicaSet YAML from Chapter 3:*
```yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: nginx-replicaset
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest
```

**Key Commands:**
```bash
kubectl get rs # List ReplicaSets
kubectl describe rs <replicaset-name> # Get details
kubectl scale rs <replicaset-name> --replicas=5 # Manually scale (usually done via Deployment)
```

## Deployments for Rolling Updates and Rollbacks

**Deployments** are the standard and most common way to manage stateless applications in Kubernetes. They provide declarative updates for Pods and ReplicaSets.

*   **Purpose:** Manage the deployment and scaling of stateless applications, handle updates gracefully, and allow easy rollbacks.
*   **How it works:**
    1.  You define a desired state in the Deployment object (e.g., image version, number of replicas).
    2.  The Deployment controller creates a ReplicaSet based on this desired state.
    3.  The ReplicaSet creates the required Pods.
    4.  When you update the Deployment (e.g., change the image tag), the Deployment controller creates a *new* ReplicaSet with the updated template.
    5.  It then gradually scales down the old ReplicaSet and scales up the new ReplicaSet, managing the Pod transition according to a defined **update strategy**.
*   **Update Strategies:**
    *   `RollingUpdate` (Default): Gradually replaces old Pods with new ones, ensuring zero downtime if configured correctly (with health checks). You can control the pace with `maxUnavailable` (how many Pods can be unavailable during the update) and `maxSurge` (how many extra Pods can be created above the desired count).
    *   `Recreate`: Terminates all old Pods before creating new ones. Results in downtime but is simpler.

*Review Deployment YAML from Chapter 3:*
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  strategy: # Optional: Define update strategy
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1 # Max 1 pod unavailable during update
      maxSurge: 1       # Max 1 extra pod created during update
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.21 # Current version
        ports:
        - containerPort: 80
        readinessProbe: # Add readiness probe for smoother updates
          httpGet:
            path: /
            port: 80
          initialDelaySeconds: 5
          periodSeconds: 5
```

**Managing Deployments:**

*   **Create/Apply:** `kubectl apply -f deployment.yaml`
*   **List:** `kubectl get deployments` or `kubectl get deploy`
*   **Describe:** `kubectl describe deployment <deployment-name>`
*   **Scale:** `kubectl scale deployment <deployment-name> --replicas=5`
*   **Update Image:**
    ```bash
    # Imperative command (quick change)
    kubectl set image deployment/<deployment-name> <container-name>=<new-image>:<new-tag>
    # Example:
    kubectl set image deployment/nginx-deployment nginx-container=nginx:1.22

    # Declarative approach (preferred for GitOps/IaC)
    # 1. Edit deployment.yaml (change spec.template.spec.containers[0].image)
    # 2. kubectl apply -f deployment.yaml
    ```
*   **Check Rollout Status:**
    ```bash
    kubectl rollout status deployment/<deployment-name>
    # Output: deployment "nginx-deployment" successfully rolled out
    ```
*   **View Rollout History:**
    ```bash
    kubectl rollout history deployment/<deployment-name>
    # Shows revision history
    kubectl rollout history deployment/<deployment-name> --revision=2 # View details of a specific revision
    ```
*   **Rollback:**
    ```bash
    # Rollback to the previous version
    kubectl rollout undo deployment/<deployment-name>

    # Rollback to a specific revision
    kubectl rollout undo deployment/<deployment-name> --to-revision=1
    ```

## DaemonSets for Node-Specific Tasks

A **DaemonSet** ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed, those Pods are garbage collected.

*   **Purpose:** Deploy system daemons or agents that need to run on every (or a subset of) node(s) in the cluster.
*   **Use Cases:**
    *   **Log Collectors:** Running a log collection agent like Fluentd or Logstash on each node.
    *   **Node Monitoring:** Running a monitoring agent like Prometheus Node Exporter or Datadog agent.
    *   **Cluster Storage:** Running a cluster storage daemon like Glusterd or Ceph on each node.
    *   **Network Plugins (CNI):** Some CNI implementations run as DaemonSets.
*   **How it works:** Similar to Deployments, it uses a Pod template. However, instead of a fixed replica count, it ensures one Pod per matching node. It uses node selectors or affinity rules to target specific nodes if needed.

*Example DaemonSet (`fluentd-daemonset.yaml`):*
```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd-logging
  labels:
    app: fluentd
spec:
  selector:
    matchLabels:
      app: fluentd
  template:
    metadata:
      labels:
        app: fluentd
    spec:
      tolerations: # Often needed to run on control-plane nodes too
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluentd-container
        image: fluent/fluentd:v1.14-debian-1 # Example image
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
          limits:
            cpu: 200m
            memory: 400Mi
        volumeMounts: # Mount host directories to collect logs
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes: # Define volumes to mount from the host node
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
```

**Managing DaemonSets:**
```bash
kubectl apply -f fluentd-daemonset.yaml
kubectl get ds # List DaemonSets
kubectl describe ds fluentd-logging
kubectl get pods -l app=fluentd -o wide # See pods running on different nodes
kubectl delete ds fluentd-logging
```

## Jobs and CronJobs for Batch Processing

**Jobs** and **CronJobs** are used for running tasks that execute once and complete, rather than running continuously like services.

### Job

*   **Purpose:** Run a task to completion. Creates one or more Pods and ensures that a specified number of them successfully terminate.
*   **Use Cases:** Batch processing, data migration, running tests, performing backups.
*   **How it works:** A Job creates Pods based on its template. It tracks the successful completions. Once the desired number of completions is reached, the Job itself is considered complete. The Pods are usually *not* deleted automatically (controlled by `ttlSecondsAfterFinished`) to allow inspection of logs.
*   **Completion Modes:**
    *   `Non-parallel Job`: Runs a single Pod to completion. (Default)
    *   `Parallel Job with a fixed completion count`: Runs multiple Pods in parallel until a specified number (`.spec.completions`) succeed.
    *   `Parallel Job with a work queue`: Pods coordinate among themselves or with an external service to determine what work to perform.

*Example Job (`pi-calculator-job.yaml`):*
```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: pi-job
spec:
  template: # Pod template
    spec:
      containers:
      - name: pi
        image: perl:5.34 # Perl image
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"] # Calculate Pi to 2000 digits
      restartPolicy: Never # Or OnFailure - Never is common for batch jobs
  backoffLimit: 4 # Number of retries before marking job as failed (default 6)
  # ttlSecondsAfterFinished: 100 # Optional: Auto-delete job after 100s
```

**Managing Jobs:**
```bash
kubectl apply -f pi-calculator-job.yaml
kubectl get jobs
kubectl describe job pi-job
kubectl get pods -l job-name=pi-job # Find pods created by the job
kubectl logs <pod-name-from-job> # View the output (the digits of Pi)
kubectl delete job pi-job
```

### CronJob

*   **Purpose:** Run a Job on a time-based schedule (like a Linux cron job).
*   **How it works:** A CronJob creates Job objects based on its schedule.
*   **Schedule Format:** Uses standard Cron syntax (`Minute Hour DayOfMonth Month DayOfWeek`). Example: `0 */2 * * *` means "at minute 0 past every 2nd hour".
*   **Concurrency Policy:** Controls what happens if it's time for a new Job run but the previous one hasn't finished:
    *   `Allow` (Default): Allows concurrent Job runs.
    *   `Forbid`: Skips the new run if the previous one is still active.
    *   `Replace`: Cancels the currently running Job and replaces it with the new one.

*Example CronJob (`hello-cronjob.yaml`):*
```yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: hello-cron
spec:
  schedule: "*/1 * * * *" # Run every minute
  jobTemplate: # Template for the Job object it creates
    spec:
      template: # Pod template within the Job template
        spec:
          containers:
          - name: hello
            image: busybox:latest
            command: ["/bin/sh", "-c", "date; echo Hello from the Kubernetes CronJob"]
          restartPolicy: OnFailure
  concurrencyPolicy: Forbid # Don't run if previous job is still running
  successfulJobsHistoryLimit: 3 # Keep history of last 3 successful jobs
  failedJobsHistoryLimit: 1     # Keep history of last 1 failed job
```

**Managing CronJobs:**
```bash
kubectl apply -f hello-cronjob.yaml
kubectl get cronjobs # or cj
kubectl describe cronjob hello-cron
kubectl get jobs --watch # Watch for new jobs being created every minute
kubectl get pods --show-labels # See pods created by the cronjob (label: job-name=...)
kubectl delete cronjob hello-cron
```

## Lab: Deploy a Scalable Web App with Rolling Updates

This lab uses a Deployment to manage a simple web application, scales it, and performs a rolling update.

**Prerequisites:**
*   `kubectl` connected to a running Kubernetes cluster.

**Steps:**

1.  **Create Deployment Manifest:**
    Save the following as `webapp-deployment.yaml`. It uses `echoserver`, a simple HTTP server that echoes request details.
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: echo-deploy
    spec:
      replicas: 2 # Start with 2 replicas
      selector:
        matchLabels:
          app: echoserver
      strategy:
        type: RollingUpdate
        rollingUpdate:
          maxUnavailable: 1
          maxSurge: 1
      template:
        metadata:
          labels:
            app: echoserver
        spec:
          containers:
          - name: echo-container
            image: k8s.gcr.io/echoserver:1.10 # Initial version
            ports:
            - containerPort: 8080
            readinessProbe: # Essential for smooth rolling updates
              httpGet:
                path: / # echoserver responds to /
                port: 8080
              initialDelaySeconds: 3
              periodSeconds: 5
    ---
    apiVersion: v1
    kind: Service # Expose the deployment internally
    metadata:
      name: echo-service
    spec:
      selector:
        app: echoserver # Selects pods managed by the deployment
      ports:
      - protocol: TCP
        port: 80 # Service port
        targetPort: 8080 # Container port
      type: ClusterIP # Internal access only for now
    ```

2.  **Deploy the Application:**
    ```bash
    kubectl apply -f webapp-deployment.yaml
    # Output: deployment.apps/echo-deploy created, service/echo-service created
    ```

3.  **Check Deployment and Pods:**
    ```bash
    kubectl get deployment echo-deploy
    kubectl get pods -l app=echoserver -o wide # See 2 pods running on potentially different nodes
    ```

4.  **Access the Service (via Port Forwarding):**
    Since it's a ClusterIP service, use port-forwarding to test:
    ```bash
    kubectl port-forward service/echo-service 9090:80
    ```
    Open `http://localhost:9090` in your browser. You should see the echoserver response. Refresh a few times; you might hit different Pods but see the same app version. Stop port-forwarding (Ctrl+C).

5.  **Scale the Deployment:**
    ```bash
    kubectl scale deployment echo-deploy --replicas=4
    kubectl get pods -l app=echoserver # Observe 4 pods eventually running
    ```

6.  **Perform a Rolling Update:**
    Update the container image to a different (non-existent for demo) tag to see the update process.
    ```bash
    kubectl set image deployment/echo-deploy echo-container=k8s.gcr.io/echoserver:1.11 # Fictional newer version
    ```
    *(Note: In a real scenario, use an actual newer image tag)*

7.  **Observe the Rollout:**
    Quickly run these commands in another terminal or sequentially:
    ```bash
    kubectl rollout status deployment/echo-deploy # Watch the update progress
    kubectl get pods -l app=echoserver -w # Watch pods terminating and new ones starting
    kubectl describe deployment echo-deploy # See the events related to scaling up new RS and scaling down old RS
    ```
    You'll see Pods with the old image terminating and new Pods starting. Because `k8s.gcr.io/echoserver:1.11` likely doesn't exist, the new pods will probably get stuck in `ImagePullBackOff` or `ErrImagePull`. This simulates a failed update.

8.  **Check Rollout History:**
    ```bash
    kubectl rollout history deployment/echo-deploy
    # Shows multiple revisions
    ```

9.  **Rollback the Failed Update:**
    ```bash
    kubectl rollout undo deployment/echo-deploy
    ```

10. **Observe the Rollback:**
    ```bash
    kubectl rollout status deployment/echo-deploy
    kubectl get pods -l app=echoserver -w # Watch pods with the bad image terminate and pods with the original image (1.10) start
    ```
    The deployment should return to a healthy state with 4 replicas running the original `echoserver:1.10` image.

11. **Clean Up:**
    ```bash
    kubectl delete -f webapp-deployment.yaml
    # Output: deployment.apps "echo-deploy" deleted, service "echo-service" deleted
    ```

**Congratulations!** You've used a Deployment to manage a stateless application, scaled it up, attempted a rolling update (and simulated a failure), and successfully rolled back to the previous stable version. This demonstrates the power of Deployments for managing application lifecycles.
