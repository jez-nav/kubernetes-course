# Chapter 2: Container Fundamentals

In Chapter 1, we introduced containers as a key concept in Kubernetes. This chapter dives deeper into what containers are, focusing on Docker as the most popular containerization technology. We'll cover how to build, manage, and share container images, along with basic networking and storage concepts relevant to containers.

## Understanding Containers: Docker Basics

**What is a Container?**

Think of a container as a standardized unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. It's like a lightweight, isolated virtual machine (VM), but instead of virtualizing the entire hardware stack, containers virtualize the operating system (OS). This means multiple containers can run on the same OS kernel, making them much more efficient in terms of resource usage (CPU, memory, disk space) compared to VMs.

**Key Benefits of Containers:**

*   **Consistency:** Containers ensure that applications run the same way regardless of where they are deployed (developer's laptop, testing environment, production server).
*   **Isolation:** Processes running inside a container are isolated from the host system and other containers, improving security and stability.
*   **Efficiency:** Containers start faster and use fewer resources than VMs because they share the host OS kernel.
*   **Portability:** Container images can be easily moved and run across different machines and cloud providers.
*   **Scalability:** Easy to create multiple instances of a container to handle increased load.

**Docker: The De Facto Standard**

Docker is an open platform for developing, shipping, and running applications using containerization. While other container runtimes exist (like containerd, CRI-O), Docker popularized containers and provides a user-friendly set of tools:

*   **Docker Engine:** The core runtime that creates and runs containers.
*   **Docker CLI:** The command-line interface (`docker`) used to interact with Docker Engine.
*   **Dockerfile:** A text file containing instructions to build a Docker image.
*   **Docker Image:** A read-only template containing the application code, libraries, dependencies, and runtime instructions. Containers are running instances of images.
*   **Docker Hub/Registry:** A service for storing and distributing Docker images.

**Basic Docker Commands:**

Assuming you have Docker installed (e.g., via Docker Desktop):

*   `docker run <image_name>`: Creates and starts a container from an image (e.g., `docker run hello-world`).
*   `docker ps`: Lists running containers.
*   `docker ps -a`: Lists all containers (running and stopped).
*   `docker images`: Lists images available locally.
*   `docker pull <image_name>`: Downloads an image from a registry.
*   `docker stop <container_id_or_name>`: Stops a running container.
*   `docker rm <container_id_or_name>`: Removes a stopped container.
*   `docker rmi <image_id_or_name>`: Removes an image.
*   `docker build -t <tag_name> .`: Builds an image from a Dockerfile in the current directory.

## Building and Managing Container Images

A **Docker image** is the blueprint for your container. It's built from a set of instructions defined in a **Dockerfile**.

**Dockerfile Example (Simple Python Web App):**

Let's say you have a simple Python Flask app in `app.py`:

```python
# app.py
from flask import Flask
app = Flask(__name__)

@app.route('/')
def hello():
    return "Hello from Container!"

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080)
```

And a `requirements.txt` file:

```
Flask==2.0.1
```

Your `Dockerfile` might look like this:

```dockerfile
# Use an official Python runtime as a parent image
FROM python:3.9-slim

# Set the working directory in the container
WORKDIR /app

# Copy the requirements file into the container at /app
COPY requirements.txt .

# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt

# Copy the current directory contents into the container at /app
COPY . .

# Make port 8080 available to the world outside this container
EXPOSE 8080

# Define environment variable
ENV NAME World

# Run app.py when the container launches
CMD ["python", "app.py"]
```

**Key Dockerfile Instructions:**

*   `FROM`: Specifies the base image to start from.
*   `WORKDIR`: Sets the working directory for subsequent instructions.
*   `COPY`: Copies files or directories from your local machine into the image.
*   `RUN`: Executes commands during the image build process (e.g., installing packages).
*   `EXPOSE`: Informs Docker that the container listens on the specified network ports at runtime (doesn't actually publish the port).
*   `ENV`: Sets environment variables.
*   `CMD`: Provides defaults for an executing container (can be overridden at runtime). `ENTRYPOINT` is another way to specify the command to run.

**Building the Image:**

Navigate to the directory containing your `Dockerfile`, `app.py`, and `requirements.txt`, then run:

```bash
docker build -t my-python-app:v1 .
```

*   `-t my-python-app:v1`: Tags the image with a name (`my-python-app`) and version (`v1`).
*   `.`: Specifies the build context (the current directory).

**Running the Container:**

```bash
docker run -d -p 8080:8080 --name my-running-app my-python-app:v1
```

*   `-d`: Run in detached mode (in the background).
*   `-p 8080:8080`: Map port 8080 on the host to port 8080 in the container.
*   `--name my-running-app`: Assign a name to the running container.

You should now be able to access your app at `http://localhost:8080`.

**Image Layers:** Docker images are built in layers. Each instruction in the Dockerfile creates a new layer. Docker caches layers, making subsequent builds faster if unchanged layers can be reused.

## Container Registries

A **container registry** is a storage and distribution system for container images.

*   **Docker Hub:** The default public registry ([https://hub.docker.com/](https://hub.docker.com/)). You can find official images for many popular software tools and host your own public or private repositories.
*   **Private Registries:** For security or compliance reasons, organizations often use private registries. Options include:
    *   Cloud Provider Registries: AWS ECR, Google GCR/Artifact Registry, Azure ACR.
    *   Self-Hosted: Harbor, Docker Registry.

**Pushing an Image:**

1.  **Tag the image:** Before pushing, you need to tag your image with the registry's address (and your username for Docker Hub).
    ```bash
    # For Docker Hub (replace 'yourusername' with your actual Docker Hub username)
    docker tag my-python-app:v1 yourusername/my-python-app:v1

    # For a private registry (e.g., AWS ECR)
    # docker tag my-python-app:v1 <aws_account_id>.dkr.ecr.<region>.amazonaws.com/my-python-app:v1
    ```

2.  **Log in (if necessary):**
    ```bash
    docker login # For Docker Hub
    # For private registries, login commands vary (e.g., aws ecr get-login-password...)
    ```

3.  **Push the image:**
    ```bash
    docker push yourusername/my-python-app:v1
    # Or push to your private registry address
    ```

Now, others (or your Kubernetes cluster) can pull and run this image.

## Docker Networking and Storage Basics

While Kubernetes manages networking and storage at a higher level (covered in Chapter 3), understanding the Docker basics is helpful:

**Networking:**

*   **Bridge Network (Default):** Containers on the same default bridge network can communicate using IP addresses. Docker creates a virtual bridge `docker0`. Port mapping (`-p`) exposes container ports to the host.
*   **Host Network:** Removes network isolation between the container and the host. The container shares the host's network namespace.
*   **Overlay Network:** Used for connecting containers across multiple Docker hosts (relevant for Docker Swarm, less so for Kubernetes basics).
*   **None Network:** Disables networking for the container.

**Storage:**

Containers are ephemeral by default â€“ data written inside a container's filesystem is lost when the container is removed. To persist data, Docker offers:

*   **Volumes:** The preferred mechanism. Volumes are managed by Docker (`docker volume create`, `docker volume ls`) and stored in a dedicated area on the host filesystem. They can be easily backed up or migrated. You mount a volume into a container using the `-v` or `--mount` flag in `docker run`.
    ```bash
    docker run -d --name some-db -v my-db-data:/var/lib/mysql mysql:latest
    # Creates/uses a volume named 'my-db-data' and mounts it at /var/lib/mysql in the container
    ```
*   **Bind Mounts:** Mount a file or directory from the host machine directly into the container. Useful for development (mounting source code) but less portable and can have permission issues.
    ```bash
    docker run -d -p 8080:8080 -v $(pwd):/app my-python-app:v1
    # Mounts the current host directory into /app in the container
    ```
*   **tmpfs Mounts:** Store data in the host's memory only (temporary).

Kubernetes builds upon these concepts with its own Volume and Networking abstractions, which provide more robust and cluster-wide solutions.

## Lab: Build and Push a Custom Docker Image

This lab reinforces the concepts of building a Docker image and pushing it to a registry (we'll use Docker Hub for simplicity).

**Prerequisites:**
*   Docker installed and running.
*   A Docker Hub account ([https://hub.docker.com/signup](https://hub.docker.com/signup)).

**Steps:**

1.  **Create Project Files:**
    *   Create a directory named `mywebapp`.
    *   Inside `mywebapp`, create `app.py` with the simple Flask app code from the example above.
    *   Inside `mywebapp`, create `requirements.txt` containing just `Flask`.
    *   Inside `mywebapp`, create a `Dockerfile` with the content from the example above.

2.  **Build the Docker Image:**
    *   Open your terminal and navigate into the `mywebapp` directory.
    *   Run the build command, replacing `yourusername` with your Docker Hub username:
        ```bash
        docker build -t yourusername/mywebapp:1.0 .
        ```
    *   Verify the image was created:
        ```bash
        docker images
        # You should see 'yourusername/mywebapp' with tag '1.0'
        ```

3.  **Test the Image Locally:**
    *   Run a container from your new image:
        ```bash
        docker run -d -p 5000:8080 --name test-app yourusername/mywebapp:1.0
        ```
        *(Note: We map host port 5000 to container port 8080 here to avoid potential conflicts with other services running on 8080)*
    *   Open your browser and go to `http://localhost:5000`. You should see "Hello from Container!".
    *   Stop and remove the test container:
        ```bash
        docker stop test-app
        docker rm test-app
        ```

4.  **Log in to Docker Hub:**
    *   In your terminal, run:
        ```bash
        docker login
        ```
    *   Enter your Docker Hub username and password when prompted.

5.  **Push the Image to Docker Hub:**
    *   Run the push command:
        ```bash
        docker push yourusername/mywebapp:1.0
        ```
    *   Wait for the push to complete.

6.  **Verify on Docker Hub (Optional):**
    *   Log in to your Docker Hub account in your web browser.
    *   Go to your repositories. You should see `mywebapp` listed with the `1.0` tag.

**Congratulations!** You have successfully built a custom Docker image containing a simple web application and pushed it to Docker Hub, making it available for others (and your future Kubernetes deployments) to use. This foundational skill is essential for working with Kubernetes.
