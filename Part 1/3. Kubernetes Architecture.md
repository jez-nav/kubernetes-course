# Chapter 3: Kubernetes Architecture

Now that you understand containers and the basic purpose of Kubernetes, let's explore its architecture. Understanding the components of a Kubernetes cluster and how they interact is crucial for effective deployment and management. We'll also introduce fundamental Kubernetes objects like Pods, Deployments, Services, and Volumes.

## Cluster Components

A Kubernetes cluster consists of two main types of components: the **Control Plane** components and the **Node** components.

### Control Plane Components

The Control Plane acts as the brain of the cluster. It manages the state of the cluster, makes scheduling decisions, and responds to events. Typically, the Control Plane components run on dedicated machines (often called master nodes), but in development environments like Minikube, they might run alongside worker components on a single machine.

*   **`kube-apiserver`:** The front-end for the Kubernetes control plane. It exposes the Kubernetes API, which is used by `kubectl`, other cluster components, and external users to interact with the cluster. It processes and validates REST requests, updates the state in `etcd`, and serves as the central hub for communication.
*   **`etcd`:** A consistent and highly-available distributed key-value store used as Kubernetes' backing store for all cluster data. All cluster state (like Pod definitions, Service configurations, Node status, etc.) is stored here. Having a reliable `etcd` is critical for cluster stability.
*   **`kube-scheduler`:** Watches for newly created Pods that haven't been assigned to a Node. Based on resource requirements, hardware constraints, affinity/anti-affinity rules, and other policies, the scheduler selects an optimal Node for each Pod to run on.
*   **`kube-controller-manager`:** Runs various controller processes in the background. Controllers watch the state of the cluster through the API Server and work to move the current state towards the desired state. Examples include:
    *   **Node Controller:** Responsible for noticing and responding when nodes go down.
    *   **Replication Controller/ReplicaSet Controller:** Maintains the correct number of Pods for replication objects.
    *   **Endpoints Controller:** Populates the Endpoints object (joins Services & Pods).
    *   **Service Account & Token Controllers:** Create default accounts and API access tokens for new namespaces.
*   **`cloud-controller-manager` (Optional):** Embeds cloud-specific control logic. It allows you to link your cluster into your cloud provider's API, separating components that interact with the cloud platform from components that just interact with your cluster. This is not present in all environments (e.g., bare-metal clusters).

### Node Components

Node components run on every worker node in the cluster, maintaining running Pods and providing the Kubernetes runtime environment.

*   **`kubelet`:** An agent that runs on each node in the cluster. It ensures that containers described in PodSpecs (provided by the API server) are running and healthy. It communicates with the container runtime to start/stop containers and reports node and Pod status back to the control plane.
*   **`kube-proxy`:** A network proxy that runs on each node. It maintains network rules on nodes, enabling network communication to your Pods from network sessions inside or outside of your cluster. It uses the operating system packet filtering layer (like iptables or IPVS) or forwards traffic itself.
*   **Container Runtime:** The software responsible for running containers. Kubernetes supports several runtimes conforming to the Container Runtime Interface (CRI), including:
    *   **Docker:** (Historically popular, now often uses containerd via a shim).
    *   **containerd:** An industry-standard core container runtime.
    *   **CRI-O:** Another OCI-compliant runtime specifically designed for Kubernetes.

**Interaction:** The `kubectl` command-line tool communicates with the `kube-apiserver`. The API server validates the request and stores the desired state in `etcd`. The `kube-scheduler` assigns Pods to Nodes. The `kubelet` on the assigned Node receives the Pod specification from the API server and instructs the container runtime to start the required containers. `kube-proxy` sets up the necessary network rules. Controllers continuously monitor the cluster state via the API server and take corrective actions to match the desired state stored in `etcd`.

## Core Workload Resources: Pods, ReplicaSets, and Deployments

Kubernetes uses objects to represent the state of your cluster. These objects define your applications, their configurations, and the rules governing their behavior.

*   **Pod:**
    *   The smallest deployable unit in Kubernetes.
    *   Represents a single instance of an application.
    *   Can contain one or more tightly coupled containers that share the same network namespace (IP address, port space) and storage volumes.
    *   Pods are *ephemeral*: they are not designed to be long-lived. If a Pod fails or the node it runs on fails, the Pod is not automatically recreated. Higher-level controllers manage Pod lifecycle.

    *Example Pod Definition (`pod.yaml`):*
    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
      name: my-nginx-pod
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx-container
        image: nginx:latest
        ports:
        - containerPort: 80
    ```
    *(Apply with `kubectl apply -f pod.yaml`, view with `kubectl get pods`, delete with `kubectl delete pod my-nginx-pod`)*

*   **ReplicaSet:**
    *   Ensures that a specified number of identical Pod replicas are running at any given time.
    *   If a Pod managed by a ReplicaSet fails or is deleted, the ReplicaSet controller creates a new one to maintain the desired count.
    *   Uses **selectors** (based on labels) to identify the Pods it manages.
    *   You typically don't create ReplicaSets directly; Deployments manage them for you.

    *Example ReplicaSet Definition (`replicaset.yaml`):*
    ```yaml
    apiVersion: apps/v1
    kind: ReplicaSet
    metadata:
      name: nginx-replicaset
    spec:
      replicas: 3 # Desired number of pods
      selector:
        matchLabels:
          app: nginx # Manages pods with this label
      template: # Pod template used to create new pods
        metadata:
          labels:
            app: nginx # Label applied to created pods
        spec:
          containers:
          - name: nginx-container
            image: nginx:latest
            ports:
            - containerPort: 80
    ```

*   **Deployment:**
    *   A higher-level object that manages ReplicaSets and provides declarative updates for Pods.
    *   You define the desired state (e.g., image version, number of replicas) in the Deployment object.
    *   The Deployment controller creates a ReplicaSet to manage the Pods.
    *   Handles rolling updates (updating Pods gradually with zero downtime) and rollbacks (reverting to a previous version).
    *   This is the most common way to deploy stateless applications in Kubernetes.

    *Example Deployment Definition (`deployment.yaml`):*
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: nginx-deployment # Same name as in Chapter 1 lab
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
        spec:
          containers:
          - name: nginx-container
            image: nginx:1.21 # Specify a version
            ports:
            - containerPort: 80
    ```
    *(Apply with `kubectl apply -f deployment.yaml`. Update image with `kubectl set image deployment/nginx-deployment nginx-container=nginx:1.22` or by editing the YAML and reapplying. Check rollout status with `kubectl rollout status deployment/nginx-deployment`)*

## Networking: CNI, Services, and DNS

Kubernetes networking enables communication between containers, Pods, Services, and external clients.

*   **Container Network Interface (CNI):** A specification and set of libraries for configuring network interfaces in Linux containers. Kubernetes uses CNI plugins to provide Pod networking. Each Pod gets its own unique IP address within the cluster network. Popular CNI plugins include Calico, Flannel, Weave Net, and Cilium. The choice of CNI plugin affects network features, performance, and security policies. Your cluster setup tool (Minikube, kind, cloud provider) usually installs a default CNI plugin.
*   **Service:** An abstraction that defines a logical set of Pods (usually determined by a label selector) and a policy by which to access them. Services provide a stable IP address and DNS name for a set of Pods, whose actual IPs might change as they are created and destroyed.
    *   **Types of Services:**
        *   `ClusterIP` (Default): Exposes the Service on an internal IP reachable only within the cluster.
        *   `NodePort`: Exposes the Service on each Node's IP at a static port. Allows external access via `<NodeIP>:<NodePort>`. (Used in Chapter 1 lab).
        *   `LoadBalancer`: Exposes the Service externally using a cloud provider's load balancer. Creates a `NodePort` and `ClusterIP` service automatically. Requires cloud provider integration.
        *   `ExternalName`: Maps the Service to an external DNS name (e.g., `my.database.example.com`) by returning a CNAME record.

    *Example Service Definition (`service.yaml`):*
    ```yaml
    apiVersion: v1
    kind: Service
    metadata:
      name: nginx-service
    spec:
      selector:
        app: nginx # Selects pods with the label app=nginx
      ports:
        - protocol: TCP
          port: 80       # Port the service is available on within the cluster
          targetPort: 80 # Port the container is listening on
      type: ClusterIP # Or NodePort, LoadBalancer
    ```
    *(Apply with `kubectl apply -f service.yaml`. View with `kubectl get services`)*

*   **Kubernetes DNS:** An internal DNS service (usually CoreDNS) runs within the cluster. It creates DNS records for Services and Pods, allowing them to discover each other using predictable DNS names (e.g., `nginx-service.default.svc.cluster.local` or simply `nginx-service` if accessed from the same namespace).

## Storage: Volumes and Persistent Volumes

Containers have ephemeral filesystems. Kubernetes provides abstractions for managing persistent storage.

*   **Volume:** A directory, possibly containing data, which is accessible to the Containers in a Pod. How that directory comes to be, the medium that backs it, and its contents are determined by the volume type.
    *   **Lifecycle:** A Volume's lifecycle is tied to the Pod that encloses it. Data in a Volume persists across container restarts within the same Pod, but is typically lost when the Pod is deleted (depending on the volume type).
    *   **Types:** Many types exist, including:
        *   `emptyDir`: A temporary directory created when a Pod is assigned to a Node. Lasts for the life of the Pod. Useful for scratch space or sharing files between containers in a Pod.
        *   `hostPath`: Mounts a file or directory from the host Node's filesystem into the Pod. Use with caution (security risks, node-specific data).
        *   `configMap`/`secret`: Mounts configuration data or secrets as files into the Pod.
        *   Cloud storage (`awsElasticBlockStore`, `gcePersistentDisk`, `azureDisk`): Mounts cloud provider storage.
        *   Network storage (`nfs`, `cephfs`, `iscsi`): Mounts network-attached storage.
        *   `persistentVolumeClaim` (Most common for persistent data): Allows a Pod to request specific storage resources (defined by PersistentVolumes).

*   **PersistentVolume (PV):** A piece of storage in the cluster that has been provisioned by an administrator or dynamically provisioned using StorageClasses. It's a cluster resource like a Node. PVs have a lifecycle independent of any individual Pod.
*   **PersistentVolumeClaim (PVC):** A request for storage by a user (or Pod). It's similar to how a Pod consumes Node resources; a PVC consumes PV resources. The user requests specific size and access modes (e.g., ReadWriteOnce, ReadOnlyMany, ReadWriteMany). Kubernetes finds a matching PV (or dynamically provisions one if a StorageClass is configured) and binds the PVC to the PV.
*   **StorageClass:** Provides a way for administrators to describe the "classes" of storage they offer (e.g., "fast-ssd", "slow-hdd", "backup"). Different classes might map to different quality-of-service levels, backup policies, or arbitrary policies determined by the cluster administrator. Dynamic provisioning relies on StorageClasses.

**Workflow:**
1.  Admin defines StorageClasses and/or pre-provisions PVs.
2.  User creates a PVC requesting specific storage (size, access mode, optional StorageClass).
3.  Kubernetes binds the PVC to a suitable available PV.
4.  User creates a Pod that references the PVC in its `volumes` section.
5.  The `kubelet` mounts the underlying storage (defined by the PV) into the Pod at the specified path.

*Example PVC and Pod using it (`pvc-pod.yaml`):*
```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce # Can be mounted read-write by a single node
  resources:
    requests:
      storage: 1Gi # Request 1 Gibibyte of storage
  # storageClassName: standard # Optional: Request a specific storage class
---
apiVersion: v1
kind: Pod
metadata:
  name: my-storage-pod
spec:
  containers:
  - name: busybox-container
    image: busybox:latest
    command: ["/bin/sh", "-c", "while true; do echo $(date) >> /data/output.log; sleep 5; done"]
    volumeMounts:
    - name: my-storage # Must match volume name below
      mountPath: /data # Mount path inside the container
  volumes:
  - name: my-storage # Volume name referenced by volumeMounts
    persistentVolumeClaim:
      claimName: my-pvc # Reference the PVC created above
```
*(Apply with `kubectl apply -f pvc-pod.yaml`. Check PVC status with `kubectl get pvc`. Check PV status with `kubectl get pv`. Exec into the pod `kubectl exec -it my-storage-pod -- /bin/sh` and check `/data/output.log`)*

## Lab: Explore a Kubernetes Cluster with `kubectl`

This lab uses `kubectl` to inspect the components and objects within your local Kubernetes cluster (Minikube recommended).

**Prerequisites:**
*   Minikube (or similar) installed and running (`minikube start`).
*   `kubectl` installed and configured.

**Steps:**

1.  **Check Cluster and Node Status:**
    ```bash
    kubectl cluster-info
    # Shows addresses of Kubernetes master and services like KubeDNS

    kubectl get nodes
    # Shows the node(s) in your cluster and their status (should be 'Ready')

    kubectl describe node <your-node-name> # e.g., kubectl describe node minikube
    # Shows detailed information about the node, including labels, taints, capacity, allocated resources, and running pods (System pods + any you deployed)
    ```

2.  **Explore Control Plane Components (System Pods):**
    Kubernetes runs its own control plane components as Pods within a dedicated namespace, usually `kube-system`.
    ```bash
    kubectl get pods -n kube-system
    # Lists pods like etcd, kube-apiserver, kube-controller-manager, kube-proxy, kube-scheduler, coredns, storage-provisioner etc.

    kubectl describe pod <apiserver-pod-name> -n kube-system
    # Shows details about the API server pod, including its image, node, IP, status, events, etc. Try describing etcd or scheduler pods too.
    ```

3.  **Deploy an Application (if none running):**
    If you cleaned up from Chapter 1, deploy Nginx again:
    ```bash
    kubectl create deployment nginx-deployment --image=nginx:latest
    kubectl expose deployment nginx-deployment --type=NodePort --port=80
    ```

4.  **Inspect Workload Objects:**
    ```bash
    kubectl get deployments
    # Shows your nginx-deployment

    kubectl describe deployment nginx-deployment
    # Shows details, including the ReplicaSet it manages, update strategy, status, and events. Note the selector.

    kubectl get replicasets
    # Shows the ReplicaSet created by the deployment. Note its name often includes the deployment name and a hash.

    kubectl describe replicaset <replicaset-name>
    # Shows details, including the Pod template and the selector it uses to manage Pods.

    kubectl get pods -l app=nginx # Use the label selector defined in the deployment/rs
    # Shows the Pod(s) managed by the ReplicaSet/Deployment

    kubectl describe pod <pod-name>
    # Shows detailed info about a specific application pod: IP address, node, container(s), image(s), volumes, events, status, etc.
    ```

5.  **Inspect Network Objects:**
    ```bash
    kubectl get services
    # Lists services, including the default 'kubernetes' service and your 'nginx-deployment' service.

    kubectl describe service nginx-deployment
    # Shows details: selector, type (NodePort), IP (ClusterIP), Port/TargetPort/NodePort, and Endpoints.

    kubectl get endpoints
    # Shows Endpoints objects. Find the one for 'nginx-deployment'.

    kubectl describe endpoints nginx-deployment
    # Shows the IP addresses and ports of the Pods currently selected by the 'nginx-deployment' service. This is how the service knows where to send traffic.
    ```

6.  **Inspect Storage Objects (if PVC lab was done):**
    If you applied the `pvc-pod.yaml` example earlier:
    ```bash
    kubectl get pvc
    # Shows my-pvc, its status (Bound), the PV it's bound to, capacity, etc.

    kubectl get pv
    # Shows the PersistentVolume that was bound (might be dynamically provisioned). Note its capacity, reclaim policy, status, and claim reference.

    kubectl describe pvc my-pvc
    kubectl describe pv <pv-name>
    ```

7.  **Clean Up:**
    ```bash
    kubectl delete service nginx-deployment
    kubectl delete deployment nginx-deployment
    # If you created the storage pod/pvc:
    # kubectl delete pod my-storage-pod
    # kubectl delete pvc my-pvc
    # Note: Depending on the reclaim policy, the underlying PV might persist or be deleted.
    ```

**Congratulations!** You've now explored the key architectural components and fundamental objects within a Kubernetes cluster using `kubectl`. This hands-on experience provides a solid foundation for understanding how Kubernetes manages containerized applications. Part 2 will delve deeper into these core concepts.
